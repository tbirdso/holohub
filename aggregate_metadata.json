[
    {
        "metadata": {
            "name": "Riva ASR to local-LLM",
            "authors": [
                {
                    "name": "Nigel Nelson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Speech-to-text",
                "Large Language Model",
                "ASR",
                "Local-LLM"
            ],
            "ranking": 4,
            "dependencies": {
                "python-packages": {
                    "requests": {
                        "version": "2.31.0",
                        "license-url": "https://github.com/opentracing-contrib/python-requests/blob/master/LICENSE"
                    },
                    "nvidia-riva-client": {
                        "version": "2.14.0",
                        "license-url": "https://github.com/nvidia-riva/python-clients/blob/main/LICENSE"
                    },
                    "pyaudio": {
                        "version": "0.2.13",
                        "license-url": "https://github.com/CristiFati/pyaudio/blob/master/LICENSE.txt"
                    },
                    "pynput": {
                        "version": "0.2.13",
                        "license-url": "https://github.com/moses-palmer/pynput/blob/master/COPYING.LGPL"
                    }
                },
                "OSS": {
                    "llama.cpp": {
                        "commit": "cf9b08485c4c2d4d945c6e74fe20f273a38b6104",
                        "license-url": "https://github.com/ggerganov/llama.cpp/blob/master/LICENSE"
                    }
                }
            },
            "run": {
                "command": "python3 asr_to_llm.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# HoloHub Applications\n\nThis directory contains applications based on the Holoscan Platform.\nSome applications might require specific hardware and software packages which are described in the \nmetadata.json and/or README.md for each application.\n\n# Contributing to HoloHub Applications\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute applications.\n\n# HoloHub Application Organization Conventions\n\n## Required Conventions\n\nWe expect that an application contributed to HoloHub conforms to the following organization:\n\n- Each project must provide a `metadata.json` file reflecting several key components such as the application name and description, authors, dependencies, and the primary project language. \n- Each project must provide a `README` or `README.md` file.\n  - We strongly recommend that the project `README` file provides at least the information given in the [template README](./template/README.md.template), including the project description and a splash image.\n- Each project must be organized in its own subfolder under `holohub/applications/`.\n\nSee the [HoloHub application template](./template/) for example `README` and `metadata.json` documents to get started.\n\n## Recommended Conventions\n\nContributors may additionally opt to lay out their project structure in a way that conforms to HoloHub conventions in order to enable common infrastructure for their project, including streamlined build and run support in the [`dev_container`](../dev_container) and [`run`](../run) scripts and search support on the HoloHub landing page.\n\nIf your code does not adhere to these conventions, please set the field `manual_setup` to `true` in your project `metadata.json` file to opt out and indicate that your project is not eligible for streamlined infrastructure support.\n\nHoloHub recommended application convention is as follows:\n- Languages\n  - Project is either C++ or Python language\n  - If multiple language implementations are provided, each must be added to its own language subfolder as follows:\n```\napplications/\n  \u2514\u2500\u2500 my_project/\n        \u251c\u2500\u2500 cpp/\n        \u2502     \u251c\u2500\u2500 ...\n        \u2502     \u2514\u2500\u2500 ...\n        \u2514\u2500\u2500 python/\n              \u251c\u2500\u2500 ...\n              \u2514\u2500\u2500 ...\n```\n\n- Container Environment\n  - Project may provide its own container environment or opt to use the default HoloHub environment\n  - If the project specifies its own container:\n    - Default project environment must be named `Dockerfile`\n    - Project `Dockerfile` must be located at either:\n      - The same directory as `metadata.json`, or\n      - If the project defines a language directory (`cpp`, `python`), may provide at the project folder one level above, or\n      - The project may provide an alternative default Dockerfile path in `metadata.json`\n  - If the project does not specify a `Dockerfile` then the [default HoloHub `Dockerfile`](../Dockerfile) will be used\n\n- Build and Run Instructions\n  - Must provide a run command in `metadata.json` for `./run launch` to reference\n  - Must otherwise comply with `./dev_container build_and_run` command options for the default use case\n  - Advanced instructions may also be specified in the project README\n",
        "application_name": "asr_to_llm",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run asr_to_llm --language python"
    },
    {
        "metadata": {
            "name": "CV-CUDA: Basic Interoperability",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "CV-CUDA",
                "Computer Vision",
                "CV"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ],
                "libraries": [
                    {
                        "name": "cvcuda",
                        "version": "0.3.1-beta"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/cvcuda_basic --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Simple CV-CUDA application\n\nThis application demonstrates seamless interoperability between Holoscan tensors and CV-CUDA tensors. The image processing pipeline is just a simple flip of the video orientation.\n\nNote that the C++ version of this application currently requires extra code to handle conversion\nback and forth between CV-CUDA and Holoscan tensor types. On the Python side, the conversion is\ntrivial due to the support for the [DLPack Python\nspecification](https://dmlc.github.io/dlpack/latest/python_spec.html) in both CV-CUDA and Holoscan.\nWe provide two [operators](../../operators/cvcuda_holoscan_interop/README.md) to handle the\ninteroperability between CVCUDA and Holoscan tensors.\n\n# Using the docker file\n\nThis application requires a compiled version of [CV-CUDA](https://github.com/CVCUDA/CV-CUDA).\nFor simplicity a DockerFile is available. To generate the container run:\n\n```bash\n./dev_container build --docker_file ./applications/cvcuda_basic/Dockerfile\n```\n\nThe C++ version of the application can then be built by launching this container and using the provided `run` script.\n\n```bash\n./dev_container launch\n./run build cvcuda_basic\n```\n\n# Running the Application\n\nThis application uses the endoscopy dataset as an example. The `run build` command above will automatically download it. This application is then run inside the container.\n\n```bash\n./dev_container launch\n```\n\nThe Python version of the simple CV-CUDA pipeline example can be run via\n```\npython applications/cvcuda_basic/python/cvcuda_basic.py --data=/workspace/holohub/data/endoscopy\n```\n\nor using the run script\n\n```bash\n./run launch cvcuda_basic python\n```\n\nThe C++ version of the simple CV-CUDA pipeline example can then be run via\n```\n./build/applications/cvcuda_basic/cpp/cvcuda_basic --data=/workspace/holohub/data/endoscopy\n```\n\nor using the run script\n\n```bash\n./run launch cvcuda_basic cpp\n```\n",
        "application_name": "cvcuda_basic",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run cvcuda_basic --language cpp"
    },
    {
        "metadata": {
            "name": "CV-CUDA: Basic Interoperability",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "CV-CUDA",
                "Computer Vision",
                "CV"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ],
                "libraries": [
                    {
                        "name": "cvcuda",
                        "version": "0.3.1-beta"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/cvcuda_basic.py --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Simple CV-CUDA application\n\nThis application demonstrates seamless interoperability between Holoscan tensors and CV-CUDA tensors. The image processing pipeline is just a simple flip of the video orientation.\n\nNote that the C++ version of this application currently requires extra code to handle conversion\nback and forth between CV-CUDA and Holoscan tensor types. On the Python side, the conversion is\ntrivial due to the support for the [DLPack Python\nspecification](https://dmlc.github.io/dlpack/latest/python_spec.html) in both CV-CUDA and Holoscan.\nWe provide two [operators](../../operators/cvcuda_holoscan_interop/README.md) to handle the\ninteroperability between CVCUDA and Holoscan tensors.\n\n# Using the docker file\n\nThis application requires a compiled version of [CV-CUDA](https://github.com/CVCUDA/CV-CUDA).\nFor simplicity a DockerFile is available. To generate the container run:\n\n```bash\n./dev_container build --docker_file ./applications/cvcuda_basic/Dockerfile\n```\n\nThe C++ version of the application can then be built by launching this container and using the provided `run` script.\n\n```bash\n./dev_container launch\n./run build cvcuda_basic\n```\n\n# Running the Application\n\nThis application uses the endoscopy dataset as an example. The `run build` command above will automatically download it. This application is then run inside the container.\n\n```bash\n./dev_container launch\n```\n\nThe Python version of the simple CV-CUDA pipeline example can be run via\n```\npython applications/cvcuda_basic/python/cvcuda_basic.py --data=/workspace/holohub/data/endoscopy\n```\n\nor using the run script\n\n```bash\n./run launch cvcuda_basic python\n```\n\nThe C++ version of the simple CV-CUDA pipeline example can then be run via\n```\n./build/applications/cvcuda_basic/cpp/cvcuda_basic --data=/workspace/holohub/data/endoscopy\n```\n\nor using the run script\n\n```bash\n./run launch cvcuda_basic cpp\n```\n",
        "application_name": "cvcuda_basic",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run cvcuda_basic --language python"
    },
    {
        "metadata": {
            "name": "Holoscan XR Demo Application",
            "authors": [
                {
                    "name": "Andreas Heumann",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Connor Smith",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Cristiana Dinea",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Tom Birdsong",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Antonio Ospite",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Jiwen Cai",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Jochen Stier",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Korcan Hussein",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Robbie Bridgewater",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial release"
            },
            "dockerfile": "applications/volume_rendering_xr/Dockerfile",
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Rendering",
                "OpenXR",
                "Mixed",
                "Reality",
                "XR"
            ],
            "ranking": 2,
            "dependencies": {
                "hardware": [
                    {
                        "name": "Magic Leap 2",
                        "description": "Magic Leap 2 mixed reality headset",
                        "url": "https://www.magicleap.com/magic-leap-2"
                    }
                ],
                "libraries": [
                    {
                        "name": "windrunner",
                        "description": "Magic Leap OpenXR native backend",
                        "version": "1.11.73",
                        "license": "Magic Leap 2 Software Agreement",
                        "license-url": "https://www.magicleap.com/software-license-agreement-ml2"
                    },
                    {
                        "name": "Magic Leap Remote Viewer apk",
                        "version": "1.11.64",
                        "license": "Magic Leap 2 Software Agreement",
                        "license-url": "https://www.magicleap.com/software-license-agreement-ml2"
                    }
                ]
            },
            "run": {
                "command": "ml_start.sh ${ML_START_OPTIONS} && ml_pair.sh && <holohub_app_bin>/xr_hello_holoscan",
                "workdir": "<holohub_app_bin>"
            }
        },
        "readme": "# XR \"Hello Holoscan\"\n\nThis application provides a simple scene demonstrating mixed reality viewing with Holoscan SDK.\n\n![Stereo scene view](doc/screenshot.png)\n\n## Background\n\nWe created this test application as part of a collaboration between the Magic Leap and NVIDIA Holoscan teams.\nSee the [`volume_rendering_xr`](/applications/volume_rendering_xr/) application for a demonstration of medical viewing\nin XR with Holoscan SDK.\n\n## Description\n\nThe application provides a blueprint for how to set up a mixed reality scene for viewing with Holoscan SDK and\nHoloHub components.\n\nThe mixed reality demonstration scene includes:\n- Static components such as scene axes and cube primitives;\n- A primitive overlay on the tracked controller input;\n- A static UI showcasing sensor inputs and tracking.\n\n## Getting Started\n\nRefer to the [`volume_rendering_xr` README](/applications/volume_rendering_xr/README.md#prerequisites) for details on hardware, firmware, and software prerequisites.\n\nTo run the application, run the following command in the HoloHub folder on your host machine:\n```bash\n./dev_container build_and_run xr_hello_holoscan\n```\n\nTo pair your Magic Leap 2 device with the host, open the QR Reader application in the ML2 headset and scan the QR code printed in console output on the host machine.\n\n## Frequently Asked Questions\n\n### Can I test the application without a Magic Leap 2 device?\n\nYes, a debug GUI not requiring a headset is installed inside the application container by default. Follow the steps\nbelow to launch the debug GUI and run the application:\n\n```bash\n# Build and launch the container\n./dev_container build --img holohub:xr_hello_holoscan --docker_file ./applications/volume_rendering_xr/Dockerfile\n./dev_container launch --img holohub:xr_hello_holoscan\n\n# Build the application\n./run build xr_hello_holoscan\n\n# Launch the debug GUI and the application\nexport ML_START_OPTIONS=\"debug\"\n./run launch xr_hello_holoscan\n```\n\nThe ImGui debug application will launch. Click and slide the position entries to adjust your view of the scene.\n\n![Hello XR Debug GUI](doc/debug.png)\n",
        "application_name": "xr_hello_holoscan",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run xr_hello_holoscan --language cpp"
    },
    {
        "metadata": {
            "name": "Medical Image viewer in XR",
            "authors": [
                {
                    "name": "Andreas Heumann",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Connor Smith",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Cristiana Dinea",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Tom Birdsong",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Antonio Ospite",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Jiwen Cai",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Jochen Stier",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Korcan Hussein",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Robbie Bridgewater",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial release",
                "0.1": "Update for Magic Leap 2 firmware v1.5.0",
                "0.2": "Update for Magic Leap 2 firmware v1.6.0",
                "1.0": "Enhance interactivity and update for Holoscan SDK v2.0.0 deployment stack"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Rendering",
                "OpenXR",
                "Mixed",
                "Reality",
                "XR"
            ],
            "ranking": 2,
            "dependencies": {
                "hardware": [
                    {
                        "name": "Magic Leap 2",
                        "description": "Magic Leap 2 mixed reality headset",
                        "url": "https://www.magicleap.com/magic-leap-2"
                    }
                ],
                "libraries": [
                    {
                        "name": "windrunner",
                        "description": "Magic Leap OpenXR native backend",
                        "version": "1.11.73",
                        "license": "Magic Leap 2 Software Agreement",
                        "license-url": "https://www.magicleap.com/software-license-agreement-ml2"
                    },
                    {
                        "name": "Magic Leap Remote Viewer apk",
                        "version": "1.11.64",
                        "license": "Magic Leap 2 Software Agreement",
                        "license-url": "https://www.magicleap.com/software-license-agreement-ml2"
                    }
                ]
            },
            "run": {
                "command": "if [ -v ML_START_OPTIONS ]; then ml_start.sh ${ML_START_OPTIONS}; else ml_start.sh debug; fi && ml_pair.sh && <holohub_app_bin>/volume_rendering_xr --config <holohub_app_source>/configs/ctnv_bb_er.json --density <holohub_data_dir>/volume_rendering_xr/highResCT.mhd --mask <holohub_data_dir>/volume_rendering_xr/smoothmasks.seg.mhd",
                "workdir": "<holohub_app_bin>"
            }
        },
        "readme": "# Medical Image Viewer in XR\n\n![Volume Rendering Screenshot](./doc/stereo_skeleton.png)\n\n## Description\n\nWe collaborated with Magic Leap on a proof of concept mixed reality viewer for medical imagery built on the Holoscan platform.\n\nMedical imagery is one of the fastest-growing sources of data in any industry. When we think about typical diagnostic imaging, X-ray, CT scans, and MRIs come to mind. X-rays are 2D images, so viewing them on a lightbox or, if they\u2019re digital, a computer, is fine. But CT scans and MRIs are 3D. They\u2019re incredibly important technologies, but our way of interacting with them is flawed. This technology helps physicians in so many ways, from training and education to making more accurate diagnoses and ultimately to planning and even delivering more effective treatments.\n\nYou can use this viewer to visualize a segmented medical volume with a mixed reality device.\n\n## Prerequisites\n\n### Host Machine\n\nReview the [HoloHub README document](/README.md#prerequisites) for supported platforms and software requirements.\n\nThe application supports x86_64 or IGX dGPU platforms. IGX iGPU, AGX, and RHEL platforms are not fully tested at this time.\n\n#### Magic Leap 2 Device\n\nThe following packages and applications are required to run remote rendering with a Magic Leap 2 device:\n\n| Requirement | Platform | Version | Source |\n|--|------|---------|--|\n| Magic Leap Hub | Windows or macOS PC | latest | [Magic Leap Website](https://ml2-developer.magicleap.com/downloads) |\n| Headset Firmware | Magic Leap 2 | v1.6.0 | Magic Leap Hub |\n| Headset Remote Rendering Viewer (.apk) | Magic Leap 2 | [1.11.64](https://thelab.magicleap.cloud/packages_mlhub/artifacts/com.magicleap.remote_render/1.11.64/ml_remote_viewer.apk) | Magic Leap Download Link |\n| Windrunner OpenXR Backend | HoloHub Container | 1.11.74 | Included in Container |\n| Magic Leap 2 Pro License | | | Magic Leap |\n\nRefer to the Magic Leap 2 documentation for more information:\n- [Updating your device with Magic Leap Hub](https://www.magicleap.care/hc/en-us/articles/5341445649805-Updating-Your-Device);\n- [Installing `.apk` packages with Magic Leap Hub](https://developer-docs.magicleap.cloud/docs/guides/developer-tools/ml-hub/ml-hub-package-manager/)\n\n## Quick Start\n\nRun the following command in the top-level HoloHub folder to build and run the host application:\n\n```bash\n./dev_container build_and_run volume_rendering_xr\n```\n\nA QR code will be visible in the console log. Refer to Magic Leap 2 [Remote Rendering Setup documentation](https://developer-docs.magicleap.cloud/docs/guides/remote-rendering/remote-rendering/#:~:text=Put%20on%20the%20Magic%20Leap,headset%20by%20looking%20at%20it.&text=The%20QR%20code%20launches%20a,Click%20Continue.) to pair the host and device in preparation for remote viewing. Refer to the [Remote Viewer](#starting-the-magic-leap-2-remote-viewer) section to regenerate the QR code as needed, or to use the local debugger GUI in place of a physical device.\n\nThe application supports the following hand or controller interactions by default:\n- **Translate**: Reach and grab inside the volume with your hand or with the controller trigger to move the volume.\n- **Scale**: Grab any face of the bounding box and move your hand or controller to scale the volume.\n- **Rotate**: Grab any edge of the bounding box and move your hand or controller to rotate the volume.\n- **Crop**: Grab any vertex of the bounding box and move your hand or controller to translate the cropping planes.\n\n## Advanced Setup\n\nYou can use the `--dryrun` option to see the individual commands run by the quick start option above:\n```\n./dev_container build_and_run volume_rendering_xr --dryrun\n```\n\nAlternatively, follow the steps below to set up the interactive container session.\n\n### Build the Container\n\nRun the following commands to build and enter the interactive container environment:\n```bash\n./dev_container build --img holohub:volume_rendering_xr --docker_file ./applications/volume_rendering_xr/Dockerfile # Build the dev container\n./dev_container launch --img holohub:volume_rendering_xr # Launch the container\n```\n\n### Build the Application\n\nInside the container environment, build the application:\n```bash\n./run build volume_rendering_xr # Build the application\n```\n\n### Run the Application\n\nInside the container environment, start the application:\n```bash\nexport ML_START_OPTIONS=<\"\"/\"debug\"> # Defaults to \"debug\" to run XR device simulator GUI\n./run launch volume_rendering_xr\n```\n\n### Deploying as a Standalone Application\n\n`volume_rendering_xr` can be packaged in a self-contained release container with datasets and binaries.\n\nTo build the release container:\n```bash\n# Generate HoloHub `volume_rendering_xr` installation in the \"holohub/install\" folder\n./dev_container launch --img holohub:volume_rendering_xr -c ./run build volume_rendering_xr --configure-args \"-DCMAKE_INSTALL_PREFIX:PATH=/workspace/holohub/install\"\n./dev_container launch --img holohub:volume_rendering_xr -c cmake --build ./build --target install\n\n# Copy files into a release container\n./dev_container build --img holohub:volume_rendering_xr_rel --docker_file ./applications/volume_rendering_xr/scripts/Dockerfile.rel --base_img nvcr.io/nvidia/cuda:12.4.1-runtime-ubuntu22.04\n```\n\nTo run the release container, first create the container startup script:\n```bash\ndocker run --rm holohub:volume_rendering_xr_rel > ./render-volume-xr\nchmod +x ./render-volume-xr\n```\n\nThen execute the script to start the Windrunner service and the app:\n```bash\n./render-volume-xr\n```\n\nFor more options, e.g. list available datasets or to select a different dataset, type\n```bash\n./render-volume-xr --help\n```\n\nOptions not recognized by the render-volume-xr script are forwarded to the application.\n\n## Additional Notes\n\n### Supported Formats\n\nThis application loads static volume files from the local disk. See HoloHub [`VolumeLoaderOp`](/operators/volume_loader/README.md#supported-formats) documentation for supported volume formats and file conversion tools.\n\n### Launch Options\n\nUse the `--extra-args` to see all options, including how to specify a different dataset or configuration file to use.\n```bash\n./run launch volume_rendering_xr --extra_args --help\n...\nHoloscan OpenXR volume renderer.Usage: /workspace/holohub/build/applications/volume_rendering_xr/volume_rendering_xr [options]\nOptions:\n  -h, --help                            Display this information\n  -c <FILENAME>, --config <FILENAME>    Name of the renderer JSON configuration file to load (default '/workspace/holoscan-openxr/data/volume_rendering/config.json')\n  -d <FILENAME>, --density <FILENAME>   Name of density volume file to load (default '/workspace/holoscan-openxr/data/volume_rendering/highResCT.mhd')\n  -m <FILENAME>, --mask <FILENAME>      Name of mask volume file to load (default '/workspace/holoscan-openxr/data/volume_rendering/smoothmasks.seg.mhd')\n```\n\nTo use a new dataset with the application, mount its volume location from the host machine when launching the container and pass all required arguments explicitly to the executable:\n```bash\n./dev_container launch --as_root --img holohub:openxr-dev --add-volume /host/path/to/data-dir\n>>> ./build/applications/volume_rendering_xr/volume_rendering_xr \\\n      -c /workspace/holohub/data/volume_rendering/config.json \\\n      -d /workspace/volumes/path/to/data-dir/dataset.nii.gz \\\n      -m /workspace/volumes/path/to/data-dir/dataset.seg.nii.gz\n```\n\n### Starting the Magic Leap OpenXR runtime\n\nOpenXR runtimes are implementations of the OpenXR API that allow the Holoscan XR operators to create XR sessions and render content. The Magic Leap OpenXR runtime including a CLI are by default installed in the dev container. __From a terminal inside the dev container__ you can execute the following scripts:\n\n```\nml_start.sh\n```\nstarts the OpenXR runtime service. After executing this command, the remote viewer on the Magic Leap device should connect to this runtime service. If not, then you still have to pair the device with the host computer running the Holoscan application.\n\nFor rapid iteration without a Magic Leap device, pass the argument `debug` to `ml_start.sh` i.e.\n```\nml_start.sh debug\n```\nThis will enable a debug view on your computer showing what the headset would see. You may click into this window and navigate with the keyboard and mouse to manipulate the virtual head position.\n\nIf you connect an ML2 while the debug view is active, you can continue to view the content on the debug view but can no longer adjust the virtual position, as the real position is used instead.\n\n```\nml_pair.sh\n```\ndisplays a QR code used to pair the device with the host. Start the QR code reader App on the device and scan the QR code displayed in the terminal. Note that the OpenXR runtime has to have been started using the __ml_start__ command in order for the paring script to execute correctly.\n\n```\nml_stop.sh\n```\nstops the OpenXR runtime service.\n\n### Starting the Magic Leap 2 Remote Viewer\n\nWhen using a Magic Leap 2 device for the first time or after a software upgrade, the device must be provided with the IP address of the host running the OpenXR runtime. From a terminal inside the dev container run the\n\n```\nml_pair.sh\n```\n\ncommand, which will bring up a QR code that has to be scanned using the __QR Code App__ on the Magic Leap 2 device. Once paired with the host, the device  will automatically start the remote viewer which will then prompt you to start an OpenXR application on the host. Any time thereafter, start the remote viewer via the App menu.\n\n### Developing with a Different OpenXR Backend\n\n`volume_renderer_xr` is an OpenXR compatible application. The Magic Leap Remote Rendering runtime is installed in the application container by default, but a compatible runtime can be used if appropriate to your use case. See [https://www.khronos.org/openxr/](https://www.khronos.org/openxr/) for more information on conformant OpenXR runtimes.\n\n### Volume Rendering\n\nThe application carries out volume rendering via the HoloHub [`volume_renderer`](../../operators/volume_renderer/) operator,\nwhich in turn wraps the NVIDIA [ClaraViz](https://github.com/NVIDIA/clara-viz) rendering project. ClaraViz JSON configurations provided in the [config folder](./configs/) are available for specifying default scene parameters.\n\nSee [`volume_renderer` Configuration section](../../operators/volume_renderer/README.md#configuration) for details on\nmanipulating configuration values, along with [how to create a new configuration file](../../operators/volume_renderer/README.md#creating-a-configuration-file) to fit custom data.\n\n### Troubleshooting\n\nPlease verify that you are building from the latest HoloHub `main` branch before reviewing troubleshooting steps.\n\n```sh\ngit checkout main\n```\n\n#### Libraries are missing when building the application (Vulkan, OpenXR, etc)\n\nThis error may indicate that you are building inside the default HoloHub container instead of the expected `volume_rendering_xr` container.\nReview the [build steps](#building-the-application) and ensure that you have launched the container with the appropriate\n`dev_container --img` option.\n\n#### Unexpected CMake errors\n\nYou may need to clear your CMake build cache. See the HoloHub [Cleaning](/README.md#cleanup) section for instructions.\n\n#### \"Seccomp\" Errors\n\nThe Magic Leap Windrunner OpenXR backend and remote rendering host application use seccomp to limit syscalls on Linux platforms.\nYou can exempt individual syscalls for local development by adding them to the [application syscall whitelist](thirdparty/magicleap/seccomp_whitelist.cfg).\n\n#### Debug GUI does not appear\n\nThe `./run launch volume_rendering_xr` command initializes the Magic Leap Windrunner debug GUI by default. If you do not see\nthe debug GUI appear in your application, or if the application appears to stall with no further output after the pairing QR\ncode appears, try any of the following:\n\n1. Manually set the `ML_START_OPTIONS` environment variable so that `run launch` initializes with the debug view:\n```sh\nexport ML_START_OPTIONS=\"debug\"\n```\n\n2. Follow [Advanced Setup Instructions](#advanced-setup) and add the `--as_root` option to launch the container with root permissions.\n```sh\n./dev_container launch --img holohub:volume_rendering_xr --as_root\n```\n\n3. Clear the build cache and any home cache folders in the HoloHub workspace.\n```sh\n./run clear_cache\nrm -rf .cache/ .cmake/ .config/ .local/\n```",
        "application_name": "volume_rendering_xr",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run volume_rendering_xr --language cpp"
    },
    {
        "metadata": {
            "name": "cuda_quantum",
            "authors": [
                {
                    "name": "Sean Huver",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0.0": "Initial release of cuda_quantum application"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Quantum Computing",
                "CUDA",
                "VQE"
            ],
            "ranking": 4,
            "dependencies": {
                "cuda_quantum": "^0.4.0"
            },
            "run": {
                "command": "pip install -r requirements.txt && python3 cuda_quantum.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Hybrid-Computing Sample App - CUDA Quantum Variational Quantum Eigensolver (VQE) Application\n\n## Variational Quantum Eigensolver (VQE)\nThe Variational Quantum Eigensolver (VQE) is a quantum algorithm designed to approximate the ground state energy of quantum systems. This energy, represented by what is called the Hamiltonian of the system, is central to multiple disciplines, including drug discovery, material science, and condensed matter physics. The goal of VQE is to find the state that minimizes the expectation value of this Hamiltonian, which corresponds to the ground state energy.\n\nAt its core, VQE is a lighthouse example of the synergy between classical and quantum computing, requiring them both to tackle problems traditionally deemed computationally intractable. Even in the current landscape where fault-tolerant quantum computing\u2014a stage where quantum computers are resistant to errors\u2014is not yet realized, VQE is seen as a practical tool. This is due to its design as a 'near-term' algorithm, built to operate on existing noisy quantum hardware. \n\n## Key Components of VQE\n1. **Hamiltonian**: This represents the total energy of the quantum system, which is known ahead of time. In VQE, we aim to find the lowest eigenvalue (ground state energy) of this Hamiltonian.\n  \n2. **Ansatz (or trial wavefunction)**: The ansatz is the initial guess for the state of the quantum system, represented by a parameterized quantum circuit. It's crucial for this state to be a good representation, as the quality of the ansatz can heavily influence the final results. VQE iteratively refines the parameters of this ansatz to approximate the true ground state of the Hamiltonian.\n\n## VQE Mechanism\nThe VQE operates by employing a hybrid quantum-classical approach:\n\n1. **Quantum Circuit Parameterization**: VQE begins with a parameterized quantum circuit, effectively serving as an initial guess or representation of the system's state.\n2. **Evaluation and Refinement**: The quantum system's energy is evaluated using the current quantum circuit parameters. Classical optimization algorithms then adjust these parameters in a quest to minimize the energy.\n3. **Iterative Process**: The combination of quantum evaluation and classical refinement is iterative. Over multiple cycles, the parameters are tuned to get increasingly closer to the true ground state energy.\n\n## Integration with Holoscan and CUDA Quantum\n- **NVIDIA Holoscan SDK**: The Holoscan SDK is designed for efficient handling of high-throughput, low-latency GPU tasks. Within the context of VQE, the Holoscan SDK facilitates the rapid classical computations necessary for parameter adjustments and optimization. The `ClassicalComputeOp` in the provided code sample is an example of this SDK in action, preparing the quantum circuits efficiently.\n- **CUDA Quantum**: CUDA Quantum is a framework that manages hybrid quantum-classical workflows. For VQE, CUDA Quantum processes quantum data and executes quantum operations. The `QuantumComputeOp` operator in the code uses the cuQuantum simulator backend, but the user may optionally switch out the simulator for a real quantum cloud backend provided by either IonQ or Quantinuum ([see CUDA Quantum backend documentation](https://nvidia.github.io/cuda-quantum/latest/using/hardware.html#)).\n\nHoloscan ensures swift and efficient classical computations, while CUDA Quantum manages the quantum components with precision.\n\n## Usage\n\nTo run the application, you need to have CUDA Quantum, Qiskit, and Holoscan installed. You also need an IBM Quantum account to use their quantum backends.\n\n1. Clone the repository and navigate to the `cuda_quantum` directory containing.\n\n2. Install the requirements `pip install -r requirements.txt`\n\n3. Either use or replace the `'hamiltonian'` in `cuda_quantum.yaml` dependent on the physical system you wish to model.\n\n4. Run the application with the command `python cuda_quantum.py`.\n\n## Operators\n\nThe application uses three types of operators:\n\n- `ClassicalComputeOp`: This operator performs classical computations. It also creates a quantum kernel representing the initial ansatz, or guess of the state of the system, and a Hamiltonian.\n\n- `QuantumComputeOp`: This operator performs quantum computations. It uses the quantum kernel and Hamiltonian from `ClassicalComputeOp` to iterate towards the ground state energy and parameter using VQE.\n\n- `PrintOp`: This operator prints the result from `QuantumComputeOp`.\n\n## Operator Connections\n\nThe operators are connected as follows:\n\n```mermaid\nflowchart LR\n    ClassicalComputeOp --> QuantumComputeOp\n    QuantumComputeOp --> PrintOp\n```\n\n`ClassicalComputeOp` sends the quantum kernel and Hamiltonian to `QuantumComputeOp`, which computes the energy and parameter and sends the result to `PrintOp`.",
        "application_name": "cuda_quantum",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run cuda_quantum --language python"
    },
    {
        "metadata": {
            "name": "DDS Video",
            "authors": [
                {
                    "name": "Ian Stewart",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DDS",
                "RTI Connext",
                "Video",
                "Shapes"
            ],
            "ranking": 2,
            "dependencies": {
                "packages": [
                    {
                        "name": "RTI Connext",
                        "author": "Real-Time Innovations",
                        "license": "Closed",
                        "version": "6.1.2",
                        "url": "https://www.rti.com/products"
                    }
                ]
            },
            "run": {
                "command": "./dds_video",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# DDS Video Application\n\nThe DDS Video application demonstrates how video frames can be written to or\nread from a DDS databus in order to provide flexible integration between\nHoloscan applications and other applications (using Holoscan or not) via DDS.\n\nThe application can be run as either a publisher or as a subscriber. In either case,\nit will use the [VideoFrame](../../operators/dds/video/VideoFrame.idl) data topic\nregistered by the `DDSVideoPublisherOp` or `DDSVideoSubscriberOp` operators in order\nto write or read the video frame data to/from the DDS databus, respectively.\n\nWhen run as a publisher, the source for the input video frames will come from an\nattached V4L2-compatible camera via the `V4L2VideoCaptureOp` operator.\n\nWhen run as a subscriber, the application will use Holoviz to render the received\nvideo frames to the display. In addition to the video stream, the subscriber\napplication will also subscribe to the `Square`, `Circle`, and `Triangle` topics\nas used by the [RTI Shapes Demo](https://www.rti.com/free-trial/shapes-demo).\nAny shapes received by this subscriber will also be overlaid on top of the\nHoloviz output.\n\n![DDS Video Application Workflow](docs/workflow_dds_video_app.png)\n\n## Building the Application\n\nThis application requires [RTI Connext](https://content.rti.com/l/983311/2024-04-30/pz1wms)\nbe installed and configured with a valid RTI Connext license prior to use.\nTo build on an IGX devkit (using the `armv8` architecture), follow the\n[instructions to build Connext DDS applications for embedded Arm targets](https://community.rti.com/kb/how-do-i-create-connext-dds-application-rti-code-generator-and-build-it-my-embedded-target-arm)\nup to step 5 (Installing Java and setting JREHOME).\n\nTo build the application, the `RTI_CONNEXT_DDS_DIR` CMake variable must point to\nthe installation path for RTI Connext. This can be done automatically by setting\nthe `NDDSHOME` environment variable to the RTI Connext installation directory\n(such as when using the RTI `setenv` scripts), or manually at build time, e.g.:\n\n```sh\n$ ./run build dds_video --configure-args -DRTI_CONNEXT_DDS_DIR=~/rti/rti_connext_dds-6.1.2\n```\n\n### Building with a Container\n\nDue to the license requirements of RTI Connext it is not currently supported to\ninstall RTI Connext into a development container. Instead, Connext should be\ninstalled onto the host as above and then the development container can be\nlaunched with the RTI Connext folder mounted at runtime. To do so, ensure that\nthe `NDDSHOME` environment variable is set and use the following:\n\n```sh\n./dev_container launch --docker_opts \"-v $NDDSHOME:/opt/dds -e NDDSHOME=/opt/dds\"\n```\n\n## Running the Application\n\nBoth a publisher and subscriber process must be launched to see the result of\nwriting to and reading the video stream from DDS, respectively.\n\nTo run the publisher process, use the `-p` option:\n\n```sh\n$ ./run launch dds_video --extra_args \"-p\"\n```\n\nTo run the subscriber process, use the `-s` option:\n\n```sh\n$ ./run launch dds_video --extra_args \"-s\"\n```\n\nIf running the application generates an error about `RTI Connext DDS No Source\nfor License information`, ensure that the RTI Connext license has either been\ninstalled system-wide or the `NDDSHOME` environment variable has been set to\npoint to your user's RTI Connext installation path.\n\nNote that these processes can be run on the same or different systems, so long as they\nare both discoverable by the other via RTI Connext. If the processes are run on\ndifferent systems then they will communicate using UDPv4, for which optimizations have\nbeen defined in the default `qos_profiles.xml` file. These optimizations include\nincreasing the buffer size used by RTI Connext for network sockets, and so the systems\nrunning the application must also be configured to increase their maximum send and\nreceive socket buffer sizes. This can be done by running the `set_socket_buffer_sizes.sh`\nscript within this directory:\n\n```sh\n$ ./set_socket_buffer_sizes.sh\n```\n\nFor more details, see the [RTI Connext Guide to Improve DDS Network Performance on Linux Systems](https://community.rti.com/howto/improve-rti-connext-dds-network-performance-linux-systems)\n\nThe QoS profiles used by the application can also be modified by editing the\n`qos_profiles.xml` file in the application directory. For more information about modifying\nthe QoS profiles, see the [RTI Connext Basic QoS](https://community.rti.com/static/documentation/connext-dds/6.1.2/doc/manuals/connext_dds_professional/getting_started_guide/cpp11/intro_qos.html)\ntutorial or the [RTI Connext QoS Reference Guide](https://community.rti.com/static/documentation/connext-dds/6.1.2/doc/manuals/connext_dds_professional/qos_reference/index.htm).\n\n### Publishing Shapes from the RTI Shapes Demo\n\nThe [RTI Shapes Demo](https://www.rti.com/free-trial/shapes-demo) can be used to\npublish shapes which are then read and overlaid onto the video stream by this\napplication. However, the domain participant QoS used by this application is not\ncompatible with the default DDS QoS settings, so the RTI Shapes Demo must be\nconfigured to use the QoS settings provided by this application.  To do this,\nfollow these steps:\n\n1. Launch the RTI Shapes Demo\n2. Select `Controls`, then `Configuration` from the menu bar\n3. Click `Stop` to disable the default domain participant\n4. Click `Manage QoS`\n5. Click `Add` then navigate to and select the `qos_profiles.xml` file in this\n   application's directory.\n6. Click `OK` to close the `Manage QoS` window.\n7. In the `Choose the profile` drop-down, select `HoloscanDDSTransport::SHMEM+LAN`\n8. Click `Start` to join the domain.\n\nOnce the Shapes Demo is running and has joined the domain of a running\n`dds_video` subscriber, shapes published by the application should be\nrendered on top of the subscriber's video stream.\n",
        "application_name": "dds_video",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run dds_video --language cpp"
    },
    {
        "metadata": {
            "name": "Intel RealSense Camera Visualizer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [],
            "ranking": 0,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/realsense_visualizer",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# RealSense Visualizer\n\nVisualizes frames captured from an Intel RealSense camera.\n![](screenshot.png)<br>\n\n# Build and Run\nThis application requires an Intel RealSense camera.\n\nAt the top level of the holohub run the following command:\n\n```bash\n./dev_container build_and_run realsense_visualizer\n```\n",
        "application_name": "realsense_visualizer",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run realsense_visualizer --language cpp"
    },
    {
        "metadata": {
            "name": "Yolo Detection Application",
            "authors": [
                {
                    "name": "Meiran Peng",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Yolo",
                "bounding box"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.22.3"
                    },
                    {
                        "name": "cupy",
                        "version": "11.3"
                    }
                ]
            },
            "run": {
                "command": "python3 yolo_detection --data=./ --source=replayer --video_dir=./example_video",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Holoscan-Yolo\nThis project is aiming to provide basic guidance to deploy Yolo-based model to Holoscan SDK as \"Bring Your Own Model\"\n\nThe reference application's pipeline is the same as [Ultrasound Segmentation Application in Holoscan SDK](https://github.com/nvidia-holoscan/holohub/tree/main/applications/ultrasound_segmentation).\n\n\nThe operators applied in this operation including:\n- Video Stream Replayer (replayer as source) | AJASourceOp (AJA card is as source)\n- Format Converter (float32 + resize)\n- TensorRT Inference\n- Detection PostProcessor (customized Python Operator to extract Bounding Box)\n- HoloViz\n\nThis project includes below procedures:\n1. [Prerequisition](#prerequisition)\n    1.1 Prepare Holoscan SDK env\n    1.2 Prepare dependent libraries inside container.\n2. [Deploy Procedures](#procedures)\n    2.1 [Step1: prepare the model with NMS](#step-1-prepare-the-model-with-nms-layer-and-nms-plugin) depend on pytorch env, can be done outside Holoscan SDK, refer each models' installation env section.     \n    2.2 [Step2: Deployment](#step-2-deployment)\n        - Prepare Env\n        - Update Model with NHWC as input\n        - Prepare test video with gxf support format\n        - Update application with py/yaml\n        - Prepare working folder\n        - Run the Application.\n\nThis repo takes yolo v7 as an example.\n\n## Dependencies Repos\n- Holoscan SDK: https://github.com/nvidia-holoscan/holoscan-sdk\n- Yolo v7 repo: https://github.com/WongKinYiu/yolov7 \n- Yolo v8 repo: https://github.com/ultralytics/ultralytics \n- Yolo v8 export repo: https://github.com/triple-Mu/YOLOv8-TensorRT \n\n\n## Prerequisition\n- Holoscan environment:\n    - NGC container: [Holoscan container image on NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/containers/holoscan)\n\n- Install below libraries inside container via, or skip it to [Step 2: Deployment](#step-2-deployment)\n    ```\n        pip install onnxruntime\n        pip install nvidia-pyindex\n        pip install onnx-graphsurgeon\n        apt update\n        apt install ffmpeg\n    ```\n    - onnxruntime, nvidia-pyindex,onnx-graphsurgeon are needed for [graph_surgeon.py](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#graph_surgeonpy) scripts to do onnx model conversion. Since when converting a model from PyTorch to ONNX, it is likely that the input of the model is in the form NCHW (batch, channels, height, width), and needs to be converted to NHWC (batch, height, width, channels). \n    - cupy is used in custom operators for performance improvements\n    - numpy is used in script to convert video in [Step 2](#step-2-deployment)\n\n## Procedures\n### Step 1: Prepare the Model with NMS Layer and NMS Plugin\n- [Yolo_v8](https://github.com/triple-Mu/YOLOv8-TensorRT)\n    - ```git clone https://github.com/triple-Mu/YOLOv8-TensorRT.git```\n    - Installation dependent package as described in this repo. Recommend to use latest [PyTorch docker](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) for model export. \n    - Export the model\n        ```\n            wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt\n            python3 export-det.py --weights yolov8s.pt --iou-thres 0.65 --conf-thres 0.25 --topk 100 --opset 11 --sim --input-shape 1 3 640 640 --device cuda:0\n        ```\n    - Check the model via [netron](https://netron.app/) shall include EfficientNMS_TRT layer with output ```num_dets```, ```bboxes```, ```scores```, and ```labels```\n\n- [Yolo_v7](https://github.com/WongKinYiu/yolov7)\n    - ```git clone https://github.com/WongKinYiu/yolov7.git```\n    - Prepare env, docker recommended as [yolo v7 installation](https://github.com/WongKinYiu/yolov7#installation)\n    - Export the model\n        ```\n        wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\n        python3 export.py --weights ./yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640\n        ```\n        NOTE: the ```end2end``` parameter will output the the end to end model with NMS Layer and Plugin. Other parameters pls refer detail information via [link](https://github.com/WongKinYiu/yolov7/blob/main/export.py#L19)\n    - Check the model via [netron](https://netron.app/) shall include EfficientNMS_TRT layer with output ```num_dets```, ```det_boxes```, ```det_scores```, and ```det_classes```\n\nNOTE: The output name maybe different for different export models, pls correct yolo_detecton.yaml section \"output_binding_names\" in detection_inference part accordingly.\n\n### Step 2: Deployment \n- ```git clone ``` this repo to your local folder, e.g. ./holohub\n\n- ```cd ./holohub/applications/yolo_model_deployment```\n\n- Copy the onnx model with NMS in Step 1 to this application folder. (The onnx model name in this example is yolov8s.onnx for yolo v8 model, and yolov7-tiny.onnx for yolo v7 model)\n\n- Run the Holoscan container via\n    ```\n    # Update the ngc container image path as needed\n    export NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v0.6.0\"\n\n    # DISPLAY env may be different due to different settings,\n    # try DISPLAY=:1 if failure with \"Failed to open display :0\"\n    export DISPLAY=:0\n    xhost +local:docker\n\n    # Find the nvidia_icd.json file which could reside at different paths\n    # Needed due to https://github.com/NVIDIA/nvidia-container-toolkit/issues/16\n    nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f 2>/dev/null | grep .) || (echo \"nvidia_icd.json not found\" >&2 && false)\n\n    # Run the container\n    docker run -it --rm --net host \\\n    --runtime=nvidia \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\n    -v $nvidia_icd_json:$nvidia_icd_json:ro \\\n    -v ${PWD}:/holohub-yolo \\\n    -w /holohub-yolo \\\n    -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \\\n    -e DISPLAY=$DISPLAY \\\n    ${NGC_CONTAINER_IMAGE_PATH}\n    ```      \n\n- Install necessary libraries inside the container via:\n    _This section is intended for the initial preparation of the model and video, and can be skipped once the model has been prepared._   \n    ```\n    apt update\n    apt install ffmpeg\n    pip3 install --upgrade setuptools\n\n    cat requirement.txt | xargs -n 1 -L 1 pip3 install \n    ```     \n\n- Update the model with input from NCHW format to NHWC format inside the container    \n    _This section is intended for the initial preparation of the model and video, and can be skipped once the model has been prepared._      \n\n    If you are converting your model from PyTorch to ONNX, chances are your input is NCHW and will need to be converted to NHWC. We provide an example [transformation script on Github](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#graph_surgeonpy) named ```graph_surgeon.py``` to do such conversion. You may need to modify the dimensions as needed before modifying your model. For this example, use below command to do the conversion. \n\n    ```\n    python3 ./scripts/graph_surgeon_yolo.py [yolov8s.onnx] [yolov8-nms-update.onnx]\n    ```\n    NOTE: the yolov8s.onnx refer to input onnx model name, yolov8-nms-update.onnx is the updated model which will be used in yolo application for inference. The yolov8-nms-update.onnx will be used in yolo_detection.py, if the onnx model name changed, pls update yolo_detection.py ```self.model_file_path``` accordingly.\n\n- Prepare the Video with gxf format inside the container    \n    _This section is intended for the initial preparation of the model and video, and can be skipped once the model has been prepared._ \n    \n    Video files need to be converted into a GXF replayable tensor format to be used as stream inputs. This step has already been done for the sample applications. To do so for your own video data, we provide a [utility script on GitHub](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) named ```convert_video_to_gxf_entities.py```. This script should yield two files in .gxf_index and .gxf_entities formats, which can be used as inputs with Holoscan.\n    \n    Follow below procedures to convert the video. \n\n    1. Copy your video to the working directory. In this example to /holohub-yolo/example_video   \n    2. Take cars.mp4 from [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_cars_video) as an example. Download it to  /holohub-yolo/example_video\n        ```\n        git clone https://github.com/nvidia-holoscan/holoscan-sdk.git\n        cd ./example_video\n        ffmpeg -i cars.mp4 -pix_fmt rgb24 -f rawvideo pipe:1 | python3 ../holoscan-sdk/scripts/convert_video_to_gxf_entities.py --width 1920 --height 1080 --channels 3 --directory ./ --basename cars\n        ```\n\n        NOTE:\n        - ```basename``` will be used in VideoStreamReplayerOp, you can see yolo_detection.yaml, replayer section, the basename shall be defined as \"cars\" in this example, change the yaml file if needed.\n        - ```height``` and ```width``` are the height and the width of the video. \n\n\n- Run the application inside the container\n    Please make sure that the yolov8-nms-update.onnx file exists under the specified data path.\n    \n    ```\n    python3 ./yolo_detection.py  --data=<your data path> --source=replayer --video_dir=./example_video\n    ```\n\n## Results:\n![](docs/cars_yolo_v8.png)\n\n\n",
        "application_name": "yolo_model_deployment",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run yolo_model_deployment --language python"
    },
    {
        "metadata": {
            "name": "WebRTC Video Server",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Server",
                "Video"
            ],
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    },
                    {
                        "name": "aiohttp",
                        "version": "3.8.5"
                    },
                    {
                        "name": "numpy",
                        "version": "1.23"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/webrtc_server.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# WebRTC Video Server\n\n![](screenshot.png)<br>\n\nThis app reads video frames from a file and sends it to a browser using WebRTC.\n\nThe app starts a web server, the pipeline starts when a browser is connected to the web server and the `Start` button is pressed. The pipeline stops when the `Stop` button is pressed.\n\n```mermaid\nflowchart LR\n    subgraph Server\n        A[(VideoFile)] --> VideoStreamReplayerOp\n        VideoStreamReplayerOp --> FormatConverterOp\n        FormatConverterOp --> WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer <--> Browser\n        WebRTCServerOp <--> Browser\n    end\n```\n\n> **_NOTE:_** When using VPN there might be a delay of several seconds between pressing the `Start` button and the first video frames are display. The reason for this is that the STUN server `stun.l.google.com:19302` used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.\n\n## Prerequisites\n\nThe app is using [AIOHTTP](https://docs.aiohttp.org/en/stable/) for the web server and [AIORTC](https://github.com/aiortc/aiortc) for WebRTC. Install both using pip.\n\n```bash\npip install aiohttp aiortc\n```\n\n## Run Instructions\n\nRun the command:\n\n```bash\n./run launch webrtc_video_server\n```\n\nOn the same machine open a browser and connect to `127.0.0.1:8080`. You can also connect from a different machine by connecting to the IP address the app is running on.\n\nPress the `Start` button. Video frames are displayed. To stop, press the `Stop` button. Pressing `Start` again will continue the video.\n\n### Command Line Arguments\n\n```\nusage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:<ip>:<port>[<username>:<password>]` or `stun:<ip>:<port>`. This option can be specified multiple times to add multiple ICE servers.\n```\n\n## Running With TURN server\n\nA TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.\n\nRun the TURN server in the same machine that you're running the app on.\n\n**Note: It is strongly recommended to run the TURN server with docker network=host for best performance**\n\n```\n# This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"<ip>\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n```\n\nThen you can pass in the TURN server config into the app\n\n```\npython webrtc_server.py --ice-server \"turn:<ip>:3478[admin:admin]\"\n```\n\nThis will enable you to access the webRTC browser application from different machines.",
        "application_name": "webrtc_video_server",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run webrtc_video_server --language python"
    },
    {
        "metadata": {
            "name": "Ultrasound Segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "Segmentation"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/ultrasound_segmentation  --data <holohub_data_dir>/ultrasound_segmentation",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Ultrasound Bone Scoliosis Segmentation\n\nFull workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. \n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n    applications/ultrasound_segmentation/cpp/ultrasound_segmentation --data <data_dir>/ultrasound_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n    applications/ultrasound_segmentation/cpp/ultrasound_segmentation\n    ```\n",
        "application_name": "ultrasound_segmentation",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run ultrasound_segmentation --language cpp"
    },
    {
        "metadata": {
            "name": "Ultrasound Segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "Segmentation"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 <holohub_app_source>/ultrasound_segmentation.py  --data <holohub_data_dir>/ultrasound_segmentation",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Ultrasound Bone Scoliosis Segmentation\n\nFull workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. \n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/ultrasound_segmentation/python\n    python3 ultrasound_segmentation.py --source=replayer --data <DATA_DIR>/ultrasound_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/ultrasound_segmentation/python\n    python3 ultrasound_segmentation.py --source=aja\n    ```\n",
        "application_name": "ultrasound_segmentation",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run ultrasound_segmentation --language python"
    },
    {
        "metadata": {
            "name": "Power Spectral Density with cuNumeric",
            "authors": [
                {
                    "name": "Adam Thompson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Life Sciences, Aerospace, Defense, Communications"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.24.2"
                    },
                    {
                        "name": "cupy",
                        "version": "12.0"
                    },
                    {
                        "name": "cunumeric",
                        "version": "23.03"
                    }
                ]
            }
        },
        "readme": "# Calculate Power Spectral Density with Holoscan and cuNumeric\n\n[cuNumeric](https://github.com/nv-legate/cunumeric) is an drop-in replacement for NumPy that aims to provide a distributed and accelerated drop-in replacement for the NumPy API on top of the [Legion](https://legion.stanford.edu/) runtime. It works best for programs that have very large arrays of data that can't fit in the the memory of a single GPU or node.\n\nIn this example application, we are using the cuNumeric library within a Holoscan application graph to determine the Power Spectral Density (PSD) of an incoming signal waveform. Notably, this is simply achieved by taking the absolute value of the FFT of a data array.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and cuNumeric\n- Demonstrate how to scale a given workload to multiple GPUs using cuNumeric\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-cunumeric-demo python=3.9\nconda activate holoscan-cunumeric-demo\nconda install -c nvidia -c conda-forge -c legate cunumeric cupy\npip install holoscan\n```\n\nThe cuNumeric PSD processing pipeline example can then be run via\n```\nlegate --gpus 2 applications/cunumeric_integration/cunumeric_psd.py\n```\n\nWhile running the application, you can confirm multi GPU utilization via watching `nvidia-smi` or using another GPU utilization tool\n\nTo run the same application without cuNumeric, simply change `import cunumeric as np` to `import cupy as np` in the code and run\n```\npython applications/cunumeric_integration/cunumeric_psd.py\n```\n",
        "application_name": "cunumeric_integration",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run cunumeric_integration --language python"
    },
    {
        "metadata": {
            "name": "FM-ASR",
            "authors": [
                {
                    "name": "Joshua Martinez",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.4.1",
                "tested_versions": [
                    "0.4.1",
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Signal Processing",
                "NLP",
                "ASR",
                "Automatic Speech Recognition"
            ],
            "ranking": 3,
            "dependencies": {
                "libraries": [
                    {
                        "name": "cusignal",
                        "version": "23.06"
                    },
                    {
                        "name": "nvidia-riva-client",
                        "version": "2.10.0"
                    },
                    {
                        "name": "rtl-sdr",
                        "version": "0.6.0-3"
                    },
                    {
                        "name": "NVIDIA Riva",
                        "container": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart"
                    }
                ]
            },
            "run": {
                "command": "./fm_asr_app.py holoscan.yml",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# FM ASR\nThis project is proof-of-concept demo featuring the combination of real-time, low-level signal processing and deep learning inference. It currently supports the [RTL-SDR](https://www.rtl-sdr.com/). Specifically, this project demonstrates the demodulation, downsampling, and automatic transcription of live, civilian FM radio broadcasts. The pipeline architecture is shown in the figure below. \n\n![Pipeline Architecture](docs/images/pipeline_arch.png)\n\nThe primary pipeline segments are written in Python. Future improvements will introduce a fully C++ system.\n\nThis project leverages NVIDIA's [Holoscan SDK](https://github.com/nvidia-holoscan/holoscan-sdk) for performant GPU pipelines, cuSignal package for GPU-accelerated signal processing, and the [RIVA SDK](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/overview.html) for high accuracy automatic speech recognition (ASR).\n\n## Table of Contents\n- [Install](#install)    \n    - [Local Sensor](#local-sensor---basic-configuration)\n        - [x86](#local-sensor---basic-configuration)\n        - [Jetson - TODO](#local-jetson-container)\n    - [Remote Sensor - TODO](#remote-sensor---network-in-the-loop)\n        - [x86](#remote-sensor---network-in-the-loop)\n        - [Jetson](#remote-jetson-containers)  \n    - [Bare Metal - TODO](#bare-metal-install)  \n- [Startup](#startup)\n    - [Scripted Launch](#scripted-launch)\n    - [Manual Launch](#manual-launch)\n- [Configuration Parameters](#configuration-parameters)\n- [Known Issues](#known-issues)\n\n## Install\nTo begin installation, clone this repository using the following:\n```bash\ngit clone https://github.com/nvidia-holoscan/holohub.git\n```\nNVIDIA Riva is required to perform the automated transcriptions. You will need to install and configure the [NGC-CLI](https://ngc.nvidia.com/setup/installers/cli) tool, if you have not done so already, to obtain the Riva container and API. The Riva installation steps may be found at this link: [Riva-Install](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html). Note that Riva performs a TensorRT build during setup and requires access to the targeted GPU. \nThis project has been tested with RIVA 2.10.0.\n\nContainer-based development and deployment is supported. The supported configurations are explained in the sections that follow. \n\n### Local Sensor - Basic Configuration\nThe Local Sensor configuration assumes that the RTL-SDR is connected directly to the GPU-enabled system via USB. I/Q samples are collected from the RTL-SDR directly, using the SoapySDR library. Specialized containers are provided for Jetson devices.\n\nOnly two containers are used in this configuration: \n- The Application Container which includes all the necessary low level libraries, radio drivers, Holoscan SDK for the core application pipeline, and the Riva client API; and\n- The Riva SDK container that houses the ASR transcription service.\n\n![LocalSensor](./docs/images/Local-Sensor-Arch.png)\n\nFor convenience, container build scripts are provided to automatically build the application containers for Jetson and x86 systems. The Dockerfiles can be readily modified for ARM based systems with a discrete GPU. To build the container for this configuration, run the following:\n```bash\n# Starting from FM-ASR root directory\ncd scripts\n./build_application_container.sh # builds Application Container\n```\nNote that this script does not build the Riva container.\n\nA script for running the application container is also provided. The run scripts will start the containers and leave the user at a bash terminal for development. Separate launch scripts are provided to automatically run the application.\n```bash\n# Starting from FM-ASR root directory\n./scripts/run_application_container.sh\n```\n\n#### Local Jetson Container\nHelper scripts will be provided in a future release.\n\n\n### Remote Sensor - Network in the Loop\nThis configuration is currently in work and will be provided in a future release. Developers can modify this code base to support this configuration if desired.\n\n### Bare Metal Install\nWill be added in the future. Not currently supported.\n\n## Startup\nAfter installation, the following steps are needed to launch the application:\n1. Start the Riva ASR service\n2. Launch the Application Container\n\n### Scripted Launch\nThe above steps are automated by some helper scripts.\n```bash\n# Starting from FM-ASR root directory\n./scripts/lauch_application.sh # Starts Application Container and launches app using the config file defined in the script\n\n```\n### Manual Launch\nAs an alternative to `launch_application.sh`, the FM-ASR pipeline can be run from inside the Application Container using the following commands:\n```bash\ncd /workspace\nexport CONFIG_FILE=/workspace/params/holoscan.yml # can be edited by user\npython fm_asr_app.py $CONFIG_FILE\n```\n\n### Initialize and Start the Riva Service\nRiva can be setup following the [Quickstart guide](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#) (version 2.10.0 currently supported). To summarize it, run the following:\n```bash\ncd <riva_quickstart_download_directory>\nbash riva_init.sh\nbash riva_start.sh\n```\nThe initialization step will take a while to complete but only needs to be done once. Riva requires a capable GPU to setup and run properly. If your system has insufficient resources, the initialization script may hang. \n\nWhen starting the service, Riva may output a few \"retrying\" messages. This is normal and not an indication that the service is frozen. You should see a message saying ```Riva server is ready...``` once successful. \n\n*Note for users with multiple GPUs:*\n\nIf you want to specify which GPU Riva uses (defaults to device 0), open and edit ```<riva_quickstart_download_directory>/config.sh```, then change line\n```bash\ngpus_to_use=\"device=0\"\n```\nto\n```bash\ngpus_to_use=\"device=<your-device-number>\"\n# or, to guarantee a specific device\ngpus_to_use=\"device=<your-GPU-UUID>\"\n```\nYou can determine your GPUs' UUIDs by running ```nvidia-smi -L```.\n\n## Configuration Parameters\nA table of the configuration parameters used in this project is shown below, organized by application operator.\n\n| Parameter | Type | Description |\n| --------- | ---- |  ----------- |\n| run_time | int | Number of seconds that pipeline will execute |\n| RtlSdrGeneratorOp|||\n| sample_rate | float | Reception sample rate used by the radio. RTL-SDR max stable sample rate without dropping is 2.56e6.|\n| tune_frequency | float | Tuning frequency for the radio in Hz. |\n| gain | float | 40.0 | Gain applied to received signal. Max for RTL-SDR is 40. |\n| PlayAudioOp |||\n| play_audio | bool | Flag used to enable simultaneous audio playback of signal. |\n| RivaAsrOp |||\n| sample_rate | int | Audio sample rate expected by the Riva ASR model. Riva default is to 16000, other values will incurr an additional resample operation within Riva. |\n| max_alternatives | int | Riva - Maximum number of alternative transcripts to return (up to limit configured on server). Setting to 1 returns only the best response. |\n| word-time-offsets | bool | Riva - Option to output word timestamps in transcript.|\n| automatic-punctuation | bool | Riva - Flag that controls if transcript should be automatically punctuated. |\n| uri | str | localhost:50051 | Riva - URI/IP address to access the Riva server. Must match IP that Riva service was configured with. Default is localhost:50051. |\n| no-verbatim-transcripts | bool | Riva - If specified, text inverse normalization will be applied |\n| boosted_lm_words | str | Riva - words to boost when decoding. Useful for handling jargon and acronyms. |\n| boosted_lm_score | float | Value by which to boost words when decoding |\n| language-code | str | Riva - Language code of the model to be used. US English is en-US. Check Riva docs for more options|\n| interim_transcriptions | bool | Riva - Flag to include interim transcriptions in the output file. |\n| ssl_cert | str | Path to SSL client certificates file. Not currently utilized |\n| use_ssl | bool | Boolean to control if SSL/TLS encryption should be used. Not currently utilized. |\n| recognize_interval| int | Specifies the amount of data RIVA processes per request, in time (s). |\n| TranscriptSinkOp |||\n| output_file | str | File path to store a transcript. Existing files will be overwritten. |\n\n\n#### Known Issues\nThis table will be populated as issues are identified.\n\n| Issue | Description | Status|\n| ----- | ----------- | ---|\n\n\n",
        "application_name": "fm_asr",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run fm_asr --language python"
    },
    {
        "metadata": {
            "name": "Chat with NVIDIA Inference Microservice (NIM)",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3",
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "OpenAI API",
                "NIM"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 <holohub_app_source>/app.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Chat with NVIDIA NIM\n\nThis is a sample application that shows how to use the OpenAI SDK with NVIDIA Inference Microservice (NIM). Whether you are using a NIM from [build.nvidia.com/](https://build.nvidia.com/) or a self-hosted NIM, this sample application will work for both.\n\n### Quick Start\n\n1. Add API key in `nvidia_nim.yaml`\n2. `./dev_container build_and_run nvidia_nim_chat`\n\n## Configuring the sample application\n\nUse the `nvidia_nim.yaml` configuration file to configure the sample application:\n\n### Connection Information\n\n```\nnim:\n  base_url: https://integrate.api.nvidia.com/v1\n  api_key:\n\n```\n\n`base_url`: The URL of your NIM instance. Defaults to NVIDIA hosted NIMs.\n`api_key`: Your API key to access NVIDIA hosted NIMs.\n\n### Model Information\n\nThe `models` section in the YAML file is configured with multiple NVIDIA hosted models by default. This allows you to switch between different models easily within the application by sending the prompt `/m` to the application.\n\nModel parameters may be added or adjusted in the `models` section as well per model.\n\n## Run the sample application\n\nThere are a couple of options to run the sample application:\n\n### Run using Docker\n\nTo run the sample application with Docker, you must first build a Docker image that includes the sample application and its dependencies:\n\n```\n# Build the Docker images from the root directory of Holohub\n./dev_container build --docker_file applications/nvidia_nim/Dockerfile\n```\n\nThen, run the Docker image:\n\n```bash\n./dev_container  launch\n```\n\nContinue to the [Start the Application](#start-the-application) section once inside the Docker container.\n\n### Run the Application without Docker\n\nInstall all dependencies from the `requirements.txt` file:\n\n```bash\n# optionally create a virtual environment and activate it\npython3 -m venv .venv\nsource .venv/bin/activate\n\n# install the required packages\npip install -r applications/nvidia_nim/chat/requirements.txt\n```\n\n### Start the Application\n\nTo use the NIMs on [build.nvidia.com/](https://build.nvidia.com/), configure your API key in the `nvidia_nim.yaml` configuration file and run the sample app as follows:\n\nnote: you may also configure your api key using an environment variable.\nE.g., `export API_KEY=...`\n\n```bash\n# To use NVIDIA hosted NIMs available on build.nvidia.com, export your API key first\nexport API_KEY=[enter your api key here]\n\n./run launch nvidia_nim_chat\n```\n\nHave fun!\n\n\n## Connecting with Locally Hosted NIMs\n\nTo use a locally hosted NIM, first download and start the NIM.\nThen configure the `base_url` parameter in the `nvidia_nim.yaml` configuration file to point to your local NIM instance.\n\nThe following example shows a NIM running locally and serving its APIs and the `meta-llama3-8b-instruct` model from `http://0.0.0.0:8000/v1`.\n\n```bash\nnim:\n  base_url: http://0.0.0.0:8000/v1/\n\nmodels:\n  llama3-8b-instruct:\n    model: meta-llama3-8b-instruct # name of the model serving by the NIM\n    # add/update/remove the following key/value pairs to configure the parameters for the model\n    top_p: 1\n    n: 1\n    max_tokens: 1024\n    frequency_penalty: 1.0\n```",
        "application_name": "nvidia_nim_chat",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run nvidia_nim_chat --language python"
    },
    {
        "metadata": {
            "name": "WebRTC Holoviz Server",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Server",
                "Holoviz"
            ],
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    },
                    {
                        "name": "aiohttp",
                        "version": "3.8.5"
                    },
                    {
                        "name": "numpy",
                        "version": "1.23"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/webrtc_server.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# WebRTC Holoviz Server\n\n![](screenshot.png)<br>\n\nThis app generates video frames with user specified content using Holoviz and sends it to a browser using WebRTC. The goal is to show how to remote control operators and view the output of a Holoscan pipeline.\n\nThe app starts a web server, the pipeline starts when a browser is connected to the web server and the `Start` button is pressed. The pipeline stops when the `Stop` button is pressed.\n\nThe web page has user inputs for specifying text and for the speed the text moves across the screen.\n\n```mermaid\nflowchart LR\n    subgraph Server\n        GeometryGenerationOp --> HolovizOp\n        HolovizOp --> FormatConverterOp\n        FormatConverterOp --> WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer <--> Browser\n        WebRTCServerOp <--> Browser\n    end\n```\n\n> **_NOTE:_** When using VPN there might be a delay of several seconds between pressing the `Start` button and the first video frames are display. The reason for this is that the STUN server `stun.l.google.com:19302` used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.\n\n## Prerequisites\n\nThe app is using [AIOHTTP](https://docs.aiohttp.org/en/stable/) for the web server and [AIORTC](https://github.com/aiortc/aiortc) for WebRTC. Install both using pip.\n\n```bash\npip install aiohttp aiortc\n```\n\n## Run Instructions\n\nRun the command:\n\n```bash\n./run launch webrtc_holoviz_server\n```\n\nOn the same machine open a browser and connect to `127.0.0.1:8080`. You can also connect from a different machine by connecting to the IP address the app is running on.\n\nPress the `Start` button. Video frames are displayed. To stop, press the `Stop` button. Pressing `Start` again will continue the video.\n\nChange the text input and the speed slider to control the generated video frame content.\n\n### Command Line Arguments\n\n```\nusage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:<ip>:<port>[<username>:<password>]` or `stun:<ip>:<port>`. This option can be specified multiple times to add multiple ICE servers.\n```\n\n\n## Running With TURN server\n\nA TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.\n\nRun the TURN server in the same machine that you're running the app on.\n\n**Note: It is strongly recommended to run the TURN server with docker network=host for best performance**\n\n```\n# This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"<ip>\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n```\n\nThen you can pass in the TURN server config into the app\n\n```\npython webrtc_server.py --ice-server \"turn:<ip>:3478[admin:admin]\"\n```\n\nThis will enable you to access the webRTC browser application from different machines.",
        "application_name": "webrtc_holoviz_server",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run webrtc_holoviz_server --language python"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/orsi_segmentation_ar <holohub_app_bin>/orsi_segmentation_ar.yaml --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi Non Organic Structure Segmentation and AR sample app\n\n\n<center> <img src=\"./docs/orsi_segmentation.png\" ></center>\n<center> Fig. 1: Application screenshot  </center><br>\n\n\n## Introduction\n\n3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows an in-app screenshot where the 3D model is aligned with the anatomy and the tools are segmented on top.\n\n<center> <img src=\"./docs/3Dmodel_stent.png\" ></center>\n<center> Fig. 2: 3D model of nutcracker case </center><br>\n\nThe application was successfully used to verify stent location during a nutcracker syndrome stent removal. Nutcracker syndrome is a rare vein compression disorder where the left renal vein is squeezed between the superior mesenteric artery and abdominal aorta, obstructing blood drainage and even risking blood to flow backwards, causing pain and blood in the urine. Typically, blood flow is restored through endovascular stenting. Although effective, over time the stent had migrated beyond the compression site, causing the symptoms to return and in addition posing an incidental obstruction for blood flow. The stent ofcourse had to be removed. Figure 2 shows a 3D model depicting the current situation.\n\n## Pipeline\n\n<center> <img src=\"./docs/segmentation_pipeline.png\" ></center>\n<center> Fig. 3: Schematic overview of Holoscan application </center><br>\n\nTowards realtime segmentation and AR overlay, a binary segmentation model for non-organic items was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Segmentation Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) operator the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation dataset. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the [Segmentation Postprocessor](/operators/orsi/orsi_segmentation_postprocessor/segmentation_postprocessor.cpp) resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program.\n\n##  Controls\n\n| Action | Control |\n| -------- | ------- |\n| Enable/Disable anonymization | B |\n| Enable/Disable manipulations | T |\n| Load 3D model orientation preset | CTRL + L |\n| Save current 3D model orientation as preset **(will overwrite default preset)** | CTRL + S |\n| Rotate 3D model (3 degrees of freedom)  | Left Click + Drag |\n| Rotate 3D model (1 degree of freedom) | CTRL + Left Click + Drag |\n| Zoom 3D model | Right Click + Drag |\n| Translate 3D  model | SHIFT + Left Click + Drag |\n| Enable/Disable 3D model | E |\n| Enable/Disable segmentation overlay | O |\n| Increase opacity 3D model | + |\n| Decrease opacity 3D model | - |\n| Toggle venous tree | 0 |\n| Toggle venous stent zone | 1 |\n| Toggle stent | 2 |\n\n\n## Build app\n\n```bash\n./run build orsi_segmentation_ar\n```\n\n## Launch app\n\n```bash\n./run launch orsi_segmentation_ar cpp\n```\n\nor\n\n```bash\n./run launch orsi_segmentation_ar python\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>\n",
        "application_name": "orsi_segmentation_ar",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run orsi_segmentation_ar --language cpp"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/orsi_segmentation_ar.py --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi Non Organic Structure Segmentation and AR sample app\n\n\n<center> <img src=\"./docs/orsi_segmentation.png\" ></center>\n<center> Fig. 1: Application screenshot  </center><br>\n\n\n## Introduction\n\n3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows an in-app screenshot where the 3D model is aligned with the anatomy and the tools are segmented on top.\n\n<center> <img src=\"./docs/3Dmodel_stent.png\" ></center>\n<center> Fig. 2: 3D model of nutcracker case </center><br>\n\nThe application was successfully used to verify stent location during a nutcracker syndrome stent removal. Nutcracker syndrome is a rare vein compression disorder where the left renal vein is squeezed between the superior mesenteric artery and abdominal aorta, obstructing blood drainage and even risking blood to flow backwards, causing pain and blood in the urine. Typically, blood flow is restored through endovascular stenting. Although effective, over time the stent had migrated beyond the compression site, causing the symptoms to return and in addition posing an incidental obstruction for blood flow. The stent ofcourse had to be removed. Figure 2 shows a 3D model depicting the current situation.\n\n## Pipeline\n\n<center> <img src=\"./docs/segmentation_pipeline.png\" ></center>\n<center> Fig. 3: Schematic overview of Holoscan application </center><br>\n\nTowards realtime segmentation and AR overlay, a binary segmentation model for non-organic items was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Segmentation Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) operator the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation dataset. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the [Segmentation Postprocessor](/operators/orsi/orsi_segmentation_postprocessor/segmentation_postprocessor.cpp) resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program.\n\n##  Controls\n\n| Action | Control |\n| -------- | ------- |\n| Enable/Disable anonymization | B |\n| Enable/Disable manipulations | T |\n| Load 3D model orientation preset | CTRL + L |\n| Save current 3D model orientation as preset **(will overwrite default preset)** | CTRL + S |\n| Rotate 3D model (3 degrees of freedom)  | Left Click + Drag |\n| Rotate 3D model (1 degree of freedom) | CTRL + Left Click + Drag |\n| Zoom 3D model | Right Click + Drag |\n| Translate 3D  model | SHIFT + Left Click + Drag |\n| Enable/Disable 3D model | E |\n| Enable/Disable segmentation overlay | O |\n| Increase opacity 3D model | + |\n| Decrease opacity 3D model | - |\n| Toggle venous tree | 0 |\n| Toggle venous stent zone | 1 |\n| Toggle stent | 2 |\n\n\n## Build app\n\n```bash\n./run build orsi_segmentation_ar\n```\n\n## Launch app\n\n```bash\n./run launch orsi_segmentation_ar cpp\n```\n\nor\n\n```bash\n./run launch orsi_segmentation_ar python\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>\n",
        "application_name": "orsi_segmentation_ar",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run orsi_segmentation_ar --language python"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/orsi_in_out_body <holohub_app_bin>/orsi_in_out_body.yaml --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi In - Out - Body Detection sample app\n\n\n<center> <img src=\"./docs/anonymization.png\" ></center>\n<center> Fig. 1: Example of anonymized result after inference </center><br>\n\n## Introduction\n\nIn robotic surgery, anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.\n\n## Pipeline\n\n<center> <img src=\"./docs/Holoscan_oob_pipeline.png\" ></center>\n<center> Fig. 2: Schematic overview of Holoscan application </center><br>\n\nTowards realtime anonymization, a binary out-of-body classifier was trained and deployed using Holoscan platform. Figure 2 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Anonymization Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) operator the tensor pixel values of every collor channel are normalized using the corresponding means and standard deviations of the anonymization dataset. After model inference with the Multi-AI inference operator, the result frame is anonymized in the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator according to the model output. The blurring is applied using a glsl program.\n## Controls\n\n| Action    | Control |\n| -------- | ------- |\n| Enable anonymization | B |\n\n\n## Build app\n\n```bash\n./run build orsi_in_out_body\n```\n\n## Launch app\n\n**C++** \n\n```bash\n./run launch orsi_in_out_body cpp\n```\n\n**Python**\n\n```bash\n./run launch orsi_in_out_body python\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>",
        "application_name": "orsi_in_out_body",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run orsi_in_out_body --language cpp"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/orsi_in_out_body.py --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi In - Out - Body Detection sample app\n\n\n<center> <img src=\"./docs/anonymization.png\" ></center>\n<center> Fig. 1: Example of anonymized result after inference </center><br>\n\n## Introduction\n\nIn robotic surgery, anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.\n\n## Pipeline\n\n<center> <img src=\"./docs/Holoscan_oob_pipeline.png\" ></center>\n<center> Fig. 2: Schematic overview of Holoscan application </center><br>\n\nTowards realtime anonymization, a binary out-of-body classifier was trained and deployed using Holoscan platform. Figure 2 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Anonymization Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) operator the tensor pixel values of every collor channel are normalized using the corresponding means and standard deviations of the anonymization dataset. After model inference with the Multi-AI inference operator, the result frame is anonymized in the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator according to the model output. The blurring is applied using a glsl program.\n## Controls\n\n| Action    | Control |\n| -------- | ------- |\n| Enable anonymization | B |\n\n\n## Build app\n\n```bash\n./run build orsi_in_out_body\n```\n\n## Launch app\n\n**C++** \n\n```bash\n./run launch orsi_in_out_body cpp\n```\n\n**Python**\n\n```bash\n./run launch orsi_in_out_body python\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>",
        "application_name": "orsi_in_out_body",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run orsi_in_out_body --language python"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/orsi_multi_ai_ar <holohub_app_bin>/orsi_multi_ai_ar.yaml --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi Multi AI and AR sample app\n\n\n\n<center> <img src=\"./docs/multi_ai_1.png\" width=\"650\" height=\"400\"> <img src=\"./docs/multi_ai_2.png\" width=\"650\" height=\"400\"></center>\n<center> Fig. 1: Application screenshots  </center><br>\n\n## Introduction\n\n3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows in-app screenshots where the 3D model is used to support navigation. The large tumor in green is visible as anatomical landmark.  In addition, the application features an out-of-body detector. Anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.\n\n\n<center> <img src=\"./docs/3D model.png\" ></center>\n<center> Fig. 2: 3D model of kidney tumor case </center><br>\n\nThe application was successfully used to remove a kidney tumor during a partial nephrectomy. Figure 2 shows the kidney 3D model where the tumor is colored in yellow.\n\n## Pipeline\n\n<center> <img src=\"./docs/multiai_pipeline.png\" ></center>\n<center> Fig. 3: Schematic overview of Holoscan application </center><br>\n\nTowards realtime segmentation, AR overlay and anonymization, a binary segmentation model for non-organic items and a binary out-of-body detector was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Segmentation Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) and Anonymization Preprocessor operators the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation and anonymization dataset respectively. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the [Segmentation Postprocessor](/operators/orsi/orsi_segmentation_postprocessor/segmentation_postprocessor.cpp) resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program. Finally, the frame is anonymized according to the out-of-body detector output.\n\n\n##  Controls\n\n| Action | Control |\n| -------- | ------- |\n| Enable/Disable anonymization | B |\n| Enable/Disable manipulations | T |\n| Load 3D model orientation preset | CTRL + L |\n| Save current 3D model orientation as preset **(will overwrite default preset)** | CTRL + S |\n| Rotate 3D model (3 degrees of freedom)  | Left Click + Drag |\n| Rotate 3D model (1 degree of freedom) | CTRL + Left Click + Drag |\n| Zoom 3D model | Right Click + Drag |\n| Translate 3D  model | SHIFT + Left Click + Drag |\n| Enable/Disable 3D model | E |\n| Enable/Disable segmentation overlay | O |\n| Increase opacity 3D model | + |\n| Decrease opacity 3D model | - |\n| Toggle arterial tree | 1 |\n| Toggle venous tree | 2 |\n| Toggle ureter | 4 |\n| Toggle parenchyma | 5 |\n| Toggle tumor | 6 |\n\n\n## Build app\n\n```bash\n./run build orsi_multi_ai_ar\n```\n\n## Launch app\n\n```bash\n./run launch orsi_multi_ai_ar cpp\n```\n\nor\n\n```bash\n./run launch orsi_multi_ai_ar python\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>",
        "application_name": "orsi_multi_ai_ar",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run orsi_multi_ai_ar --language cpp"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/orsi_multi_ai_ar.py --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi Multi AI and AR sample app\n\n\n\n<center> <img src=\"./docs/multi_ai_1.png\" width=\"650\" height=\"400\"> <img src=\"./docs/multi_ai_2.png\" width=\"650\" height=\"400\"></center>\n<center> Fig. 1: Application screenshots  </center><br>\n\n## Introduction\n\n3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows in-app screenshots where the 3D model is used to support navigation. The large tumor in green is visible as anatomical landmark.  In addition, the application features an out-of-body detector. Anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.\n\n\n<center> <img src=\"./docs/3D model.png\" ></center>\n<center> Fig. 2: 3D model of kidney tumor case </center><br>\n\nThe application was successfully used to remove a kidney tumor during a partial nephrectomy. Figure 2 shows the kidney 3D model where the tumor is colored in yellow.\n\n## Pipeline\n\n<center> <img src=\"./docs/multiai_pipeline.png\" ></center>\n<center> Fig. 3: Schematic overview of Holoscan application </center><br>\n\nTowards realtime segmentation, AR overlay and anonymization, a binary segmentation model for non-organic items and a binary out-of-body detector was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Segmentation Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) and Anonymization Preprocessor operators the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation and anonymization dataset respectively. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the [Segmentation Postprocessor](/operators/orsi/orsi_segmentation_postprocessor/segmentation_postprocessor.cpp) resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program. Finally, the frame is anonymized according to the out-of-body detector output.\n\n\n##  Controls\n\n| Action | Control |\n| -------- | ------- |\n| Enable/Disable anonymization | B |\n| Enable/Disable manipulations | T |\n| Load 3D model orientation preset | CTRL + L |\n| Save current 3D model orientation as preset **(will overwrite default preset)** | CTRL + S |\n| Rotate 3D model (3 degrees of freedom)  | Left Click + Drag |\n| Rotate 3D model (1 degree of freedom) | CTRL + Left Click + Drag |\n| Zoom 3D model | Right Click + Drag |\n| Translate 3D  model | SHIFT + Left Click + Drag |\n| Enable/Disable 3D model | E |\n| Enable/Disable segmentation overlay | O |\n| Increase opacity 3D model | + |\n| Decrease opacity 3D model | - |\n| Toggle arterial tree | 1 |\n| Toggle venous tree | 2 |\n| Toggle ureter | 4 |\n| Toggle parenchyma | 5 |\n| Toggle tumor | 6 |\n\n\n## Build app\n\n```bash\n./run build orsi_multi_ai_ar\n```\n\n## Launch app\n\n```bash\n./run launch orsi_multi_ai_ar cpp\n```\n\nor\n\n```bash\n./run launch orsi_multi_ai_ar python\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>",
        "application_name": "orsi_multi_ai_ar",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run orsi_multi_ai_ar --language python"
    },
    {
        "metadata": {
            "name": "Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Multiai",
                "SSD",
                "bounding box",
                "Detection",
                "MONAI",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    },
                    {
                        "name": "A SSD Detection model for Endoscopy Surgical Tools",
                        "description": "This resource contains a SSD Detection model for the identification of surgical tools",
                        "url": "https://api.ngc.nvidia.com/v2/resources/nvidia/clara-holoscan/ssd_surgical_tool_detection_model"
                    },
                    {
                        "name": "Model for HoloHub Sample App for MONAI Endoscopic Tool Segmentation",
                        "url": "https://api.ngc.nvidia.com/v2/resources/nvidia/clara-holoscan/monai_endoscopic_tool_segmentation_model"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/post-proc-cpu/multiai_endoscopy --data <holohub_data_dir>/",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation \nIn this application, we show how to build a Multi AI application with detection and segmentation models, write postprocessing operators using CuPy and NumPy in Python tensor interop and [MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax)](https://github.com/NVIDIA/MatX) in C++ tensor interop, and pass multiple tensors from postprocessing to Holoviz.\n\nPlease refer to the README under [./app_dev_process](./app_dev_process/README.md) to see the process of developing the applications.\n\nThe application graph looks like:\n![](./images/multiai_endoscopy_app_graph.png)\n\n## Model\nWe combine two models from the single model applications [SSD Tool Detection](https://github.com/nvidia-holoscan/holohub/tree/main/applications/ssd_detection_endoscopy_tools) and [MONAI Endoscopic Tool Segmentation](https://github.com/nvidia-holoscan/holohub/tree/main/applications/monai_endoscopic_tool_seg):\n\n - [SSD model from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/ssd_surgical_tool_detection_model) with additional NMS op: `epoch24_nms.onnx`\n - [MONAI tool segmentation model from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/monai_endoscopic_tool_segmentation_model): `model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx`\n## Data\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\n## Requirements\nEnsure you have installed the Holoscan SDK via one of the methods specified in [the SDK user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/sdk_installation.html#development-software-stack).\n\nThe directory specified by `--data` at app runtime is assumed to contain three subdirectories, corresponding to the three NGC resources specified in [Model](#model) and [Data](#data): `endoscopy`, `monai_tool_seg_model` and `ssd_model`.  These resources will be automatically downloaded to the holohub data directory when building the application.\n\n## Building the application\n\nThe repo level build command \n```sh\n./run build multiai_endoscopy\n```\nwill build one of the cpp apps `post-proc-cpu`. \n\n\n## Running the application\n### Python Apps\nTo run the Python application, you can make use of the run script\n```sh\n./run launch multiai_endoscopy python\n```\nAlternatively, to run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\nNext, run the application:\n```sh\ncd <HOLOHUB_SOURCE_DIR>/applications/multiai_endoscopy/python\npython3 multi_ai.py --data <DATA_DIR>\n```\n\n### C++ Apps\n\nThere are three versions of C++ apps, with the only difference being that they implement the inference post-processing operator `DetectionPostprocessorOp` in different ways:\n\n- `post-proc-cpu`: Multi-AI app running the inference post-processing operator on the CPU using `std` features only.\n- `post-proc-matx-cpu`: Multi-AI app running the inference post-processing operator on the CPU using the [MatX library]([GitHub - NVIDIA/MatX: An efficient C++17 GPU numerical computing library with Python-like syntax](https://github.com/NVIDIA/MatX)).\n- `post-proc-matx-gpu`: Multi-AI app running  the inference post-processing operator on the GPU using MatX (CUDA).\n\nTo run `post-proc-cpu`, since it already gets built with `./run build multiai_endoscopy`:\n```sh\n./run launch multiai_endoscopy cpp\n```\n\nFor the other two C++ applications, you'll need to build these without the run script as follows.\n\nTo run `post-proc-matx-cpu` or `post-proc-matx-gpu`, first navigate to the app directory.\n\n```shell\ncd cpp/post-proc-matx-cpu\n```\n\nNext we need to configure and build the app.\n\n#### Configuring\n\nFirst, create a build folder:\n\n```shell\nmkdir -p build\n```\n\nthen run CMake configure with:\n\n```shell\ncmake -S . -B build\n```\n\nUnless you make changes to `CMakeLists.txt`, this step only needs to be done **once**.\n\n#### Building\n\nThe app can be built with:\n\n```shell\ncmake --build build\n```\n\nor equally:\n\n```shell\ncd build\nmake\n```\n\n#### Running\n\nYou can run the app with:\n\n```shell\n./build/multi_ai --data <DATA_DIR>\n```\n",
        "application_name": "multiai_endoscopy",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run multiai_endoscopy --language cpp"
    },
    {
        "metadata": {
            "name": "Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Multiai",
                "SSD",
                "bounding box",
                "Detection",
                "MONAI",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    },
                    {
                        "name": "A SSD Detection model for Endoscopy Surgical Tools",
                        "description": "This resource contains a SSD Detection model for the identification of surgical tools",
                        "url": "https://api.ngc.nvidia.com/v2/resources/nvidia/clara-holoscan/ssd_surgical_tool_detection_model"
                    },
                    {
                        "name": "Model for HoloHub Sample App for MONAI Endoscopic Tool Segmentation",
                        "url": "https://api.ngc.nvidia.com/v2/resources/nvidia/clara-holoscan/monai_endoscopic_tool_segmentation_model"
                    }
                ],
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.22.3"
                    },
                    {
                        "name": "cupy",
                        "version": "11.6.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/multi_ai.py --data <holohub_data_dir>/",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation \nIn this application, we show how to build a Multi AI application with detection and segmentation models, write postprocessing operators using CuPy and NumPy in Python tensor interop and [MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax)](https://github.com/NVIDIA/MatX) in C++ tensor interop, and pass multiple tensors from postprocessing to Holoviz.\n\nPlease refer to the README under [./app_dev_process](./app_dev_process/README.md) to see the process of developing the applications.\n\nThe application graph looks like:\n![](./images/multiai_endoscopy_app_graph.png)\n\n## Model\nWe combine two models from the single model applications [SSD Tool Detection](https://github.com/nvidia-holoscan/holohub/tree/main/applications/ssd_detection_endoscopy_tools) and [MONAI Endoscopic Tool Segmentation](https://github.com/nvidia-holoscan/holohub/tree/main/applications/monai_endoscopic_tool_seg):\n\n - [SSD model from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/ssd_surgical_tool_detection_model) with additional NMS op: `epoch24_nms.onnx`\n - [MONAI tool segmentation model from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/monai_endoscopic_tool_segmentation_model): `model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx`\n## Data\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\n## Requirements\nEnsure you have installed the Holoscan SDK via one of the methods specified in [the SDK user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/sdk_installation.html#development-software-stack).\n\nThe directory specified by `--data` at app runtime is assumed to contain three subdirectories, corresponding to the three NGC resources specified in [Model](#model) and [Data](#data): `endoscopy`, `monai_tool_seg_model` and `ssd_model`.  These resources will be automatically downloaded to the holohub data directory when building the application.\n\n## Building the application\n\nThe repo level build command \n```sh\n./run build multiai_endoscopy\n```\nwill build one of the cpp apps `post-proc-cpu`. \n\n\n## Running the application\n### Python Apps\nTo run the Python application, you can make use of the run script\n```sh\n./run launch multiai_endoscopy python\n```\nAlternatively, to run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\nNext, run the application:\n```sh\ncd <HOLOHUB_SOURCE_DIR>/applications/multiai_endoscopy/python\npython3 multi_ai.py --data <DATA_DIR>\n```\n\n### C++ Apps\n\nThere are three versions of C++ apps, with the only difference being that they implement the inference post-processing operator `DetectionPostprocessorOp` in different ways:\n\n- `post-proc-cpu`: Multi-AI app running the inference post-processing operator on the CPU using `std` features only.\n- `post-proc-matx-cpu`: Multi-AI app running the inference post-processing operator on the CPU using the [MatX library]([GitHub - NVIDIA/MatX: An efficient C++17 GPU numerical computing library with Python-like syntax](https://github.com/NVIDIA/MatX)).\n- `post-proc-matx-gpu`: Multi-AI app running  the inference post-processing operator on the GPU using MatX (CUDA).\n\nTo run `post-proc-cpu`, since it already gets built with `./run build multiai_endoscopy`:\n```sh\n./run launch multiai_endoscopy cpp\n```\n\nFor the other two C++ applications, you'll need to build these without the run script as follows.\n\nTo run `post-proc-matx-cpu` or `post-proc-matx-gpu`, first navigate to the app directory.\n\n```shell\ncd cpp/post-proc-matx-cpu\n```\n\nNext we need to configure and build the app.\n\n#### Configuring\n\nFirst, create a build folder:\n\n```shell\nmkdir -p build\n```\n\nthen run CMake configure with:\n\n```shell\ncmake -S . -B build\n```\n\nUnless you make changes to `CMakeLists.txt`, this step only needs to be done **once**.\n\n#### Building\n\nThe app can be built with:\n\n```shell\ncmake --build build\n```\n\nor equally:\n\n```shell\ncd build\nmake\n```\n\n#### Running\n\nYou can run the app with:\n\n```shell\n./build/multi_ai --data <DATA_DIR>\n```\n",
        "application_name": "multiai_endoscopy",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run multiai_endoscopy --language python"
    },
    {
        "metadata": {
            "name": "Advanced Networking Benchmark",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.3",
            "changelog": {
                "1.3": "Allow app to have buffer multiple of ANO buffer",
                "1.2": "GPUDirect TX",
                "1.1": "GPUDirect mode without header data split"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "DPDK",
                "UDP",
                "Ethernet",
                "IP",
                "GPUDirect",
                "RDMA"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "advanced_networking_benchmark",
                        "version": "1.2"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/adv_networking_bench",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Advanced Networking Benchmark\n\nThis is a simple application to measure a lower bound on performance for the advanced networking operator\nby receiving packets, optionally doing work on them, and freeing the buffers. While only freeing the packets is\nan unrealistic workload, it's useful to see at a high level whether the application is able to keep up with\nthe bare minimum amount of work to do. The application contains both a transmitter and receiver that are\ndesigned to run on different systems, and may be configured independently.\n\nThe performance of this application depends heavily on a properly-configured system and choosing the best\ntuning parameters that are acceptable for the workload. To configure the system please see the documentation\nfor the advanced network operator. With the system tuned, the application performance will be dictated\nby batching size and whether GPUDirect is enabled. \n\nAt this time both the transmitter and receiver are written to handle an Ethernet+IP+UDP packet with a\nconfigurable payload. Other modes may be added in the future. Also, for simplicity, the transmitter and\nreceiver must be configured to a single packet size.\n\n## Transmit\n\nThe transmitter sends a UDP packet with an incrementing sequence of bytes after the UDP header. The batch\nsize configured dictates how many packets the benchmark operator sends to the advanced network operator\nin each tick. Typically with the same number of CPU cores the transmitter will run faster than the receiver, \nso this parameter may be used to throttle the sender somewhat by making the batches very small.\n\n## Receiver\n\nThe receiver receives the UDP packets in either CPU-only mode or header-data split mode. CPU-only mode\nwill receive the packets in CPU memory, copy the payload contents to a host-pinned staging buffer, and\nfreed. In header-data split mode the user may configure a split point where the bytes before that point\nare sent to the CPU, and all bytes afterwards are sent to the GPU. Header-data split should achieve higher\nrates than CPU mode since the amount of data to the CPU can be orders of magnitude lower compared to running\nin CPU-only mode. \n\n### Configuration\n\nThe application is configured using a separate transmit and receive file. The transmit file is called\n`adv_networking_bench_tx.yaml` while the receive is named `adv_networking_bench_rx.yaml`. Configure the\nadvanced networking operator on both transmit and receive per the instructions for that operator.\n\n#### Receive Configuration\n\n- `header_data_split`: bool\n  Turn on GPUDirect header-data split mode\n- `batch_size`: integer\n  Size in packets for a single batch. This should be a multiple of the advanced network RX operator batch size.\n  A larger batch size consumes more memory since any work will not start unless this batch size is filled. Consider\n  reducing this value if errors are occurring.\n- `max_packet_size`: integer\n  Maximum packet size expected. This value includes all headers up to and including UDP.\n\n#### Transmit Configuration\n\n- `batch_size`: integer\n  Size in packets for a single batch. This batch size is used to send to the advanced network TX operator, and \n  will loop sending that many packets for each burst.\n- `payload_size`: integer\n  Size of the payload to send after all L2-L4 headers \n\n### Requirements\n\nThis application requires all configuration and requirements from the advanced network operator.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory, then for the transmitter run:\n\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_tx.yaml\n```\n\nOr for the receiver:\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_tx.yaml\n```\n\nWith DOCA:\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_doca_tx_rx.yaml\n```\n\n<mark>For Holoscan internal reasons (not related to the DOCA library), build the Advanced Network Operator with `RX_PERSISTENT_ENABLED` set to 1 MAY cause problems to this application on the receive (process) side (receive hangs). If you experience any issue on the receive side, please read carefully in the Advanced Network Operator README how to solve this problem.</mark>\n",
        "application_name": "adv_networking_bench",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run adv_networking_bench --language cpp"
    },
    {
        "metadata": {
            "name": "Advanced Networking Benchmark",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "UDP",
                "Ethernet",
                "IP",
                "TCP"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "adv_network_rx",
                        "version": "1.0"
                    },
                    {
                        "name": "adv_network_tx",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/main.py",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "",
        "application_name": "adv_networking_bench",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run adv_networking_bench --language python"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Segmentation from MONAI Model Zoo Application",
            "authors": [
                {
                    "name": "Jin Li",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.1",
            "changelog": {
                "1.0": "Initial Release",
                "1.1": "Update TensorRTInferenceOp to InferenceOp"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "MONAI",
                "Endoscopy",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": []
            },
            "run": {
                "command": "python3 <holohub_app_source>/tool_segmentation.py --data <holohub_data_dir>",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopic Tool Segmentation from MONAI Model Zoo\nThis endoscopy tool segmentation application runs the MONAI Endoscopic Tool Segmentation from [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation).\n\n\nThis HoloHub application has been verified on the GI Genius sandbox and is currently deployable to GI Genius Intelligent Endoscopy Modules. [GI Genius](https://www.cosmoimd.com/gi-genius/) is Cosmo Intelligent Medical Devices\u2019 AI-powered endoscopy system. This implementation by Cosmo Intelligent Medical Devices showcases the fast and seamless deployment of HoloHub applications on products/platforms running on NVIDIA Holoscan.\n\n## Model\nWe will be deploying the endoscopic tool segmentation model from [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation). <br>\nNote that you could also use the MONAI model zoo repo for training your own semantic segmentation model with your own data, but here we are directly deploying the downloaded MONAI model checkpoint into Holoscan. \n\n\n### Model conversion to ONNX\nBefore deploying the MONAI Model Zoo's trained model checkpoint in Holoscan SDK, we convert the model checkpoint into ONNX. <br>\nYou can choose to \n- download the [MONAI Endoscopic Tool Segmentation Model on NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/monai_endoscopic_tool_segmentation_model) directly and skip the rest of this Model section, or \n- go through the following conversion steps yourself. \n\n 1. Download the PyTorch model checkpoint linked in the README of [endoscopic tool segmentation](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation#model-overview). We will assume its name to be `model.pt`.\n 2. Clone the MONAI Model Zoo repo. \n```\ncd [your-workspace]\ngit clone https://github.com/Project-MONAI/model-zoo.git\n```\nand place the downloaded PyTorch model into `model-zoo/models/endoscopic_tool_segmentation/`.\n\n 3. Pull and run the docker image for [MONAI](https://hub.docker.com/r/projectmonai/monai). We will use this docker image for converting the PyTorch model to ONNX. \n```\ndocker pull projectmonai/monai\ndocker run -it --rm --gpus all -v [your-workspace]/model-zoo:/workspace/model-zoo -w /workspace/model-zoo/models/endoscopic_tool_segmentation/ projectmonai/monai\n```\n 4. Install onnxruntime within the container\n ```\npip install onnxruntime onnx-graphsurgeon\n ```\n 5. Convert model\n \nWe will first export the model.pt file to ONNX by using the [export_to_onnx.py](https://github.com/Project-MONAI/model-zoo/blob/dev/models/endoscopic_tool_segmentation/scripts/export_to_onnx.py) file. Modify the backbone in [line 122](https://github.com/Project-MONAI/model-zoo/blob/dev/models/endoscopic_tool_segmentation/scripts/export_to_onnx.py#L122) to be efficientnet-b2:\n```\nmodel = load_model_and_export(modelname, outname, out_channels, height, width, multigpu, backbone=\"efficientnet-b2\")\n```\nNote that the model in the Model Zoo here was trained to have only two output channels: label 1 = tools, label 0 = everything else, but the same Model Zoo repo can be repurposed to train a model with a different dataset that has more than two classes.\n```\npython scripts/export_to_onnx.py --model model.pt --outpath model_endoscopic_tool_seg.onnx --width 736 --height 480 --out_channels 2\n```\nFold constants in the ONNX model.\n```\npolygraphy surgeon sanitize --fold-constants model_endoscopic_tool_seg.onnx -o model_endoscopic_tool_seg_sanitized.onnx\n```\nFinally, modify the input and output channels to have shape [n, height, width, channels], [n, channels, height, width]. \n```\npython scripts/graph_surgeon_tool_seg.py --orig_model model_endoscopic_tool_seg_sanitized.onnx --new_model model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\n```\n\n## Data\nFor this application we will use the same [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) as the Holoscan SDK reference applications.\n\n## Requirements\nThe only requirement is to make sure the model and data are accessible by the application. At runtime we will need to specify via the `--data` arg, assuming the directory specified contains two subdirectories `endoscopy/` (endoscopy video data directory) and `monai_tool_seg_model/` (model directory).\n\n## Running the application\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\nNext, run the application, where <DATA_DIR> is a directory that contains two subdirectories `endoscopy/` and `monai_tool_seg_model/`.:\n\n```\npython3 tool_segmentation.py --data <DATA_DIR>\n```\nIf you'd like the application to run at the input framerate, change the `replayer` config in the yaml file to `realtime: true`.",
        "application_name": "monai_endoscopic_tool_seg",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run monai_endoscopic_tool_seg --language python"
    },
    {
        "metadata": {
            "name": "Videomaster transmitter example",
            "authors": [
                {
                    "name": "Laurent Radoux",
                    "affiliation": "DELTACAST"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Deltacast",
                "VideoMaster"
            ],
            "ranking": 2,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/deltacast_transmitter --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Deltacast transmitter\n\nThis application demonstrates the use of videomaster_transmitter to transmit a video stream through a dedicated IO device.\n\n### Requirements\n\nThis application uses the DELTACAST.TV capture card for input stream. Contact [DELTACAST.TV](https://www.deltacast.tv/) for more details on how access the SDK and to setup your environment.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nSee instructions from the top level README on how to build this application.\nNote that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the system.\nThis can be done with the following command, from the top level Holohub source directory:\n\n```bash\n./run build deltacast_transmitter --configure-args -DVideoMaster_SDK_DIR=<Path to VideoMasterSDK>\n```\n\n### Run Instructions\n\nFrom the build directory, run the command:\n\n```bash\n./applications/deltacast_transmitter/deltacast_transmitter --data <holohub_data_dir>/endoscopy\n```\n",
        "application_name": "deltacast_transmitter",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run deltacast_transmitter --language cpp"
    },
    {
        "metadata": {
            "name": "Colonoscopy segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Colonoscopy",
                "Classification"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI Colonoscopy Segmentation of Polyps",
                        "description": "This resource contains a segmentation model for the identification of polyps during colonoscopies trained on the Kvasir-SEG dataset [1], using the ColonSegNet model architecture [2], as well as a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_colonoscopy_sample_data"
                    }
                ]
            },
            "run": {
                "command": "python3 colonoscopy_segmentation.py --data <holohub_data_dir>/colonoscopy_segmentation",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Colonoscopy Polyp Segmentation\n\nFull workflow including a generic visualization of segmentation results from a polyp segmentation models.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the colonoscopy data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI Colonoscopy Segmentation of Polyps](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_colonoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/colonoscopy_segmentation\n    python3 colonoscopy_segmentation.py --source=replayer --data=<DATA_DIR>/colonoscopy_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/colonoscopy_segmentation\n    python3 colonoscopy_segmentation.py --source=aja\n    ```\n\n### Holoscan SDK version\n\nColonoscopy segmentation application in HoloHub requires version 0.6+ of the Holoscan SDK.\nIf the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:\n\n* In python/CMakeLists.txt: update the holoscan SDK version from `0.6` to `0.5`\n* In python/multiai_ultrasound.py: `InferenceOp` is replaced with `MultiAIInferenceOp`\n",
        "application_name": "colonoscopy_segmentation",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run colonoscopy_segmentation --language python"
    },
    {
        "metadata": {
            "name": "prohawk_video_replayer",
            "authors": [
                {
                    "name": "Tim Wooldridge",
                    "affiliation": "Prohawk Technology Group"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Added watermark to the Prohawk restoration engine"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.1",
                "tested_versions": [
                    "0.5.1",
                    "0.6.0"
                ]
            },
            "platforms": [
                "arm64"
            ],
            "tags": [
                "Video Processing",
                "Prohawk"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "prohawk",
                        "version": "1.0.0"
                    }
                ]
            },
            "run": {
                "command": "./prohawk_video_replayer --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Prohawk video replayer\n\nThis application utilizes the ProHawk restoration operator along with Holoscan's Video Replayer and Holoviz operators to enhance and restore medical imagery in real-time, offering superior image quality. The user-friendly interface of the application provides a range of filter options, enabling users to dynamically select the most suitable filter for optimal results.\n\n![](screenshot.png)\n\n## ProHawk Vision Restoration Operator \n\nThe ProHawk Vision Operator is a groundbreaking solution that is transforming both healthcare and manufacturing industries by revolutionizing computer vision technology with its patented restoration capabilities. It seamlessly integrates into the NVIDIA Holoscan full-stack infrastructure, fundamentally altering the way healthcare professionals diagnose and treat patients, while also optimizing manufacturing processes. In healthcare, the ProHawk Vision Operator Plugin automatically interprets medical imaging frames, identifies real-world conditions, and employs precise control over ProHawk Vision Restoration algorithms, all driven by an objective mathematical model using quantitative measurements to enhance accuracy in diagnoses and treatments by restoring degraded frames. In manufacturing, the ProHawk Vision Operator Plugin algorithms reveal manufacturing line defects ensuring product quality.\n\n## Application Controls\n\nThe operator can be controlled with keyboard shortcuts:\n\n- **AFS (0)** - Automatic Filter Selection (AFS) \u2013 Automatically and continuously adjust the image pixel values to maximize image detail and visibility.\n- **LowLight (1)** - Lowlight preset filter that corrects lighting compromised imagery.\n- **Vascular Detail (2)** - Vasculature Detail Preset Filter that reveals the fine details of vasculature structures without the need for dye or contrast agents.\n- **Vapor (3)** - Vapor Preset Filter that removes vapor, smoke, and stream from the video.\n- **Disable Restoration (d)** - Disable ProHawk Vision computer vision restoration.\n- **Side-by-Side View (v)** - Display Side-by-Side (restored/non-restores) Video.\n- **Display Menu Items (m)** - Display menus control items.\n- **Quit (q)** - Exit the application\n\n## Data\n\nThe following dataset is used by this application:\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data).\n\n##  Building the application\n\nThe easiest way to build this application is to use the provided Docker file.\n\nFrom the Holohub main directory run the following command:\n\n  ```bash\n  ./dev_container build --docker_file applications/prohawk_video_replayer/Dockerfile --img holohub:prohawk\n  ```\n\nThen launch the container to build the application:\n\n  ```bash\n  ./dev_container launch --img holohub:prohawk\n  ```\n\nInside the container build the application:\n\n  ```bash\n  ./run build prohawk_video_replayer\n  ```\n  \nInside the container run the application:\n\n- C++:\n    ```bash\n    ./run launch prohawk_video_replayer cpp\n    ```\n- Python:\n    ```bash\n    export PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/python/lib/\n    python <prohawk_app_dir>/python/prohawk_video_replayer.py\n    ```\n\nFor more information about this application and operator please visit [https://prohawk.ai/prohawk-vision-operator/#learn](https://prohawk.ai/prohawk-vision-operator/#learn)\nFor technical support or other assistance, please don't hesitate to visit us at [https://prohawk.ai/contact](https://prohawk.ai/contact)\n",
        "application_name": "prohawk_video_replayer",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run prohawk_video_replayer --language cpp"
    },
    {
        "metadata": {
            "name": "prohawk_video_replayer_py",
            "authors": [
                {
                    "name": "Nigel Nelson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Enable Python support"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "arm64"
            ],
            "tags": [
                "Video Processing",
                "Prohawk"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "prohawk",
                        "version": "1.0.0"
                    }
                ]
            },
            "run": {
                "command": "python prohawk_video_replayer.py",
                "workdir": "/workspace/holohub/applications/prohawk_video_replayer/python"
            }
        },
        "readme": "# Prohawk video replayer\n\nThis application utilizes the ProHawk restoration operator along with Holoscan's Video Replayer and Holoviz operators to enhance and restore medical imagery in real-time, offering superior image quality. The user-friendly interface of the application provides a range of filter options, enabling users to dynamically select the most suitable filter for optimal results.\n\n![](screenshot.png)\n\n## ProHawk Vision Restoration Operator \n\nThe ProHawk Vision Operator is a groundbreaking solution that is transforming both healthcare and manufacturing industries by revolutionizing computer vision technology with its patented restoration capabilities. It seamlessly integrates into the NVIDIA Holoscan full-stack infrastructure, fundamentally altering the way healthcare professionals diagnose and treat patients, while also optimizing manufacturing processes. In healthcare, the ProHawk Vision Operator Plugin automatically interprets medical imaging frames, identifies real-world conditions, and employs precise control over ProHawk Vision Restoration algorithms, all driven by an objective mathematical model using quantitative measurements to enhance accuracy in diagnoses and treatments by restoring degraded frames. In manufacturing, the ProHawk Vision Operator Plugin algorithms reveal manufacturing line defects ensuring product quality.\n\n## Application Controls\n\nThe operator can be controlled with keyboard shortcuts:\n\n- **AFS (0)** - Automatic Filter Selection (AFS) \u2013 Automatically and continuously adjust the image pixel values to maximize image detail and visibility.\n- **LowLight (1)** - Lowlight preset filter that corrects lighting compromised imagery.\n- **Vascular Detail (2)** - Vasculature Detail Preset Filter that reveals the fine details of vasculature structures without the need for dye or contrast agents.\n- **Vapor (3)** - Vapor Preset Filter that removes vapor, smoke, and stream from the video.\n- **Disable Restoration (d)** - Disable ProHawk Vision computer vision restoration.\n- **Side-by-Side View (v)** - Display Side-by-Side (restored/non-restores) Video.\n- **Display Menu Items (m)** - Display menus control items.\n- **Quit (q)** - Exit the application\n\n## Data\n\nThe following dataset is used by this application:\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data).\n\n##  Building the application\n\nThe easiest way to build this application is to use the provided Docker file.\n\nFrom the Holohub main directory run the following command:\n\n  ```bash\n  ./dev_container build --docker_file applications/prohawk_video_replayer/Dockerfile --img holohub:prohawk\n  ```\n\nThen launch the container to build the application:\n\n  ```bash\n  ./dev_container launch --img holohub:prohawk\n  ```\n\nInside the container build the application:\n\n  ```bash\n  ./run build prohawk_video_replayer\n  ```\n  \nInside the container run the application:\n\n- C++:\n    ```bash\n    ./run launch prohawk_video_replayer cpp\n    ```\n- Python:\n    ```bash\n    export PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/python/lib/\n    python <prohawk_app_dir>/python/prohawk_video_replayer.py\n    ```\n\nFor more information about this application and operator please visit [https://prohawk.ai/prohawk-vision-operator/#learn](https://prohawk.ai/prohawk-vision-operator/#learn)\nFor technical support or other assistance, please don't hesitate to visit us at [https://prohawk.ai/contact](https://prohawk.ai/contact)\n",
        "application_name": "prohawk_video_replayer",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run prohawk_video_replayer --language python"
    },
    {
        "metadata": {
            "name": "Networked Radar Pipeline",
            "authors": [
                {
                    "name": "Dylan Eustice",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.1",
            "changelog": {
                "1.0": "Initial Release",
                "1.1": "Update to work with ANO 1.2"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Networking",
                "Network",
                "UDP",
                "IP",
                "Signal Processing",
                "RADAR"
            ],
            "ranking": 2,
            "dependencies": {
                "operators": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    },
                    {
                        "name": "advanced_network",
                        "version": "1.2"
                    }
                ],
                "libraries": [
                    {
                        "name": "MatX",
                        "version": "0.6.0",
                        "url": "https://github.com/NVIDIA/MatX.git"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/network_radar_pipeline",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Network Radar Pipeline\nThe Network Radar Pipeline demonstrates signal processing on data streamed via packets over a network. It showcases the use of both the Advanced Network Operator and Basic Network Operator to send or receive data, combined with the signal processing operators implemented in the Simple Radar Pipeline application.\n\nUsing the GPUDirect capabilities afforded by the Advanced Network Operator, this pipeline has been tested up to 100 Gbps (Tx/Rx) using a ConnectX-7 NIC and A30 GPU.\n\nThe motivation for building this application is to demonstrate how data arrays can be assembled from packet data in real-time for low-latency, high-throughput sensor processing applications. The main components of this work are defining a message format and writing code connecting the network operators to the signal processing operators.\n\nThis application supports the Advanced Network Operator DPDK and DOCA GPUNetIO transport layers.\n\n## Prerequisites\nSee the README for the Advanced Network Operator for requirements and system tuning needed to enable high-throughput GPUDirect capabilities.\n\n## Environment\nNote: Dockerfile should be cross-compatible, but has only been tested on x86. Needs to be edited if different versions / architectures are required.\n\n## Build\nPlease refer to the top level Holohub README.md file for information on how to build this application: `./run build network_radar_pipeline`.\n\n## Run\nNote: must properly configure YAML files before running. To run with DPDK as ANO transport layer:\n- On Tx machine: `./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source.yaml`\n- On Rx machine: `./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process.yaml`\n\nTo run with DOCA GPUNetIO as ANO transport layer:\n- On Tx machine: `./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source_doca.yaml`\n- On Rx machine: `./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process_doca.yaml`\n\n<mark>For Holoscan internal reasons (not related to the DOCA library), build the Advanced Network Operator with `RX_PERSISTENT_ENABLED` set to 1 MAY cause problems to this application on the receive (process) side (receive hangs in process.cu file). If you experience any issue on the receive side, please read carefully in the Advanced Network Operator README about how to solve this problem.</mark>\n\n## Network Operator Connectors\nSee each operators' README before using / for more detailed information.\n### Basic Network Operator Connector\nImplementation in `basic_network_connectors`. Only supports CPU packet receipt / transmit. Uses cudaMemcpy to move data between network operator and MatX tensors.\n### Advanced Network Operator Connector\nImplementation in `advanced_network_connectors`. RX connector is only configured to run with GPUDirect enabled, in header-data split (HDS) mode. TX connector supports both GPUDirect/HDS or CPU-only.\n#### Testing RX on generic packet data\nWhen using the Advanced network operator, the application supports testing the radar processing component in a \"spoof packets\" mode. This functionality allows for easier benchmarking of the application by ingesting generic packet data and writing in header fields such that the full radar pipeline will still be exercised. When \"SPOOF_PACKET_DATA\" (adv_networking_rx.h) is set to \"true\", the index of the packet will be used to set fields appropriately. This functionality is currently unsupported using the basic network operator connectors.\n\n## Message format\nThe message format is defined by `RFPacket`. It is a byte array, represented by `RFPacket::payload`, where the first 16 bytes are reserved for metadata and the rest are used for representing complex I/Q samples. The metadata is:\n- Sample index: The starting index for a single pulse/channel of the transmitted samples (2 bytes)\n- Waveform ID: Index of the transmitted waveform (2 bytes)\n- Channel index: Index of the channel (2 bytes)\n- Pulse index: Index of the pulse (2 bytes)\n- Number samples: Number of I/Q samples transmitted (2 bytes)\n- End of array: Boolean - true if this is the last message for the waveform (2 bytes)",
        "application_name": "network_radar_pipeline",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run network_radar_pipeline --language cpp"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Tracking Distributed",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Tracking"
            ],
            "ranking": 0,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/endoscopy_tool_tracking_distributed --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Distributed Endoscopy Tool Tracking\n\nThis application is similar to the Endoscopy Tool Tracking application, but the distributed version divides the application into three fragments:\n\n1. Video Input: get video input from a pre-recorded video file.\n2. Inference: run the inference using LSTM and run the post-processing script.\n3. Visualization: display input video and inference results.\n\nBased on an LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\nThe provided applications are configured to use a pre-recorded endoscopy video (replayer).\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nRun the following command to start the application.  This will run the app with a pre-recorded video as input:\n\n```sh\n./dev_container build_and_run endoscopy_tool_tracking_distributed --language cpp\n```\n",
        "application_name": "endoscopy_tool_tracking_distributed",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run endoscopy_tool_tracking_distributed --language cpp"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Tracking Distributed",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Tracking"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/endoscopy_tool_tracking.py --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Distributed Endoscopy Tool Tracking\n\nThis application is similar to the Endoscopy Tool Tracking application, but the distributed version divides the application into three fragments:\n\n1. Video Input: get video input from a pre-recorded video file.\n2. Inference: run the inference using LSTM and run the post-processing script.\n3. Visualization: display input video and inference results.\n\nBased on an LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to use a pre-recorded endoscopy video (replayer). \n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\n### Run Instructions\n\nRun the following command to start the application.  This will run the app with a pre-recorded video as input:\n\n```sh\n./dev_container build_and_run endoscopy_tool_tracking_distributed --language python\n```\n",
        "application_name": "endoscopy_tool_tracking_distributed",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run endoscopy_tool_tracking_distributed --language python"
    },
    {
        "metadata": {
            "name": "H264 Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "2.1",
            "changelog": {
                "1.0": "Initial Release",
                "2.0": "Upgrade to GXF 4.0",
                "2.1": "Import h.264 GXF Codelets/Components as Holoscan Operators/Resources"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Video Decoding",
                "Video Encoding"
            ],
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "videodecoder",
                        "version": "1.2.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.2.0"
                    },
                    {
                        "name": "videoencoder",
                        "version": "1.2.0"
                    },
                    {
                        "name": "videoencoderio",
                        "version": "1.2.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/h264_endoscopy_tool_tracking h264_endoscopy_tool_tracking.yaml --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# H.264 Endoscopy Tool Tracking Application\n\nThe application showcases how to use H.264 video source as input to and output\nfrom the Holoscan pipeline. This application is a modified version of Endoscopy\nTool Tracking reference application in Holoscan SDK that supports H.264\nelementary streams as the input and output.\n\n_The H.264 video decode operators do not adjust framerate as it reads the\nelementary stream input. As a result the video stream can be displayed as\nquickly as the decoding can be performed. This application uses\n`PeriodicCondition` to play video at the same speed as the source video._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. The recording of the output can be enabled by setting\n`record_output` flag in the config file to `true`. If the `record_output` flag\nin the config file is set to `true`, the output of the pipeline is again\nrecorded to a H.264 elementary stream on the disk, file name / path for this\ncan be specified in the 'h264_endoscopy_tool_tracking.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded when building the application.\n\n## Building and Running H.264 Endoscopy Tool Tracking Application\n\n* Building and running the application from the top level Holohub directory:\n\n```bash\n# C++ version\n./dev_container build_and_run h264_endoscopy_tool_tracking --docker_file applications/h264/Dockerfile --language cpp\n\n# Python version\n./dev_container build_and_run h264_endoscopy_tool_tracking --docker_file applications/h264/Dockerfile --language python\n```\n\nImportant: on aarch64, applications also need tegra folder mounted inside the container and\nthe `LD_LIBRARY_PATH` environment variable should be updated to include\ntegra folder path.\n\nOpen and edit the [Dockerfile](../Dockerfile) and uncomment line 66:\n\n```bash\n# Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n```\n\n\n## Enable recording of the output\n\nThe recording of the output can be enabled by setting `record_output` flag in\nthe config file\n`<build_dir>/applications/h264/endoscopy_tool_tracking/h264_endoscopy_tool_tracking.yaml`\nto `true`.\n\n",
        "application_name": "h264_endoscopy_tool_tracking",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run h264_endoscopy_tool_tracking --language cpp"
    },
    {
        "metadata": {
            "name": "H.264 Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Video Decoding",
                "Video Encoding"
            ],
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "videodecoder",
                        "version": "1.2.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.2.0"
                    },
                    {
                        "name": "videoencoder",
                        "version": "1.2.0"
                    },
                    {
                        "name": "videoencoderio",
                        "version": "1.2.0"
                    }
                ],
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/h264_endoscopy_tool_tracking.py --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# H.264 Endoscopy Tool Tracking Application\n\nThe application showcases how to use H.264 video source as input to and output\nfrom the Holoscan pipeline. This application is a modified version of Endoscopy\nTool Tracking reference application in Holoscan SDK that supports H.264\nelementary streams as the input and output.\n\n_The H.264 video decode operators do not adjust framerate as it reads the\nelementary stream input. As a result the video stream can be displayed as\nquickly as the decoding can be performed. This application uses\n`PeriodicCondition` to play video at the same speed as the source video._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. The recording of the output can be enabled by setting\n`record_output` flag in the config file to `true`. If the `record_output` flag\nin the config file is set to `true`, the output of the pipeline is again\nrecorded to a H.264 elementary stream on the disk, file name / path for this\ncan be specified in the 'h264_endoscopy_tool_tracking.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded when building the application.\n\n## Building and Running H.264 Endoscopy Tool Tracking Application\n\n* Building and running the application from the top level Holohub directory:\n\n```bash\n# C++ version\n./dev_container build_and_run h264_endoscopy_tool_tracking --docker_file applications/h264/Dockerfile --language cpp\n\n# Python version\n./dev_container build_and_run h264_endoscopy_tool_tracking --docker_file applications/h264/Dockerfile --language python\n```\n\nImportant: on aarch64, applications also need tegra folder mounted inside the container and\nthe `LD_LIBRARY_PATH` environment variable should be updated to include\ntegra folder path.\n\nOpen and edit the [Dockerfile](../Dockerfile) and uncomment line 66:\n\n```bash\n# Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n```\n\n\n## Enable recording of the output\n\nThe recording of the output can be enabled by setting `record_output` flag in\nthe config file\n`<build_dir>/applications/h264/endoscopy_tool_tracking/h264_endoscopy_tool_tracking.yaml`\nto `true`.\n\n",
        "application_name": "h264_endoscopy_tool_tracking",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run h264_endoscopy_tool_tracking --language python"
    },
    {
        "metadata": {
            "name": "H.264 Video Decode Reference Application",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "2.0",
            "changelog": {
                "1.0": "Initial Release",
                "2.0": "Upgrade to GXF 4.0",
                "2.1": "Import h.264 GXF Codelets/Components as Holoscan Operators/Resources"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "H.264",
                "Video Decoding"
            ],
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "videodecoder",
                        "version": "1.2.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.2.0"
                    }
                ]
            },
            "run": {
                "command": "./h264_video_decode h264_video_decode.yaml --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# H.264 Video Decode Reference Application\n\nThis is a minimal reference application demonstrating usage of H.264 video\ndecode operators. This application makes use of H.264 elementary stream reader\noperator for reading H.264 elementary stream input and uses Holoviz operator\nfor rendering decoded data to the native window.\n\n_The H.264 video decode operators do not adjust framerate as it reads the\nelementary stream input. As a result the video stream can be displayed as\nquickly as the decoding can be performed. This application uses\n`PeriodicCondition` to play video at the same speed as the source video._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. To use any other stream, the filename / path for the\ninput file can be specified in the 'h264_video_decode.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded when building the application.\n\n## Building and Running H.264 Endoscopy Tool Tracking Application\n\n* Building and running the application from the top level Holohub directory:\n\n```bash\n# C++ version\n./dev_container build_and_run h264_video_decode --docker_file applications/h264/Dockerfile --language cpp\n\n# Python version\n./dev_container build_and_run h264_video_decode --docker_file applications/h264/Dockerfile --language python\n\n```\n\nImportant: on aarch64, applications also need tegra folder mounted inside the container and\nthe `LD_LIBRARY_PATH` environment variable should be updated to include\ntegra folder path.\n\nOpen and edit the [Dockerfile](../Dockerfile) and uncomment line 66:\n\n```bash\n# Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n```",
        "application_name": "h264_video_decode",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run h264_video_decode --language cpp"
    },
    {
        "metadata": {
            "name": "H.264 Video Decode Reference Application",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "H264",
                "Video Decoding"
            ],
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "videodecoder",
                        "version": "1.2.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.2.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/h264_video_decode.py --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# H.264 Video Decode Reference Application\n\nThis is a minimal reference application demonstrating usage of H.264 video\ndecode operators. This application makes use of H.264 elementary stream reader\noperator for reading H.264 elementary stream input and uses Holoviz operator\nfor rendering decoded data to the native window.\n\n_The H.264 video decode operators do not adjust framerate as it reads the\nelementary stream input. As a result the video stream can be displayed as\nquickly as the decoding can be performed. This application uses\n`PeriodicCondition` to play video at the same speed as the source video._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. To use any other stream, the filename / path for the\ninput file can be specified in the 'h264_video_decode.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded when building the application.\n\n## Building and Running H.264 Endoscopy Tool Tracking Application\n\n* Building and running the application from the top level Holohub directory:\n\n```bash\n# C++ version\n./dev_container build_and_run h264_video_decode --docker_file applications/h264/Dockerfile --language cpp\n\n# Python version\n./dev_container build_and_run h264_video_decode --docker_file applications/h264/Dockerfile --language python\n\n```\n\nImportant: on aarch64, applications also need tegra folder mounted inside the container and\nthe `LD_LIBRARY_PATH` environment variable should be updated to include\ntegra folder path.\n\nOpen and edit the [Dockerfile](../Dockerfile) and uncomment line 66:\n\n```bash\n# Uncomment the following line for aarch64 support\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/aarch64-linux-gnu/tegra/\n```",
        "application_name": "h264_video_decode",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run h264_video_decode --language python"
    },
    {
        "metadata": {
            "name": "Endoscopy Out of Body Detection Pipeline in C++",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Classification"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI Endoscopy Out Of Body Detection",
                        "description": "This resource contains a detection model which classifies if the input frame of an endoscopy video is inside the body or out of the body, as well as a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection",
                        "license": "https://developer.nvidia.com/clara-holoscan-EULA"
                    }
                ]
            },
            "run": {
                "command": "./endoscopy_out_of_body_detection endoscopy_out_of_body_detection.yaml --data <holohub_data_dir>/endoscopy_out_of_body_detection",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Endoscopy Out of Body Detection Application\n\nThis application performs endoscopy out of body detection. The application classifies if the input frame is inside the body or out of the body. If the input frame is inside the body, application prints `Likely in-body`, otherwise `Likely out-of-body`. Each likelihood is accompanied with a confidence score. If the analytics is enabled, the output for each input frame is instead exported to the given csv file.\n\n__Note: there is no visualization component in the application.__\n\n`endoscopy_out_of_body_detection.yaml` is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the `basename` and the `directory` field in `replayer`.\n\n## Data\n\n__Note: the data is automatically downloaded and converted when building. If you need to manually convert the data follow the following steps.__\n\n\n* Endoscopy out of body detection model and the sample dataset is available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection)\n  After downloading the data in mp4 format, it must be converted into GXF tensors.\n* Script for GXF tensor conversion (`convert_video_to_gxf_entities.py`) is available with the Holoscan SDK, and can be accessed [here](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts)\n\n### Unzip and convert the sample data:\n\n```\n# unzip the downloaded sample data\nunzip [FILE].zip -d <data_dir>\n\n# convert video file into tensor\nffmpeg -i <INPUT_VIDEO_FILE> -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | python convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n\n# where <INPUT_VIDEO_FILE> is one of the downloaded MP4 files: OP1-out-2.mp4, OP4-out-8.mp4 or OP8-out-4.mp4.\n```\n\nMove the model file and converted video tensor into a directory structure similar to the following:\n\n```bash\ndata\n\u2514\u2500\u2500 endoscopy_out_of_body_detection\n    \u251c\u2500\u2500 LICENSE.md\n    \u251c\u2500\u2500 out_of_body_detection.onnx\n    \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n    \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n    \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n```\n\n## Building the application\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\nAdditionally, if the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:\n\n* In main.cpp: `#include <holoscan/operators/inference/inference.hpp>` is replaced with `#include <holoscan/operators/multiai_inference/multiai_inference.hpp>`\n* In main.cpp: `#include <holoscan/operators/inference_processor/inference_processor.hpp>` is replaced with `#include <holoscan/operators/multiai_postprocessor/multiai_postprocessor.hpp>`\n* In main.cpp: `ops::InferenceOp` is replaced with `ops::MultiAIInferenceOp`\n* In main.cpp: `ops::InferenceProcessorOp` is replaced with `ops::MultiAIPostprocessorOp`\n* In CMakeLists.txt: update the holoscan SDK version from `0.6` to `0.5`\n* In CMakeLists.txt: `holoscan::ops::inference` is replaced with `holoscan::ops::multiai_inference`\n* In CMakeLists.txt: `holoscan::ops::inference_processor` is replaced with `holoscan::ops::multiai_postprocessor`\n\n## Running the application\n\nIn your `build` directory, run\n\n```bash\napplications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection --data ../data/endoscopy_out_of_body_detection\n```\n\n## Enable analytics\n\nThe endoscopy out of body detetction application supports exporting output to the comma separated value (CSV) files. This data can later be used by analytics applications. The analytics data generation can be enabled by setting `enable_analytics` flag in the config file `<build_dir>/applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection.yaml` to `true`.\n\nThe data root directory can be specified using the environment variable `HOLOSCAN_ANALYTICS_DATA_DIRECTORY`. If not specified, it defaults to the current directory. The CSV data file name can be specified using the environment variable `HOLOSCAN_ANALYTICS_DATA_FILE_NAME`. If not specified, it defaults to the name `data.csv`. All the generated data will be stored inside a directory with the same name as the application name that is passed in the configuration file. On each run, a new directory inside the data directory will be created and a new data file will be created inside it. Each new data directory will be named with the current timestamp.\n",
        "application_name": "endoscopy_out_of_body_detection",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run endoscopy_out_of_body_detection --language cpp"
    },
    {
        "metadata": {
            "name": "matlab_image_processing",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "MathWorks Team",
                    "affiliation": "MathWorks"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "MATLAB",
                "Image Processing",
                "Signal Processing",
                "Computer Vision",
                "CUDA"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "MATLAB",
                        "version": "R2023b"
                    }
                ]
            }
        },
        "readme": "# Image Processing with MATLAB GPU Coder\n\nThis application does real-time image processing of Holoscan sample data. The image processing is implemented in MATLAB and converted to CUDA using GPU Coder. When the application is run, Holoviz will display the processed data in real time.\n\n## Folder Structure\n\n```sh\nmatlab_image_processing\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_image_processing_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_image_processing_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 matlab_image_processing.m  # MATLAB function that CUDA code is generated from\n\u2502   \u2514\u2500\u2500 test_image_processing.m  # MATLAB script to test MATLAB function\n\u251c\u2500\u2500 CMakeLists.txt  # CMake build file\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_image_processing.yaml  # Ultrasound beamforming config\n```\n\n## Generate CUDA Code with MATLAB GPU Coder\n\n### x86: Ubuntu\n\nIn order to generate the CUDA Code, start MATLAB and `cd` to the `matlab` folder and open the `generate_image_processing_x86.m` script. Run the script and a folder `codegen/dll/matlab_image_processing` will be generated in the `matlab_image_processing` folder.\n\n### arm64: Jetson\n\nOn an x86 computer with MATLAB installed, `cd` to the `matlab` folder and open the `generate_image_processing_jetson.m` script. Having an `ssh` connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the `hwobj` on line 7, also replace `<ABSOLUTE_PATH>` of `cfg.Hardware.BuildDir` on line 39, as the absolute path (on the Jetson device) to `holohub` folder. Run the script and a folder `MATLAB_ws` will be created in the `matlab_image_processing` folder.\n\n## Configure Holoscan for MATLAB\n\nIf you have not already, start by building HoloHub:\n```sh\n./dev_container build\n```\n\n### x86: Ubuntu\n\nDefine the environment variable:\n```sh\nexport MATLAB_ROOT=\"/usr/local/MATLAB\"\nexport MATLAB_VERSION=\"R2023b\"\n```\nwhere you, if need be, replace `MATLAB_ROOT` with the location of your MATLAB install and `MATLAB_VERSION` with the correct version.\n\nNext, run the HoloHub Docker container:\n```sh\n./dev_container launch \\\n    --add-volume ${MATLAB_ROOT}/${MATLAB_VERSION} \\\n    --docker_opts \"-e MATLAB_ROOT=/workspace/volumes/${MATLAB_VERSION}\"\n```\nand build the endoscopy tool tracking application to download the necessary data:\n```sh\n./run build endoscopy_tool_tracking\n```\n\n### arm64: Jetson\n\nThe folder `MATLAB_ws`, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the `codegen` folder in the `CMakeLists.txt`, in order for the build to find the required libraries. Set the variable `REL_PTH_MATLAB_CODEGEN` to the relative path where the `codegen` folder is located in the `MATLAB_ws` folder. For example, if GPU Coder created the following folder structure on the Jetson device:\n```sh\nmatlab_gpu_coder\n\u2514\u2500\u2500 MATLAB_ws\n    \u2514\u2500\u2500 R2023b\n        \u2514\u2500\u2500 C\n            \u2514\u2500\u2500 Users\n                \u2514\u2500\u2500 Jensen\n                    \u2514\u2500\u2500 holohub\n                        \u2514\u2500\u2500 applications\n                            \u2514\u2500\u2500 matlab_gpu_coder\n                                \u2514\u2500\u2500 matlab_image_processing\n                                    \u2514\u2500\u2500 matlab\n                                        \u2514\u2500\u2500 codegen\n```\nthe variable should be set as:\n```sh\nREL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab_image_processing/matlab/codegen\n```\nNext, run the HoloHub Docker container:\n```sh\n./dev_container launch\n```\nand build the endoscopy tool tracking application to download the necessary data:\n```sh\n./run build endoscopy_tool_tracking\n```",
        "application_name": "matlab_image_processing",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run matlab_image_processing --language cpp"
    },
    {
        "metadata": {
            "name": "matlab_beamform",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "MathWorks Team",
                    "affiliation": "MathWorks"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "MATLAB",
                "Ultrasound",
                "Beamforming",
                "CUDA"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "MATLAB",
                        "version": "R2023b"
                    }
                ]
            }
        },
        "readme": "# Ultrasound Beamforming with MATLAB GPU Coder\n\nThis application does real-time ultrasound beamforming of simulated data. The beamforming algorithm is implemented in MATLAB and converted to CUDA using MATLAB GPU Coder. When the application is run, Holoviz will display the beamformed data in real time.\n\n## Folder Structure\n\n```sh\nmatlab_beamform\n\u251c\u2500\u2500 data  # Data is generated with generate_data.mlx\n\u2502   \u2514\u2500\u2500 ultrasound_beamforming.bin  # Simulated ultrasound data\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_beamform_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_beamform_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 generate_data.mlx  # MATLAB script to generate simulated data\n\u2502   \u2514\u2500\u2500 matlab_beamform.m  # MATLAB function that CUDA code is generated from\n\u251c\u2500\u2500 CMakeLists.txt  # CMake build file\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_beamform.yaml  # Ultrasound beamforming config\n```\n\n## Generate Simulated Data\n\nThe required MATLAB Toolboxes are:\n* [Phased Array System Toolbox](https://uk.mathworks.com/products/phased-array.html)\n* [Communications Toolbox](https://uk.mathworks.com/products/communications.html)\n\nSimply run the script `matlab/generate_data.mlx` from MATLAB and a binary file `ultrasound_beamforming.bin` will be written to a top-level `data` folder. The binary file contains the simulated ultrasound data, prior to beamforming.\n\n## Generate CUDA Code with MATLAB GPU Coder\n\n### x86: Ubuntu\n\nIn order to generate the CUDA Code, start MATLAB and `cd` to the `matlab` folder and open the `generate_beamform_x86.m` script. Run the script and a folder `codegen/dll/matlab_beamform` will be generated in the `matlab_beamform` folder.\n\n### arm64: Jetson\n\nOn an x86 computer with MATLAB installed, `cd` to the `matlab` folder and open the `generate_beamform_jetson.m` script. Having an `ssh` connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the `hwobj` on line 7, also replace `<ABSOLUTE_PATH>` of `cfg.Hardware.BuildDir` on line 39, as the absolute path (on the Jetson device) to `holohub` folder. Run the script and a folder `MATLAB_ws` will be created in the `matlab_beamform` folder.\n\n## Configure Holoscan for MATLAB\n\nIf you have not already, start by building HoloHub:\n```sh\n./dev_container build\n```\n\n#### x86: Ubuntu\n\nDefine the environment variable:\n```sh\nexport MATLAB_ROOT=\"/usr/local/MATLAB\"\nexport MATLAB_VERSION=\"R2023b\"\n```\nwhere you, if need be, replace `MATLAB_ROOT` with the location of your MATLAB install and `MATLAB_VERSION` with the correct version.\n\nNext, run the HoloHub Docker container:\n```sh\n./dev_container launch \\\n    --add-volume ${MATLAB_ROOT}/${MATLAB_VERSION} \\\n    --docker_opts \"-e MATLAB_ROOT=/workspace/volumes/${MATLAB_VERSION}\"\n```\n\n#### arm64: Jetson\n\nThe folder `MATLAB_ws`, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the `codegen` folder in the `CMakeLists.txt`, in order for the build to find the required libraries. Set the variable `REL_PTH_MATLAB_CODEGEN` to the relative path where the `codegen` folder is located in the `MATLAB_ws` folder. For example, if GPU Coder created the following folder structure on the Jetson device:\n```sh\nmatlab_beamform\n\u2514\u2500\u2500 MATLAB_ws\n    \u2514\u2500\u2500 R2023b\n        \u2514\u2500\u2500 C\n            \u2514\u2500\u2500 Users\n                \u2514\u2500\u2500 Jensen\n                    \u2514\u2500\u2500 holohub\n                        \u2514\u2500\u2500 applications\n                            \u2514\u2500\u2500 matlab_gpu_coder\n                                \u2514\u2500\u2500 matlab_beamform\n                                    \u2514\u2500\u2500 matlab\n                                        \u2514\u2500\u2500 codegen\n```\nthe variable should be set as:\n```sh\nREL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab_beamform/matlab/codegen\n```\n\nNext, run the HoloHub Docker container:\n```sh\n./dev_container launch\n```",
        "application_name": "matlab_beamform",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run matlab_beamform --language cpp"
    },
    {
        "metadata": {
            "name": "Object detection using frcnn based pytorch model in C++",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Object detection"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "./object_detection_torch --data <holohub_data_dir>/object_detection_torch",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Object Detection Application\n\nThis application performs object detection using frcnn resnet50 model from torchvision.\nThe inference is executed using `torch` backend in `holoinfer` module in Holoscan SDK.\n\n`object_detection_torch.yaml` is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the `basename` and the `directory` field in `replayer`.\n\nThis application need `Libtorch` for inferencing. Ensure that the Holoscan SDK is build with `build_libtorch` flag as true. If not, then rebuild the SDK with following: `./run build --build_libtorch true` before running this application.\n\n## Data\n\nTo run this application, you will need the following:\n\n- Model name: frcnn_resnet50_t.pt\n    - The model should be converted to torchscript format.  The original pytorch model can be downloaded from [pytorch model](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html). `frcnn_resnet50_t.pt` is used\n- Model configuration file: frcnn_resnet50_t.yaml\n    - Model config documents input and output nodes, their dimensions and respective datatype.\n- Labels file: labels.txt\n    - Labels for identified objects.\n- Postprocessor configuration file: postprocessing.yaml\n    - This configuration stores the number and type of objects to be identified. By default, the application detects and generates bounding boxes for `car` (max 50), `person` (max 50), `motorcycle` (max 10) in the input frame. All remaining identified objects are tagged with label `object` (max 50).\n    - Additionally, color of the bounding box for each identified object can be set.\n    - Threshold of scores can be set in the `params`. Default value is 0.75.\n\nSample dataset can be any video file freely available for testing on the web. E.g. [Traffic video](https://www.pexels.com/video/cars-on-highway-854671/)\n\nOnce the video is downloaded, it must be [converted into GXF entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#usage). As shown in the command below, width and height is set to 1920x1080 by default. To reduce the size of generated tensors a lower resolution can be used. Generated entities must be saved at <data_dir>/object_detection_torch folder.\n\n```bash\nffmpeg -i <downloaded_video> -pix_fmt rgb24 -f rawvideo pipe:1 | python utilities/convert_video_to_gxf_entities.py --width 1920 --height 1080 --channels 3 --framerate 30\n```\n\nIf resolution is updated in entity generation, it must be updated in the following config files as well:\n<data_dir>/object_detection_torch/frcnn_resnet50_t.yaml\n<data_dir>/object_detection_torch/postprocessing.yaml\n\n## Building the application\n\nThe best way to run this application is inside the container, as it would provide all the required third-party packages:\n\n```bash\n# Create the container image for this application\n./dev_container build --docker_file applications/object_detection_torch/Dockerfile --img object_detection_torch\n# Launch the container\n./dev_container launch --img object_detection_torch\n# Build the application. Note that this downloads the video data as well\n./run build object_detection_torch\n# Generate the pytorch model\npython3 applications/object_detection_torch/generate_resnet_model.py  data/object_detection_torch/frcnn_resnet50_t.pt\n# Run the application\n./run launch object_detection_torch\n```\n\nPlease refer to the top level Holohub README.md file for more information on how to build this application.\n\n## Running the application\n\n```bash\n# ensure the current working directory contains the <data_dir>.\n<build_dir>/object_detection_torch\n```\n\nIf application is executed from within the holoscan sdk container and is not able to find `libtorch.so`, update `LD_LIBRARY_PATH` as below:\n\n```bash\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/libtorch/1.13.1/lib\n```\n\nOn aarch64, if application is executed from within the holoscan sdk container and libtorch throws linking errors, update the `LD_LIBRARY_PATH` as below:\n\n```bash\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/opt/hpcx/ompi/lib\"\n```\n",
        "application_name": "object_detection_torch",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run object_detection_torch --language cpp"
    },
    {
        "metadata": {
            "name": "Basic PDW Pipeline",
            "authors": [
                {
                    "name": "Joshua Anderson",
                    "affiliation": "GTRI"
                },
                {
                    "name": "Christopher Jones",
                    "affiliation": "GTRI"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "UDP",
                "Radar",
                "Electronic Support"
            ],
            "ranking": 4,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/simple_pdw_pipeline",
                "workdir": "holohub_bin"
            }
        },
        "readme": "Simple PDW Pipeline\n==================================\n\nThis is a Holoscan pipeline that shows the possibility of using Holoscan as a\nPulse Description Word (PDW) generator. This is a process that takes in IQ\nsamples (signals represented using time-series complex numbers) and picks out\npeaks in the signal that may be transmissions from another source. These PDW\nprocessors are used to see what is transmitting in your area, be they radio\ntowers or radars.\n\nsiggen.c a signal generator written in C that will transmit\nthe input to this pipeline. \n\nBasicNetworkOpRx\n--------------------------\n\nThis uses the Basic Network Operator to read udp packets this operator is\ndocumented elsewhere. \n\nPacketToTensorOp\n-------------------------\n\nThis converts the bytes from the Basic Network Operator into the packets used\nin the rest of the pipeline. The format of the incoming packets is a 16-bit id\nfollowed by 8192 IQ samples each sample has the following format:\n16 bits (I)\n16 bits (Q)\n\nFFTOp\n------------------------\nDoes what it says on the tin. Takes an FFT of the input data. Also shifts data\nso that 0 Hz is centered.\n\n\nThresholdingOp:\n------------------------\nDetects samples over a threshold and then packetizes the runs of samples that\nare above the threshold as a \u201cpulse\u201d.\n\n\nPulseDescriptiorOp\n------------------------\nTakes simple statistics of input pulses. This is where I am most excited for\nfuture work, but that is not the point of this particular project.\n\n\nPulsePrinterOp\n----------------------\nPrints the pulse to screen. Also optionally sends packets to a BasicNetworkOpTx. \nThe transmitted network packets have the following format:\nEach of the following fields are 16bit unsigned integers\n  id\n  low bin\n  high bin\n  zero bin\n  sum power\n  max amplitude\n  average amplitude\n",
        "application_name": "simple_pdw_pipeline",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run simple_pdw_pipeline --language cpp"
    },
    {
        "metadata": {
            "name": "Real-Time Face and Text Deidentification App",
            "authors": [
                {
                    "name": "Wendell Hom",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Jonathan McLeod",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Computer Vision",
                "Detection"
            ],
            "ranking": 2,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet",
                "libraries": [
                    {
                        "name": "easyocr",
                        "version": "1.7.1"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/video_deidentification.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Real-Time Face and Text Deidentification\n<center> <img src=\"./docs/video_deid.gif\" ></center>\n\nThis sample application demonstrates the use of face and text detection models to do real-time video deidentification.\nRegions identified to be face or text are blurred out from the final image.\n\n> **_NOTE:_** This application is a demonstration of real-time face and text deidentification and is not meant to be used in critical applications\nthat has zero error tolerance.  The models used in this sample application have limitations, e.g., in detecting faces and text that are\npartially occluded, in low lighting situations, when there is motion blur, etc.\n\n## Models\n\nThis application uses TAO PeopleNet model from [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet) for detecting faces.\nThe model is downloaded when building the application.\n\nFor text detection, this application uses [EasyOCR](https://github.com/JaidedAI/EasyOCR) python library which uses Character Region Awareness for Text Detection [(CRAFT)](https://github.com/clovaai/CRAFT-pytorch).\n\n## Data\n\nThis application downloads a pre-recorded video from [Pexels](https://www.pexels.com/video/young-traveler-walking-in-the-streets-of-milan-5271997/) when the application is built for use with this application.  Please review the [license terms](https://www.pexels.com/license/) from Pexels.\n\n> **_NOTE:_** The user is responsible for checking if the dataset license is fit for the intended purpose.\n\n## Input\n\nThis app currently supports three different input options:\n\n1. v4l2 compatible input device (default, see V4L2 Support below)\n2. pre-recorded video (see Video Replayer Support below)\n\n## Run Instructions\n\n## V4L2 Support\n\nThis application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device,\nplease plug in your input device and run:\n```sh\n./dev_container build_and_run video_deidentification\n```\n\nBy default, this application expects the input device to be mounted at `/dev/video0`.  If this is not the case, please update\n`applications/video_deidentification/video_deidentification.yaml` and set it to use the corresponding input device before\nrunning the application.  You can also override the default input device on the command line by running:\n```sh\n./dev_container build_and_run video_deidentification --run_args \"--video_device /dev/video0\"\n```\n\n## Video Replayer Support\n\nIf you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video.\nTo launch the application using the Video Stream Replayer as the input source, run:\n\n```sh\n./dev_container build_and_run video_deidentification --run_args \"--source replayer\"\n```\n",
        "application_name": "video_deidentification",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run video_deidentification --language python"
    },
    {
        "metadata": {
            "name": "Basic Networking Ping",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Networking",
                "Network",
                "UDP",
                "IP"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/basic_networking_ping",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Basic Networking Ping\n\nThis application takes the existing ping example that runs over Holoscan ports and instead uses the basic\nnetwork operator to run over a UDP socket.\n\nThe basic network operator allows users to send and receive UDP messages over a standard Linux socket.\nSeparate transmit and receive operators are provided so they can run independently and better suit\nthe needs of the application.\n\n### Configuration\n\nThe application is configured using the file basic_networking_ping_rx.yaml or basic_networking_ping_tx.yaml,\nwhere RX will receive packets and TX will transmit. Depending on how the machine is configured, the IP and\nUDP port likely need to be configured. All other settings do not need to be changed.\n\nPlease refer to the basic network operator documentation for more configuration information.\n\n### Requirements\n\nThis application requires:\n1. Linux\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nRunning the sample uses the standard HoloHub `run` script:\n\n\n```bash\n./run launch basic_networking_ping <language> --configure-args config_file.yaml\n```\n\nLanguage can be either C++ or Python.\n",
        "application_name": "basic_networking_ping",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run basic_networking_ping --language cpp"
    },
    {
        "metadata": {
            "name": "Basic Networking Benchmark",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "UDP",
                "Ethernet",
                "IP",
                "TCP"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/basic_networking_ping.py",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Basic Networking Ping\n\nThis application takes the existing ping example that runs over Holoscan ports and instead uses the basic\nnetwork operator to run over a UDP socket.\n\nThe basic network operator allows users to send and receive UDP messages over a standard Linux socket.\nSeparate transmit and receive operators are provided so they can run independently and better suit\nthe needs of the application.\n\n### Configuration\n\nThe application is configured using the file basic_networking_ping_rx.yaml or basic_networking_ping_tx.yaml,\nwhere RX will receive packets and TX will transmit. Depending on how the machine is configured, the IP and\nUDP port likely need to be configured. All other settings do not need to be changed.\n\nPlease refer to the basic network operator documentation for more configuration information.\n\n### Requirements\n\nThis application requires:\n1. Linux\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nRunning the sample uses the standard HoloHub `run` script:\n\n\n```bash\n./run launch basic_networking_ping <language> --configure-args config_file.yaml\n```\n\nLanguage can be either C++ or Python.\n",
        "application_name": "basic_networking_ping",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run basic_networking_ping --language cpp"
    },
    {
        "metadata": {
            "name": "Streaming Synthetic Aperture Radar",
            "authors": [
                {
                    "name": "Dan Campbell",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Amanda Butler",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "0.1.0",
            "changelog": {
                "0.1": "Initial Implementation - backprojection only, python only"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Radar",
                "SAR",
                "Synthetic Aperture"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "holoviz",
                        "version": "x.x.x"
                    }
                ]
            },
            "run": {
                "command": "python3 ./holosar.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Holoscan SAR\n\n## Description\nThis application is a demonstration of using Holoscan to construct Synthetic Aperture Radar (SAR) imagery from a data collection.  In current form, the data is assumed to be precollected and contained in a particular binary format.  It has been tested with 2 versions of the publicly available GOTCHA volumetric SAR data collection.  Python-based converters are included to manipulate the public datasets into the binary format expected by the application.  The application implements Backprojection for image formation.\n<!-- , in both Python and C++.  The Python implementation is accelerated via CuPy, and is backwardly compatible with Numpy.  The C++ implementation is accelerated with MatX.\n-->\n\n\n## Requirements\n* Holoscan (>=0.5)\n* Python implementation:\n    * Python3\n    * CuPy or Numpy\n    * Pillow\n* Scripts in ``deploy/`` will build and execute a docker environment that meets the requirements for systems using nvidia-docker\n<!-- \n    * C++:\n        * CUDA Toolkit\n* Requirements are conveniently met by building and deploying the Holoscan development container [add link]\n--> \n\n## Obtain and Format GOTCHA Dataset\n* Navigate to https://www.sdms.afrl.af.mil/index.php?collection=gotcha \n* Click the DOWNLOAD link below the images\n* Log in.  You may need to create an account to do so\n* Under \"GOTCHA Volumetric SAR Data Set Challenge Problem\" download \"Disc 1 of 2\".\n    * The data in \"Disc 2 of 2\" is compatible with this demo but not used\n* Unpack the contents of \"Disc 1 of 2\" into the ``data/`` directory.  This should create a subdirectry named ``GOTCHA-CP_Disc1/``\n* ``cd data``\n* ``python3 cp-large_convert.py``\n* This should create a data file named ``gotcha-cp-td-os.dat`` that has a file size 2766987672 bytes, and a md5sum of 554b509c2d5c2c3de8e5643983a9748d\n\n## Build and Use Docker Container (Optional)\n* This demonstration is distributed with tools to build a docker container that meets the demonstration's system requirements.  This approach will only work properly with nvidia-docker\n* From the demonstration root directory:\n* ```cd deploy```\n* ```bash build_application_container.sh``` - this will build the container\n* ```bash run_application_container.sh``` - this will launch a container that meets the demonstration system requirements\n\n## Build and Execute\n<!-- * Place ```.dat``` files in ```data/``` -->\n* Python: \n    * ```python3 holosar.py```\n\nThe application will create a window with the resolved SAR image, and update after each group of 100 pulses received.  The image represents the strength of reflectivity at points on the ground within the imaging window.  The text at the top of the window indicates the (X,Y) position of the collecting radar at the most recent pulse, along with the total count of pulses received.  The red line points in the direction of the collection vehicle's location at the most recent pulse.  \n\nA screen grab is included below for reference:\n\n![image](sar-grab.png)\n\n<!-- \n* C++:\n    * ```cd cpp```\n    ```mkdir build && cd build```\n    ```cmake ..```\n    ```make```\n    ```./holosar```\n* The application will generate one or more image files (depending on configuration) containing the SAR imagery.  For the backprojection algorithm, there will be image files for various number of ingested pulses, showing the evolution of the formed SAR imagery.  For Polar Format Algorithm, only complete images will be emitted.\n--> \n",
        "application_name": "synthetic_aperture_radar",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run synthetic_aperture_radar --language python"
    },
    {
        "metadata": {
            "name": "Simple Radar Pipeline in C++",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.4.0",
                "tested_versions": [
                    "0.4.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Signal Processing",
                "RADAR"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "MatX",
                        "version": "0.2.5",
                        "url": "https://github.com/NVIDIA/MatX.git"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/simple_radar_pipeline",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Simple Radar Pipeline Application\n\nThis demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through:\n1. Pulse Compression\n2. Moving Target Indication (MTI) Filtering\n3. Range-Doppler Map\n4. Constant False Alarm Rate (CFAR) Analysis\n\nWhile this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the `SignalGeneratorOperator`.\n\nThe output of this demonstration is a measure of the number of pulses per second processed on GPU.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n## Building the application\nMake sure CMake (https://www.cmake.org) is installed on your system (minimum version 3.20)\n\n- [Holoscan Debian Package](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_dev_deb) - Follow the instructions in the link to install the latest version of Holoscan Debian package from NGC.\n\n- Create a build directory:\n  ```bash\n  mkdir -p <build_dir> && cd <build_dir>\n  ```\n- Configure with CMake:\n\n  Make sure CMake can find your installation of the Holoscan SDK. For example, setting `holoscan_ROOT` to its install directory during configuration:\n\n  ```bash\n  cmake -S <source_dir> -B <build_dir> -DAPP_simple_radar_pipeline=1 \n  ```\n\n  _Notes:_\n  _If the error `No CMAKE_CUDA_COMPILER could be found` is encountered, make sure that the :code:`nvcc` executable can be found by adding the CUDA runtime location to your `PATH` variable:_\n\n  ```\n  export PATH=$PATH:/usr/local/cuda/bin\n  ```\n\n- Build:\n\n  ```bash\n  cmake --build <build_dir>\n  ```\n\n## Running the application\n```bash\n<build_dir>/simple_radar_pipeline\n```\n\n",
        "application_name": "simple_radar_pipeline",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run simple_radar_pipeline --language cpp"
    },
    {
        "metadata": {
            "name": "Simple Classical Radar Pipeline",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.4.0",
                "tested_versions": [
                    "0.4.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Aerospace, Defense, Communications"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.23.2"
                    },
                    {
                        "name": "cupy",
                        "version": "11.4"
                    },
                    {
                        "name": "cusignal",
                        "version": "22.12"
                    }
                ]
            },
            "run": {
                "command": "python3 simple_radar_pipeline.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Simple Radar Pipeline Application\n\nThis demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through:\n1. Pulse Compression\n2. Moving Target Indication (MTI) Filtering\n3. Range-Doppler Map\n4. Constant False Alarm Rate (CFAR) Analysis\n\nWhile this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the `SignalGeneratorOperator`.\n\nThe output of this demonstration is a measure of the number of pulses per second processed on GPU.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal\npip install holoscan\n```\n\nThe simple radar signal processing pipeline example can then be run via\n```\npython applications/simple_radar_pipeline/simple_radar_pipeline.py\n```\n",
        "application_name": "simple_radar_pipeline",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run simple_radar_pipeline --language python"
    },
    {
        "metadata": {
            "name": "Velodyne Lidar Viewer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "nvMap Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "nvMap Embedded Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Tom Birdsong",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Julien Jomier",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Jiahao Yin",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Marlene Wan",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Lidar",
                "Velodyne",
                "Point Cloud",
                "Visualization",
                "Sensor"
            ],
            "ranking": 4,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/velodyne_lidar_app <holohub_app_source>/lidar.yaml",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Velodyne VLP-16 Lidar Viewer Application\n\n![HoloViz Visual of Highway Sample Lidar Data](./doc/VLP10HzMontereyHighway.gif)\n\n## Overview\n\nIn this application we demonstrate how to use Holoscan SDK for low-latency lidar processing.\nWe receive lidar packets from a Velodyne VLP-16 lidar sensor, convert packet information to\na rolling Cartesian point cloud on GPU, then visualize the results with HoloViz.\n\n## Background\n\n\"Lidar\" (LIght Detection And Ranging) is a technique by which \"light\", typically of\nwavelengths in the infrared spectrum, is used to determine the position of reflective\nsurfaces surrounding a sensor. A 3D lidar sensor often employs a stacked vertical array of\ninfrared laser emitters and sources that it spins rapidly. Similar to radar, the strength and\ntiming of reflected lasers can be used to generate a 360-degree 3D point cloud view of the surrounding\nenvironment, with each point corresponding to an estimated point of reflection.\n\nFor demonstration purposes we selected the Velodyne VLP-16 lidar sensor as our input source.\nWe adapted existing packet processing code from NVIDIA DeepMap SDK into a custom Holoscan operator,\n`VelodyneLidarOp`, and connected it with the existing `BasicNetworkOp` and `HoloVizOp` operators\nto provide a complete viewing pipeline. We performed initial benchmarking on an NVIDIA IGX devkit.\n\n## Requirements\n\nThis application is intended to run on a Holoscan SDK support platform, namely a Linux x64\nsystem or an NVIDIA IGX developer kit.\n\nTo run the application you need a live or replayer source to stream Velodyne VLP-16 packet\ndata to the application. That may be either:\n- A Velodyne VLP-16 lidar sensor. Review the [VLP-16 user manual](https://velodynelidar.com/wp-content/uploads/2019/12/63-9243-Rev-E-VLP-16-User-Manual.pdf) for setup instructions.\n- A VLP-16 `.pcap` recording file and a packet replayer software.\n  - Visit Kitware's VeloView [Velodyne Lidar collection](https://www.paraview.org/veloview/#download) for sample VLP-16 `.pcap` files.\n  - Visit the third party [Wireshark wiki](https://gitlab.com/wireshark/wireshark/-/wikis/Tools#traffic-generators) for a curated list of software options for generating traffic from `.pcap` files.\n\n## Running the Application\n\nFirst, start your lidar stream source. If you are using a VLP-16 lidar sensor, review the [VLP-16\nuser manual](https://velodynelidar.com/wp-content/uploads/2019/12/63-9243-Rev-E-VLP-16-User-Manual.pdf) for instructions on how to properly set up your network configuration.\n\nThen, build and start the Holoscan lidar viewing application:\n\n```sh\n./dev_container build_and_run velodyne_lidar_app\n```\n\n## Benchmarks\n\nWe performed benchmarking on an NVIDIA IGX developer kit with an A4000 GPU. (Note that an A6000 GPU is standard for IGX.) We used the [holoscan_flow_benchmarking](../../../benchmarks/holoscan_flow_benchmarking/) project to collect and summarize performance. The performance for each component in the Holoscan SDK pipeline is shown in the image below.\n\nKey statistics:\n\n| | |\n| - | - |\n| Minimum Latency | 1.03 milliseconds |\n| Average Latency | 1.12 milliseconds |\n| Maximum Latency | 1.34 milliseconds |\n\nBy comparison, the VLP-16 lidar publishes packets at a rate of approximately 1.33 milliseconds per packet.\n\n![Latency Flow Diagram](./doc/latency.png)\n\n![Cumulative Density Function](./doc/cdf.png)\n\n## Frequently Asked Questions (FAQ)\n\n### How does the application work?\n\nThe application flow is as follows:\n\n1. A UDP packet is emitted from the Velodyne VLP-16 lidar sensor and received on port 2368 in the Holoscan `BasicNetworkOp` operator.\n2. The packet payload is forwarded to the Holoscan `VelodyneLidarOp` operator. The operator decodes the packet\naccording to the Velodyne lidar specification, where the VLP-16 packet defines 384 spherical points from laser firings. The operator converts the spherical points to Cartesian points on the GPU device and adds the resulting cloud to a rolling, accumulated point cloud.\n3. The Velodyne operator forwards the rolling point cloud to HoloViz, which renders the GPU point cloud to the screen.\n\n### What are some limitations of the application?\n\nThis application is intended as a simple demonstration of how a lidar sensor can be integrated for input to Holoscan SDK for low latency processing. It does not propose any novel features. Some limitations compared with more complete lidar solutions are:\n- No cloud filtering -- all zero-ranged points are kept in the buffer and visualized.\n- No advanced inference techniques -- the cloud is simply translated and visualized.\n- No RDMA -- VLP-16 lidar packets are received via the host ethernet interface on the IGX or x86_64 machine and then copied to the GPU device.\n- Monochrome visual -- HoloViz operator cloud support is currently limited to one color.\n\nEach of these limitations is merely a result of our scope of work, and could be overcome with additional attention.\n\n### Why is HoloViz not responding?\n\nIn most cases this indicates that the Holoscan application is not receiving UDP packets.\nThere are several reasons that this could be the case:\n- The VLP-16 lidar sensor is not turned on, or the ethernet cable is disconnected.\n  The sensor typically takes approximately 30 seconds between powering on and transmitting packets.\n- The VLP-16 lidar sensor network interface is not properly configured to receive packets. You can use a tool\n  such as [Wireshark](https://www.wireshark.org/) to review live packets on the network interface. Review the [VLP-16\n  user manual](https://velodynelidar.com/wp-content/uploads/2019/12/63-9243-Rev-E-VLP-16-User-Manual.pdf) for troubleshooting.\n- The HoloHub application is not properly configured. Review the [`lidar.yaml`](lidar.yaml) configuration\n  and confirm that the port and IP address match the VLP-16 configuration.\n\n## Acknowledgements\n\nThis operator was developed in part with support from the NVIDIA nvMap team and adapts portions\nof the NVIDIA DeepMap SDK.\n\n",
        "application_name": "velodyne_lidar_app",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run velodyne_lidar_app --language cpp"
    },
    {
        "metadata": {
            "name": "Software Defined Radio FM Demodulation",
            "authors": [
                {
                    "name": "Adam Thompson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.4.0",
                "tested_versions": [
                    "0.4.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Communications",
                "Aerospace",
                "Defence",
                "Lifesciences"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.23.2"
                    },
                    {
                        "name": "cupy",
                        "version": "11.4"
                    },
                    {
                        "name": "cusignal",
                        "version": "22.12"
                    },
                    {
                        "name": "SoapySDR",
                        "version": "0.8.1"
                    },
                    {
                        "name": "soapysdr-module-rtlsdr",
                        "version": "0.3"
                    },
                    {
                        "name": "pyaudio",
                        "version": "0.2.13"
                    }
                ]
            },
            "run": {
                "command": "python3 sdr_fm_demodulation.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# SDR FM Demodulation Application\n\nAs the \"Hello World\" application of software defined radio developers, this demonstration highlights real-time FM demodulation, resampling, and playback on GPU with NVIDIA's Holoscan SDK. In this example, we are using an inexpensive USB-based [RTL-SDR](https://www.rtl-sdr.com/) dongle to feed complex valued Radio Frequency (RF) samples into GPU memory and use [cuSignal](https://github.com/rapidsai/cusignal) functions to perform the relevant signal processing. The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies (and, of course, plug in a SDR into your computer). This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal soapysdr soapysdr-module-rtlsdr pyaudio\npip install holoscan\n```\n\nThe FM demodulation example can then be run via\n```\npython applications/sdr_fm_demodulation/sdr_fm_demodulation.py\n```\n",
        "application_name": "sdr_fm_demodulation",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run sdr_fm_demodulation --language python"
    },
    {
        "metadata": {
            "name": "OpenIGTLink 3D Slicer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Streaming",
                "Ethernet",
                "3DSlicer",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "openigtlink",
                        "version": "3.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/openigtlink_3dslicer",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Holoscan SDK as an Inference Backend for 3D Slicer\n\nThis application demonstrates how to interface Holoscan SDK with [3D Slicer](https://www.slicer.org/), using the [OpenIGTLink protocol](http://openigtlink.org/). The application is shown in the application graph below.\n\n![](./images/openigtlink_3dslicer_graph.png)\n\nIn summary, the `openigtlink` transmit and receive operators are used in conjunction with an AI segmentation pipeline to:\n\n1. Send Holoscan sample video data from a node running Holoscan SDK, using `OpenIGTLinkTxOp`, to 3D Slicer running on a different node (simulating a video source connected to 3D Slicer):\n    * For `cpp` application, the ultrasound sample data is sent.\n    * For `python` application, the colonoscopy sample data is sent.\n2. Transmit the video data back to Holoscan SDK using OpenIGTLinkIF Module, and receive the data with the `OpenIGTLinkRxOp` operator.\n3. Perform an AI segmentation pipeline in Holoscan:\n    * For `cpp` application, the ultrasound segmentation model is deployed.\n    * For `python` application, the colonoscopy segmentation model is deployed.\n4. Use Holoviz in `headless` mode to render image and segmentation and then send the data back to 3D Slicer using the `OpenIGTLinkTxOp` operator.\n\nThis workflow allows for sending image data from 3D Slicer over network to Holoscan SDK (running on either `x86` or `arm`), do some compute task (e.g., AI inference), and send the results back to 3D Slicer for visualization. Nodes can run distributed; for example, Holoscan SDK can run on an IGX Orin (Node A) sending the video data, 3D Slicer on a Windows laptop (Node B) and the AI inference pipeline on yet another machine (Node C). Also, note that the `openigtlink` operators can connect to any software/library that supports the OpenIGTLink protocol; here, 3D Slicer is used as it is a popular open source software package for image analysis and scientific visualization.\n\nFor the `cpp` application, which does ultrasound segmentations the results look like\n\n![](./images/cpp_ultrasound.png)\n\nand for the `python` application, which does colonoscopy segmentation, the results look like\n\n![](./images/python_colonoscopy.png)\n\nwhere the image data before Holoscan processing is shown in the left slice view, and the image data with segmentation overlay (after Holoscan processing) is shown in the right slice view.\n\n## Run Instructions\n\n### Machine running 3D Slicer\n\nOn the machine running 3D Slicer do:\n1. In 3D Slicer, open the Extensions Manager and install the `SlicerOpenIGTLink` extension.\n2. Next, load the scene `openigtlink_3dslicer/scene/openigtlink_3dslicer.mrb` into 3D Slicer.\n3. Go to the `OpenIGTLinkIF` module and make sure that the `SendToHoloscan` connector has the IP address of the machine running Holoscan SDK in the *Hostname* input box (under *Properties*).\n4. Then activate the two connectors `ReceiveFromHoloscan` and `SendToHoloscan` (click *Active* check box under *Properties*).\n\n### Machine running Holoscan SDK\n\nOn the machine running Holoscan SDK do the below steps.\n\nFirst, ensure that the `host_name` parameters of the two `OpenIGTLinkRxOp` operators (`openigtlink_tx_slicer_img` and `openigtlink_tx_slicer_holoscan`) have the IP address of the machine running 3D Slicer.\n\nNext, the application requires [OpenIGTLink](http://openigtlink.org/). For simplicity a DockerFile is available. To generate the container run:\n```sh\n./dev_container build --docker_file ./applications/openigtlink_3dslicer/Dockerfile --img holohub:openigtlink\n```\n\nThe application can then be built by launching this container and using the provided `run` script:\n```sh\n./dev_container launch --img holohub:openigtlink\n./run build openigtlink_3dslicer\n```\n\nThen, to run the `python` application do:\n```sh\n./run launch openigtlink_3dslicer python\n```\nand to run the `cpp` application do:\n```sh\n./run launch openigtlink_3dslicer cpp\n```",
        "application_name": "openigtlink_3dslicer",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run openigtlink_3dslicer --language cpp"
    },
    {
        "metadata": {
            "name": "OpenIGTLink 3D Slicer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Streaming",
                "Ethernet",
                "3DSlicer",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "openigtlink",
                        "version": "3.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/openigtlink_3dslicer.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Holoscan SDK as an Inference Backend for 3D Slicer\n\nThis application demonstrates how to interface Holoscan SDK with [3D Slicer](https://www.slicer.org/), using the [OpenIGTLink protocol](http://openigtlink.org/). The application is shown in the application graph below.\n\n![](./images/openigtlink_3dslicer_graph.png)\n\nIn summary, the `openigtlink` transmit and receive operators are used in conjunction with an AI segmentation pipeline to:\n\n1. Send Holoscan sample video data from a node running Holoscan SDK, using `OpenIGTLinkTxOp`, to 3D Slicer running on a different node (simulating a video source connected to 3D Slicer):\n    * For `cpp` application, the ultrasound sample data is sent.\n    * For `python` application, the colonoscopy sample data is sent.\n2. Transmit the video data back to Holoscan SDK using OpenIGTLinkIF Module, and receive the data with the `OpenIGTLinkRxOp` operator.\n3. Perform an AI segmentation pipeline in Holoscan:\n    * For `cpp` application, the ultrasound segmentation model is deployed.\n    * For `python` application, the colonoscopy segmentation model is deployed.\n4. Use Holoviz in `headless` mode to render image and segmentation and then send the data back to 3D Slicer using the `OpenIGTLinkTxOp` operator.\n\nThis workflow allows for sending image data from 3D Slicer over network to Holoscan SDK (running on either `x86` or `arm`), do some compute task (e.g., AI inference), and send the results back to 3D Slicer for visualization. Nodes can run distributed; for example, Holoscan SDK can run on an IGX Orin (Node A) sending the video data, 3D Slicer on a Windows laptop (Node B) and the AI inference pipeline on yet another machine (Node C). Also, note that the `openigtlink` operators can connect to any software/library that supports the OpenIGTLink protocol; here, 3D Slicer is used as it is a popular open source software package for image analysis and scientific visualization.\n\nFor the `cpp` application, which does ultrasound segmentations the results look like\n\n![](./images/cpp_ultrasound.png)\n\nand for the `python` application, which does colonoscopy segmentation, the results look like\n\n![](./images/python_colonoscopy.png)\n\nwhere the image data before Holoscan processing is shown in the left slice view, and the image data with segmentation overlay (after Holoscan processing) is shown in the right slice view.\n\n## Run Instructions\n\n### Machine running 3D Slicer\n\nOn the machine running 3D Slicer do:\n1. In 3D Slicer, open the Extensions Manager and install the `SlicerOpenIGTLink` extension.\n2. Next, load the scene `openigtlink_3dslicer/scene/openigtlink_3dslicer.mrb` into 3D Slicer.\n3. Go to the `OpenIGTLinkIF` module and make sure that the `SendToHoloscan` connector has the IP address of the machine running Holoscan SDK in the *Hostname* input box (under *Properties*).\n4. Then activate the two connectors `ReceiveFromHoloscan` and `SendToHoloscan` (click *Active* check box under *Properties*).\n\n### Machine running Holoscan SDK\n\nOn the machine running Holoscan SDK do the below steps.\n\nFirst, ensure that the `host_name` parameters of the two `OpenIGTLinkRxOp` operators (`openigtlink_tx_slicer_img` and `openigtlink_tx_slicer_holoscan`) have the IP address of the machine running 3D Slicer.\n\nNext, the application requires [OpenIGTLink](http://openigtlink.org/). For simplicity a DockerFile is available. To generate the container run:\n```sh\n./dev_container build --docker_file ./applications/openigtlink_3dslicer/Dockerfile --img holohub:openigtlink\n```\n\nThe application can then be built by launching this container and using the provided `run` script:\n```sh\n./dev_container launch --img holohub:openigtlink\n./run build openigtlink_3dslicer\n```\n\nThen, to run the `python` application do:\n```sh\n./run launch openigtlink_3dslicer python\n```\nand to run the `cpp` application do:\n```sh\n./run launch openigtlink_3dslicer cpp\n```",
        "application_name": "openigtlink_3dslicer",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run openigtlink_3dslicer --language python"
    },
    {
        "metadata": {
            "name": "Speech-to-text + Large Language Model",
            "authors": [
                {
                    "name": "Sean Huver",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Speech-to-text",
                "Large Language Model"
            ],
            "ranking": 2,
            "dependencies": {
                "openai-whisper": "^20230314",
                "openai": "^0.27.2"
            },
            "run": {
                "command": "python3 stt_to_nlp.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# HoloHub Applications\n\nThis directory contains applications based on the Holoscan Platform.\nSome applications might require specific hardware and software packages which are described in the \nmetadata.json and/or README.md for each application.\n\n# Contributing to HoloHub Applications\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute applications.\n\n# HoloHub Application Organization Conventions\n\n## Required Conventions\n\nWe expect that an application contributed to HoloHub conforms to the following organization:\n\n- Each project must provide a `metadata.json` file reflecting several key components such as the application name and description, authors, dependencies, and the primary project language. \n- Each project must provide a `README` or `README.md` file.\n  - We strongly recommend that the project `README` file provides at least the information given in the [template README](./template/README.md.template), including the project description and a splash image.\n- Each project must be organized in its own subfolder under `holohub/applications/`.\n\nSee the [HoloHub application template](./template/) for example `README` and `metadata.json` documents to get started.\n\n## Recommended Conventions\n\nContributors may additionally opt to lay out their project structure in a way that conforms to HoloHub conventions in order to enable common infrastructure for their project, including streamlined build and run support in the [`dev_container`](../dev_container) and [`run`](../run) scripts and search support on the HoloHub landing page.\n\nIf your code does not adhere to these conventions, please set the field `manual_setup` to `true` in your project `metadata.json` file to opt out and indicate that your project is not eligible for streamlined infrastructure support.\n\nHoloHub recommended application convention is as follows:\n- Languages\n  - Project is either C++ or Python language\n  - If multiple language implementations are provided, each must be added to its own language subfolder as follows:\n```\napplications/\n  \u2514\u2500\u2500 my_project/\n        \u251c\u2500\u2500 cpp/\n        \u2502     \u251c\u2500\u2500 ...\n        \u2502     \u2514\u2500\u2500 ...\n        \u2514\u2500\u2500 python/\n              \u251c\u2500\u2500 ...\n              \u2514\u2500\u2500 ...\n```\n\n- Container Environment\n  - Project may provide its own container environment or opt to use the default HoloHub environment\n  - If the project specifies its own container:\n    - Default project environment must be named `Dockerfile`\n    - Project `Dockerfile` must be located at either:\n      - The same directory as `metadata.json`, or\n      - If the project defines a language directory (`cpp`, `python`), may provide at the project folder one level above, or\n      - The project may provide an alternative default Dockerfile path in `metadata.json`\n  - If the project does not specify a `Dockerfile` then the [default HoloHub `Dockerfile`](../Dockerfile) will be used\n\n- Build and Run Instructions\n  - Must provide a run command in `metadata.json` for `./run launch` to reference\n  - Must otherwise comply with `./dev_container build_and_run` command options for the default use case\n  - Advanced instructions may also be specified in the project README\n",
        "application_name": "speech_to_text_llm",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run speech_to_text_llm --language python"
    },
    {
        "metadata": {
            "name": "WebRTC Video Client",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Client",
                "Video"
            ],
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    },
                    {
                        "name": "aiohttp",
                        "version": "3.8.5"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/webrtc_client.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# WebRTC Video Client\n\n![](screenshot.png)<br>\n\nThis app receives video frames from a web cam connected to a browser and display them on the screen.\n\nThe app starts a web server, the pipeline starts when a browser is connected to the web server and the `Start` button is pressed. The pipeline stops when the `Stop` button is pressed.\n\nThe video resolution and video codec can be selected in browser.\n\n```mermaid\nflowchart LR\n    subgraph Server\n        WebRTCClientOp --> HolovizOp\n        WebServer\n    end\n    subgraph Client\n        Webcam --> Browser\n        Browser <--> WebRTCClientOp\n        Browser <--> WebServer\n    end\n```\n\n> **_NOTE:_** When using VPN there might be a delay of several seconds between pressing the `Start` button and the first video frames are display. The reason for this is that the STUN server `stun.l.google.com:19302` used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.\n\n## Prerequisites\n\nThe app is using [AIOHTTP](https://docs.aiohttp.org/en/stable/) for the web server and [AIORTC](https://github.com/aiortc/aiortc) for WebRTC. Install both using pip.\n\n```bash\npip install aiohttp aiortc\n```\n\n## Run Instructions\n\nRun the command:\n\n```bash\n./run launch webrtc_video_client\n```\n\nOn the same machine open a browser and connect to `127.0.0.1:8080`.\n\nSelect the video resolution and codec or keep the defaults.\n\nPress the `Start` button. Video frames are displayed. To stop, press the `Stop` button. Pressing `Start` again will continue the video.\n\nYou can also connect from a different machine by connecting to the IP address the app is running on. Chrome disables features such as getUserMedia when it comes from an unsecured origin. `http://localhost` is considered as a secure origin by default, however if you use an origin that does not have an SSL/TLS certificate then Chrome will consider the origin as unsecured and disable getUserMedia.\n\nSolutions\n\n- Create an self-signed SSL/TLS certificate with `openssl req -new -newkey rsa:4096 -x509 -sha256 -days 365 -nodes -out MyCertificate.crt -keyout MyKey.key`. Pass the generated files to the `webrtc_client` using the `--cert-file` and `--key-file` arguments. Connect the browser to `https://{YOUR HOST IP}:8080`.\n- Go to chrome://flags, search for the flag `unsafely-treat-insecure-origin-as-secure`, enter the origin you want to treat as secure such as `http://{YOUR HOST IP}:8080`, enable the feature and relaunch the browser.\n\n### Command Line Arguments\n\n```\nusage: webrtc_client.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n```",
        "application_name": "webrtc_video_client",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run webrtc_video_client --language python"
    },
    {
        "metadata": {
            "name": "Orthorectification with OptiX",
            "authors": [
                {
                    "name": "Brent Bartlett",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Orthorectification",
                "Drone",
                "OptiX"
            ],
            "ranking": 4,
            "dependencies": {
                "OptiX-SDK": {
                    "version": "4.7.0"
                },
                "OptiX-Toolit": {
                    "version": "0.8.1"
                }
            }
        },
        "readme": "# HoloHub Orthorectification Application\n\nThis application is an example of utilizing the nvidia OptiX SDK via the PyOptix bindings to create per-frame orthorectified imagery. In this example, one can create a visualization of mapping frames from a drone mapping mission processed with [Open Drone Map](https://opendronemap.org/). A typical output of a mapping mission is a single merged mosaic. While this product is useful for GIS applications, it is difficult to apply algorithms on a such a large single image without incurring additional steps like image chipping. Additionally, the mosaic process introduces image artifacts which can negativley impact algorithm performance. \n\nSince this holoscan pipeline processes each frame individually, it opens the door for one to apply an algorithm to the original un-modififed imagery and then map the result. If custom image processing is desired, it is recommended to insert custom operators before the Ray Trace Ortho operator in the application flow. \n\n\n![](docs/odm_ortho_pipeline.png)<br>\nFig. 1 Orthorectification sample application workflow\n\nSteps for running the application:\n\na) Download and Prep the ODM Dataset<br>\n1. Download the [Lafayette Square Dataset](https://www.opendronemap.org/odm/datasets/) and place into ~/Data.\n\n2. Process the dataset with ODM via docker command: <br>\n```docker run -ti --rm -v ~/Data/lafayette_square:/datasets/code opendronemap/odm --project-path /datasets --camera-lens perspective --dsm```\n\nIf you run out of memory add the following argument to preserve some memory: ```--feature-quality medium```\n\nb) Clone holohub and navigate to this application directory\n\nc) Download [OptiX SDK 7.4.0](https://developer.nvidia.com/optix/downloads/7.4.0/linux64-x86_64) and extract the package in the same directory as the source code\n(i.e. applications/orthorectification_with_optix).\n\nd) Build development container <br>\n1. ```DOCKER_BUILDKIT=1 docker build -t holohub-ortho-optix:latest .```\n\nYou can now run the docker container by: <br>\n1. ```xhost +local:docker```\n2. ```nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f 2>/dev/null | grep .) || (echo \"nvidia_icd.json not found\" >&2 && false)```\n3. ```docker run -it --rm --net host --runtime=nvidia -v ~/Data:/root/Data  -v .:/work/ -v /tmp/.X11-unix:/tmp/.X11-unix  -v $nvidia_icd_json:$nvidia_icd_json:ro  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -e DISPLAY=$DISPLAY  holohub-ortho-optix```\n\nFinish prepping the input data: <br>\n1. ```gdal_translate -tr 0.25 0.25 -r cubic ~/Data/lafayette_square/odm_dem/dsm.tif ~/Data/lafayette_square/odm_dem/dsm_small.tif```\n2. ```gdal_fillnodata.py -md 0 ~/Data/lafayette_square/odm_dem/dsm_small.tif ~/Data/lafayette_square/odm_dem/dsm_small_filled.tif```\n\nFinally run the application: <br>\n1. ```python ./python/ortho_with_pyoptix.py```\n\nYou can modify the applications settings in the file \"ortho_with_pyoptix.py\" \n\n```\nsensor_resize = 0.25 # resizes the raw sensor pixels\nncpu = 8 # how many cores to use to load sensor simulation\ngsd = 0.25 # controls how many pixels are in the rendering\niterations = 425 # how many frames to render from the source images (in this case 425 is max)\nuse_mosaic_bbox = True # render to a static bounds on the ground as defined by the DEM\nwrite_geotiff = False \nnb=3 # how many bands to write to the GeoTiff\nrender_scale = 0.5 # scale the holoview window up or down\nfps = 8.0 # rate limit the simulated sensor feed to this many frames per second\n```\n\n![](docs/holohub_ortho_app.gif)<br>\nFig. 2 Running the orthorectification sample application",
        "application_name": "orthorectification_with_optix",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run orthorectification_with_optix --language python"
    },
    {
        "metadata": {
            "name": "TAO PeopleNet Detection Model on Video Stream",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0",
                    "1.0.3",
                    "2.0.0",
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Computer Vision",
                "Detection"
            ],
            "ranking": 2,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet"
            },
            "run": {
                "command": "python3 <holohub_app_source>/tao_peoplenet.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# TAO PeopleNet Detection Model on V4L2 Video Stream\n\nUse the TAO PeopleNet available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet) to detect faces and people in a V4L2 supported video stream. HoloViz is used to draw bounding boxes around the detections.\n\n## Model\n\nThis application uses the TAO PeopleNet model from [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet) for face and person classification.\nThe model is downloaded when building the application.\n\n## Data\n\nThis application downloads a pre-recorded video from [Pexels](https://www.pexels.com/video/a-woman-showing-her-ballet-skill-in-turning-one-footed-5385885/) when the application is built for use with this application.  Please review the [license terms](https://www.pexels.com/license/) from Pexels.\n\n> **_NOTE:_** The user is responsible for checking if the dataset license is fit for the intended purpose.\n\n## Input\n\nThis app currently supports three different input options:\n\n1. v4l2 compatible input device (default, see V4L2 Support below)\n2. pre-recorded video (see Video Replayer Support below)\n\n## Run Instructions\n\n## V4L2 Support\n\nThis application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device,\nplease plug in your input device and run:\n```sh\n./dev_container build_and_run tao_peoplenet\n```\n\nBy default, this application expects the input device to be mounted at `/dev/video0`.  If this is not the case, please update\n`applications/tao_peoplenet/tao_peoplenet.yaml` and set it to use the corresponding input device before\nrunning the application.  You can also override the default input device on the command line by running:\n```sh\n./dev_container build_and_run tao_peoplenet --run_args \"--video_device /dev/video0\"\n```\n\n## Video Replayer Support\n\nIf you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video.\nTo launch the application using the Video Stream Replayer as the input source, run:\n\n```sh\n./dev_container build_and_run tao_peopelnet --run_args \"--source replayer\"\n```\n",
        "application_name": "tao_peoplenet",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run tao_peoplenet --language python"
    },
    {
        "metadata": {
            "name": "Volume Rendering",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release",
                "2.0": "Update to Holoscan SDK 2.1"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3",
                    "2.0.0",
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Render",
                "ClaraViz"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/volume_rendering --config <holohub_data_dir>/volume_rendering/config.json --density <holohub_data_dir>/volume_rendering/highResCT.mhd --mask <holohub_data_dir>/volume_rendering/smoothmasks.seg.mhd",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Volume rendering using ClaraViz\n\n![](screenshot.png)<br>\n\nThis application loads a medical CT scan and renders it in real time at interactive frame rates using ClaraViz (https://github.com/NVIDIA/clara-viz).\n\nThe application uses the `VolumeLoaderOp` operator to load the medical volume data, the `VolumeRendererOp` operator to render the volume and the `HolovizOp` operator to display the result and handle the camera movement.\n\n### Data\n\nYou can find CT scan datasets for use with this application from [embodi3d](https://www.embodi3d.com/).\n\nDatasets are bundled with a default ClaraViz JSON configuration file for volume rendering. See [`VolumeRendererOp` documentation](/operators/volume_renderer/README.md#configuration) for details on configuration schema.\n\nSee [`VolumeLoaderOp` documentation](/operators/volume_loader/README.md#supported-formats) for supported volume formats.\n\n## Build and Run Instructions\n\nTo build and run this application, use the ```dev_container``` script:\n\n```bash\n# C++\n ./dev_container build_and_run volume_rendering --language cpp\n\n # Python\n  ./dev_container build_and_run volume_rendering --language python\n```\n\nThe path of the volume configuration file, volume density file and volume mask file can be passed to the application.\n\nYou can use the following command to get more information on command line parameters for this application:\n\n```bash\n./dev_container build_and_run volume_rendering --language [cpp|python] --run_args --usages\n```\n",
        "application_name": "volume_rendering",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run volume_rendering --language cpp"
    },
    {
        "metadata": {
            "name": "Volume Rendering",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3",
                    "2.0.0",
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Render",
                "ClaraViz"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 volume_rendering.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Volume rendering using ClaraViz\n\n![](screenshot.png)<br>\n\nThis application loads a medical CT scan and renders it in real time at interactive frame rates using ClaraViz (https://github.com/NVIDIA/clara-viz).\n\nThe application uses the `VolumeLoaderOp` operator to load the medical volume data, the `VolumeRendererOp` operator to render the volume and the `HolovizOp` operator to display the result and handle the camera movement.\n\n### Data\n\nYou can find CT scan datasets for use with this application from [embodi3d](https://www.embodi3d.com/).\n\nDatasets are bundled with a default ClaraViz JSON configuration file for volume rendering. See [`VolumeRendererOp` documentation](/operators/volume_renderer/README.md#configuration) for details on configuration schema.\n\nSee [`VolumeLoaderOp` documentation](/operators/volume_loader/README.md#supported-formats) for supported volume formats.\n\n## Build and Run Instructions\n\nTo build and run this application, use the ```dev_container``` script:\n\n```bash\n# C++\n ./dev_container build_and_run volume_rendering --language cpp\n\n # Python\n  ./dev_container build_and_run volume_rendering --language python\n```\n\nThe path of the volume configuration file, volume density file and volume mask file can be passed to the application.\n\nYou can use the following command to get more information on command line parameters for this application:\n\n```bash\n./dev_container build_and_run volume_rendering --language [cpp|python] --run_args --usages\n```\n",
        "application_name": "volume_rendering",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run volume_rendering --language python"
    },
    {
        "metadata": {
            "name": "SSD Detection Application",
            "authors": [
                {
                    "name": "Jin Li",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.1",
            "changelog": {
                "1.0": "Initial Release",
                "1.1": "Update from TensorRTInferenceOp to InferenceOp"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "SSD",
                "bounding box",
                "Detection"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.22.3"
                    },
                    {
                        "name": "cupy",
                        "version": "11.6.0"
                    }
                ]
            }
        },
        "readme": "# SSD Detection Application\n## Model\nWe can train the [SSD model from NVIDIA DeepLearningExamples repo]((https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)) with any data of our choosing. Here for the purpose of demonstrating the deployment process, we will use a SSD model checkpoint that is only trained for the [demo video clip](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data). \n\nPlease download the models [at this NGC Resource](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/ssd_surgical_tool_detection_model) for `epoch_24.pt`, `epoch24_nms.onnx` and `epoch24.onnx`. You can go through the next steps of Model Conversion to ONNX to convert `epoch_24.pt` into `epoch24_nms.onnx` and `epoch24.onnx`, or use the downloaded ONNX models directly.\n\n\n### Model Conversion to ONNX\nThe scripts we need to export the model from .pt checkpoint to the ONNX format are all within this dir `./scripts`. It is a two step process.\n\n\n Step 1: Export the trained checkpoint to ONNX. <br> We use [`export_to_onnx_ssd.py`](./scripts/export_to_onnx_ssd.py) if we want to use the model as is without NMS, or [`export_to_onnx_ssd_nms.py`](./scripts/export_to_onnx_ssd_nms.py) to prepare the model with NMS. \n Let's assume the re-trained SSD model checkpoint from the [repo](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD) is saved as `epoch_24.pt`.\n The export process is \n```\n# For exporting the original ONNX model\n python export_to_onnx_ssd.py --model epoch_24.pt  --outpath epoch24_temp.onnx\n```\n```\n# For preparing to add the NMS step to ONNX model\npython export_to_onnx_ssd_nms.py --model epoch_24.pt  --outpath epoch24_nms_temp.onnx\n```\nStep 2: modify input shape. <br> Step 1 produces a onnx model with input shape `[1, 3, 300, 300]`, but we will want to modify the input node to have shape `[1, 300, 300, 3]` or in general `[batch_size, height, width, channels]` for compatibility and easy of deployment in the Holoscan SDK. If we want to incorporate the NMS operation in the the ONNX model, we could add a `EfficientNMS_TRT` op, which is documented in [`graph_surgeon_ssd.py`](./scripts/graph_surgeon_ssd.py)'s nms related block.\n```\n# For exporting the original ONNX model\npython graph_surgeon_ssd.py --orig_model epoch24_temp.onnx --new_model epoch24.onnx\n```\n```\n# For adding the NMS step to ONNX model, use --nms\npython graph_surgeon_ssd.py --orig_model epoch24_nms_temp.onnx --new_model epoch24_nms.onnx --nms\n```\n\nNote that\n - `epoch24.onnx` is used in `ssd_step1.py` and `ssd_step2_route1.py`\n - `epoch24_nms.onnx` is used in `ssd_step2_route2.py` and `ssd_step2_route2_render_labels.py` \n\n## Data\nFor this application we will use the same [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) as the Holoscan SDK reference applications.\n\n## Requirements\nThere are two requirements \n1. To run `ssd_step1.py` and `ssd_step2_route1.py` with the original exported model, we need the installation of PyTorch and CuPy. \n<br> To run `ssd_step2_route2.py` and `ssd_step2_route2_render_labels.py` with the exported model with additional NMS layer in ONNX, we need the installation of CuPy. \n<br> If you're using the dGPU on the devkit, since there are no prebuilt PyTorch wheels for aarch64 dGPU, the simplest way is to modify the Dockerfile and build from source; if you're on x86 or using the iGPU on the devkit, there should be existing prebuilt PyTorch wheels.\n<br> If you choose to build the SDK from source, you can find the [modified Dockerfile here](./docker/Dockerfile) to replace the SDK repo [Dockerfile](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/Dockerfile) to satisfy the installation requirements. \n<br> The main changes in Dockerfile for dGPU: the base image changed to `nvcr.io/nvidia/pytorch:22.03-py3` instead of the `nvcr.io/nvidia/tensorrt:22.03-py3` as dGPU's base image; adding the installation of NVTX for optional profiling.\n<br>Build the SDK container following the [README instructions](https://github.com/nvidia-holoscan/holoscan-sdk#recommended-using-the-run-script). \n<br>\n Make sure the directory containing this application and the directory containing the NGC data and models are mounted in the container. Add the `-v` mount options to the `docker run` command launched by `./run launch` in the SDK repo. \n\n2. Make sure the model and data are accessible by the application. \n<br> Make sure the yaml files `ssd_endo_model.yaml` and `ssd_endo_model_with_NMS.yaml` are pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the `epoch24_nms.onnx` and `epoch24.onnx` are located at:\n```\nmodel_file_path: /byom/models/endo_ssd/epoch24_nms.onnx \nengine_cache_dir: /byom/models/endo_ssd/epoch24_nms_engines\n```\nand / or\n```\nmodel_file_path: /byom/models/endo_ssd/epoch24.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_engines\n```\nThe [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) is assumed to be at \n```\n/workspace/holoscan-sdk/data/endoscopy\n```\nPlease check and modify the paths to model and data in the yaml file if needed.\n\n## Building the application\nPlease refer to the README under [./app_dev_process](./app_dev_process/README.md) to see the process of building the applications.\n\n## Running the application\nRun the incrementally improved Python applications by:\n```\npython ssd_step1.py\n\npython ssd_step2_route1.py\n\npython ssd_step2_route2.py\n\npython ssd_step2_route2_render_labels.py --labelfile endo_ref_data_labels.csv\n```",
        "application_name": "ssd_detection_endoscopy_tools",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run ssd_detection_endoscopy_tools --language python"
    },
    {
        "metadata": {
            "name": "High Speed Endoscopy",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/high_speed_endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# High-Speed Endoscopy\n\nThe application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.\n\n### Requirements\n\nThis application requires:\n1. an Emergent Vision Technologies camera (see [setup instructions]((https://docs.nvidia.com/holoscan/sdk-user-guide/emergent_setup.html)\n2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see [prerequisites](../../README.md#prerequisites))\n3. a display with high refresh rate to keep up with the camera's framerate\n4. [additional setups](https://docs.nvidia.com/holoscan/sdk-user-guide/additional_setup.html) to reduce latency\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n> \u26a0\ufe0f At this time, camera controls are hardcoded within the `gxf_emergent_source` extension. To update them at the application level, the GXF extension, and the application need to be rebuilt.\nFor more information on the controls, refer to the [EVT Camera Attributes Manual](https://emergentvisiontec.com/resources/?tab=umg)\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory. Then, run the commands of your choice:\n\n* RDMA disabled\n    ```bash\n    # C++\n    sed -i -e 's#rdma:.*#rdma: false#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n        && sudo ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n    ```\n\n* RDMA enabled\n    ```bash\n    # C++\n    sed -i -e 's#rdma:.*#rdma: true#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n        && sudo MELLANOX_RINGBUFF_FACTOR=14 ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n    ```\n\n> \u2139\ufe0f The `MELLANOX_RINGBUFF_FACTOR` is used by the EVT driver to decide how much BAR1 size memory would be used on the dGPU. It can be changed to different number based on different use cases.\n",
        "application_name": "high_speed_endoscopy",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run high_speed_endoscopy --language cpp"
    },
    {
        "metadata": {
            "name": "High Speed Endoscopy",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/high_speed_endoscopy.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# High-Speed Endoscopy\n\nThe application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.\n\n### Requirements\n\nThis application requires:\n1. an Emergent Vision Technologies camera (see [setup instructions]((https://docs.nvidia.com/holoscan/sdk-user-guide/emergent_setup.html)\n2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see [prerequisites](../../README.md#prerequisites))\n3. a display with high refresh rate to keep up with the camera's framerate\n4. [additional setups](https://docs.nvidia.com/holoscan/sdk-user-guide/additional_setup.html) to reduce latency\n\n### Run Instructions\n\nTODO\n",
        "application_name": "high_speed_endoscopy",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run high_speed_endoscopy --language python"
    },
    {
        "metadata": {
            "name": "Hyperspectral image segmentation",
            "authors": [
                {
                    "name": "Lars Doorenbos",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Segmentation",
                "Hyperspectral"
            ],
            "ranking": 2,
            "dependencies": {
                "blosc": "^1.11.1",
                "torch": "^2.1.0",
                "onnx": "^1.15.0",
                "onnxruntime": "^1.16.1",
                "Pillow": "^10.1.0",
                "data": [
                    {
                        "name": "Hyperspectral Tissue Classification Pre-Trained Model",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/hyperspectral_segmentation",
                        "homepage": "https://github.com/IMSY-DKFZ/htc"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/hyperspectral_segmentation.py --output_folder <holohub_app_source>/ --data <holohub_data_dir>/hyperspectral/data --model <holohub_data_dir>/hyperspectral",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Hyperspectral Image Segmentation\n\n![](screenshot.png)<br>\n\nThis application segments endoscopic hyperspectral cubes into 20 organ classes. It visualizes the result together with the RGB image corresponding to the cube.\n\n## Data and Models\n\nThe data is a subset of the [HeiPorSPECTRAL](https://www.heiporspectral.org/) dataset. The application loops over the 84 cubes selected. The model is the `2022-02-03_22-58-44_generated_default_model_comparison` checkpoint from [this repository](https://github.com/IMSY-DKFZ/htc), converted to ONNX with the script in `utils/convert_to_onnx.py`.\n\n[\ud83d\udce6\ufe0f (NGC) App Data and Model for Hyperspectral Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/hyperspectral_segmentation).  This resource is automatically downloaded when building the application.\n\n## Run Instructions\n\nThis application requires some python modules to be installed.  For simplicity, a Dockerfile is available.  To generate the container run:\n```\n./dev_container build --docker_file ./applications/hyperspectral_segmentation/Dockerfile\n```\nThe application can then be built by launching this container and using the provided run script.\n```\n./dev_container launch\n./run build hyperspectral_segmentation\n```\nOnce the application is built it can be launched with the run script.\n```\n./run launch hyperspectral_segmentation\n```\n\n## Viewing Results\n\nWith the default settings, the results of this application are saved to `result.png` file in the hyperspectral segmentation app directory. Each time a new image is processed, it overwrites `result.png`.  By opening this image while the application is running, you can see the results as the updates are made (may depend on your image viewer).\n",
        "application_name": "hyperspectral_segmentation",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run hyperspectral_segmentation --language python"
    },
    {
        "metadata": {
            "name": "HoloChat",
            "authors": [
                {
                    "name": "Nigel Nelson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "0.2.0",
            "changelog": {
                "0.1.0": "Beta release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "LLM",
                "Vector Database",
                "AI-Assistant"
            ],
            "ranking": 4,
            "dependencies": {
                "OSS": [
                    {
                        "name": "Llama.cpp",
                        "version": "cf9b08485c4c2d4d945c6e74fe20f273a38b6104"
                    },
                    {
                        "name": "LangChain",
                        "version": "0.1.11"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_source>/holochat.sh",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# HoloChat\n\n### Table of Contents\n- [Hardware Requirements](#hardware-requirements-\ud83d\udc49\ud83d\udcbb)\n- [Run Instructions](#running-holochat-\ud83c\udfc3\ud83d\udca8)\n- [Intended Use](#intended-use-\ud83c\udfaf)\n- [Known Limitations](#known-limitations-\u26a0\ufe0f\ud83d\udea7)\n- [Best Practices](#best-practices-\u2705\ud83d\udc4d)\n\n\nHoloChat is an AI-driven chatbot, built on top of a **locally hosted Code-Llama model** *OR* a remote **NIM API for Llama-3-70b**, which acts as developer's copilot in Holoscan development. The LLM leverages a vector database comprised of the Holoscan SDK repository and user guide, enabling HoloChat to answer general questions about Holoscan, as well act as a Holoscan SDK coding assistant.\n\n<p align=\"center\">\n  <kbd style=\"border: 2px solid black;\">\n    <img src=\"holochat_demo.gif\" alt=\"HoloChat Demo\" />\n  </kbd>\n</p>\n\n## Hardware Requirements: \ud83d\udc49\ud83d\udcbb\n- **Processor:** x86/Arm64\n\n**If running local LLM**:\n- **GPU**: NVIDIA dGPU w/ >= 28 GB VRAM\n- **Memory**: \\>= 28 GB of available disk memory\n  - Needed to download [fine-tuned Code Llama 34B](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/models/phind-codellama-34b-v2-q5_k_m) and [BGE-Large](https://huggingface.co/BAAI/bge-large-en) embedding model\n\n*Tested using [NVIDIA IGX Orin](https://www.nvidia.com/en-us/edge-computing/products/igx/) w/ RTX A6000 and [Dell Precision 5820 Workstation](https://www.dell.com/en-us/shop/desktop-computers/precision-5820-tower-workstation/spd/precision-5820-workstation/xctopt5820us) w/ RTX A6000\n\n## Running HoloChat: \ud83c\udfc3\ud83d\udca8\n**When running HoloChat, you have two LLM options:**\n- Local: Uses [Phind-CodeLlama-34B-v2]((https://huggingface.co/Phind/Phind-CodeLlama-34B-v2)) running on your local machine using Llama.cpp\n- Remote: Uses [Llama-3-70b-Instruct](https://build.nvidia.com/meta/llama3-70b) using the [NVIDIA NIM API](https://build.nvidia.com/explore/discover)\n\n### TLDR; \ud83e\udd71\nTo run locally:\n```bash\n./dev_container build_and_run holochat --run_args --local\n```\nTo run using the NVIDIA NIM API:\n```bash\necho \"NVIDIA_API_KEY=<api_key_here>\" > ./applications/holochat/.env\n\n./dev_container build_and_run holochat\n```\n\n### Build Notes: \u2699\ufe0f\n\n**Build Time:**\n- HoloChat uses a [PyTorch container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) from [NGC](https://catalog.ngc.nvidia.com/?filters=&orderBy=weightPopularDESC&query=) and may also download the [~23 GB Phind LLM](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/models/phind-codellama-34b-v2-q5_k_m) from NGC. As such, the first time building this application **will likely take ~45 minutes** depending on your internet speeds. However, this is a one-time set-up and subsequent runs of HoloChat should take seconds to launch.\n\n**Build Location:**\n\n- If running locally: HoloChat downloads ~28 GB of model data to the `holochat/models` directory. As such, it is **recommended** to only run this application on a disk drive with ample storage (ex: the 500 GB SSD included with NVIDIA IGX Orin).\n\n\n## Running Instructions:\n\nIf connecting to your machine via SSH, be sure to forward the ports 7860 & 8080:\n```bash\nssh <user_name>@<IP address> -L 7860:localhost:7860 -L 8080:localhost:8080\n```\n### Running w/ Local LLM \ud83d\udcbb\n**To build and start the app:**\n```bash\n./dev_container build_and_run holochat --run_args --local\n```\nOnce the LLM is loaded on the GPU and the Gradio app is running, HoloChat should be available at http://127.0.0.1:7860/.\n### Running w/ NIM API \u2601\ufe0f\nTo use the NIM API you must create a .env file at:\n```bash\n./applications/holochat/.env\n```\nThis is where you should place your [NVIDIA API](https://build.nvidia.com/explore/discover) key.\n```bash\nNVIDIA_API_KEY=<api_key_here>\n```\n\n**To build and run the app:**\n```bash\n./dev_container build_and_run holochat\n```\nOnce the Gradio app is running, HoloChat should be available at http://127.0.0.1:7860/.\n\n## Usage Notes: \ud83d\uddd2\ufe0f \n\n### Intended use: \ud83c\udfaf\n  >HoloChat is developed to accelerate and assist Holoscan developers\u2019 learning and development. HoloChat serves as an intuitive chat interface, enabling users to pose natural language queries related to the Holoscan SDK. Whether seeking general information about the SDK or specific coding insights, users can obtain immediate responses thanks to the underlying Large Language Model (LLM) and vector database.\n  > \n  >HoloChat is given access to the Holoscan SDK repository, the HoloHub repository, and the Holoscan SDK user guide. This essentially allows users to engage in natural language conversations with these documents, gaining instant access to the information they need, thus sparing them the task of sifting through vast amounts of documentation themselves.\n\n### Known Limitations: \u26a0\ufe0f\ud83d\udea7\nBefore diving into how to make the most of HoloChat, it's crucial to understand and acknowledge its known limitations. These limitations can guide you in adopting the best practices below, which will help you navigate and mitigate these issues effectively.\n* **Hallucinations:** Occasionally, HoloChat may provide responses that are not entirely accurate. It's advisable to approach answers with a healthy degree of skepticism.\n* **Memory Loss:** LLM's limited attention window may lead to the loss of previous conversation history. To mitigate this, consider restarting the application to clear the chat history when necessary.\n* **Limited Support for Stack Traces**: HoloChat's knowledge is based on the Holoscan repository and the user guide, which lack large collections of stack trace data. Consequently, HoloChat may face challenges when assisting with stack traces.\n\n### Best Practices: \u2705\ud83d\udc4d\nWhile users should be aware of the above limitations, following the recommended tips will drastically minimize these possible shortcomings. In general, the more detailed and precise a question is, the better the results will be. Some best practices when asking questions are:\n* **Be Verbose**: If you want to create an application, specify which operators should be used if possible (HolovizOp, V4L2VideoCaptureOp, InferenceOp, etc.).\n* **Be Specific**: The less open-ended a question is the less likely the model will hallucinate.\n* **Specify Programming Language**: If asking for code, include the desired language (Python or C++).\n* **Provide Code Snippets:** If debugging errors include as much relevant information as possible. Copy and paste the code snippet that produces the error, the abbreviated stack trace, and describe any changes that may have introduced the error.\n\nIn order to demonstrate how to get the most out of HoloChat two example questions are posed below. These examples illustrate how a user can refine their questions and as a result, improve the responses they receive: \n\n---\n**Worst\ud83d\udc4e:**\n\u201cCreate an app that predicts the labels associated with a video\u201d\n\n**Better\ud83d\udc4c:**\n\u201cCreate a Python app that takes video input and sends it through a model for inference.\u201d\n\n**Best\ud83d\ude4c:**\n\u201cCreate a Python Holoscan application that receives streaming video input, and passes that video input into a pytorch classification model for inference. Then, collect the model\u2019s predicted class and use Holoviz to display the class label on each video frame.\u201d\n\n---\n**Worst\ud83d\udc4e:**\n\u201cWhat os can I use?\u201d\n\n**Better\ud83d\udc4c:**\n\u201cWhat operating system can I use with Holoscan?\u201d\n\n**Best\ud83d\ude4c:**\n\u201cCan I use MacOS with the Holoscan SDK?\u201d\n\n\n## Appendix:\n### Meta Terms of Use:\nBy using the Code-Llama model, you are agreeing to the terms and conditions of the [license](https://ai.meta.com/llama/license/), [acceptable use policy](https://ai.meta.com/llama/use-policy/) and Meta\u2019s [privacy policy](https://www.facebook.com/privacy/policy/).\n### Implementation Details: \n  >HoloChat operates by taking user input and comparing it to the text stored within the vector database, which is comprised of Holoscan SDK information. The most relevant text segments from SDK code and the user guide are then appended to the user's query. This approach allows the chosen LLM to answer questions about the Holoscan SDK, without being explicitly trained on SDK data.\n  >\n  >However, there is a drawback to this method - the most relevant documentation is not always found within the vector database. Since the user's question serves as the search query, queries that are too simplistic or abbreviated may fail to extract the most relevant documents from the vector database. As a consequence, the LLM will then lack the necessary context, leading to poor and potentially inaccurate responses. This occurs because LLMs strive to provide the most probable response to a question, and without adequate context, they hallucinate to fill in these knowledge gaps.\n\n",
        "application_name": "holochat",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run holochat --language python"
    },
    {
        "metadata": {
            "name": "MultiAI Ultrasound",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "MultiAI"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/multiai_ultrasound --data <holohub_data_dir>/multiai_ultrasound",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Multi-AI Ultrasound\n\nThis application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.\n\nThe Inference and the Processing operators use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.\n\nThe applications uses models and echocardiogram data from iCardio.ai. The models include:\n- a Plax chamber model, that identifies four critical linear measurements of the heart\n- a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography\n- an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis\n\nThe default configuration (`multiai_ultrasound.yaml`) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (`mgpu_multiai_ultrasound.yaml`) is present in both `cpp` and `python` applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.\n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_multi_ai_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using a pre-recorded video on multi-GPU system\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound\n    ```\n",
        "application_name": "multiai_ultrasound",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run multiai_ultrasound --language cpp"
    },
    {
        "metadata": {
            "name": "MultiAI Ultrasound",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "MultiAI"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 <holohub_app_source>/multiai_ultrasound.py  --data <holohub_data_dir>/multiai_ultrasound",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Multi-AI Ultrasound\n\nThis application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.\n\nThe Multi AI operators (inference and postprocessor) use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.\n\nThe applications uses models and echocardiogram data from iCardio.ai. The models include:\n- a Plax chamber model, that identifies four critical linear measurements of the heart\n- a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography\n- an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis\n\nThe default configuration (`multiai_ultrasound.yaml`) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (`mgpu_multiai_ultrasound.yaml`) is present in both `cpp` and `python` applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_multi_ai_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --source=replayer --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using a pre-recorded video on multi-GPU system\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --config mgpu_multiai_ultrasound.yaml --source=replayer --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --source=aja\n    ```\n",
        "application_name": "multiai_ultrasound",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run multiai_ultrasound --language python"
    },
    {
        "metadata": {
            "name": "Endoscopy Depth Estimation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Depth",
                "AJA"
            ],
            "ranking": 2,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_depth_estimation_sample_data",
                "opencv": "^4.8.0"
            },
            "run": {
                "command": "python3 <holohub_app_source>/endoscopy_depth_estimation.py --data <holohub_data_dir>/endoscopy --model <holohub_data_dir>/endoscopy_depth",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopy Depth Estimation\n\nThis application demonstrates the use of custom components for depth estimation and its rendering using Holoviz with triangle interpolation.\n\n### Requirements\n\n- Python 3.8+\n- OpenCV 4.8+\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Endoscopy](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Model\n\n[\ud83d\udce6\ufe0f (NGC) App Model for AI-based Endoscopy Depth Estimation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_depth_estimation_sample_data)\n\nThe model is automatically downloaded to the same folder as the data in ONNX format.\n\n### OpenCV-GPU\n\nThis application uses OpenCV with GPU acceleration during the preprocessing stage when it runs with Histogram Equalization (flag `--clahe` or `-c`).\nHistogram equalization reduces the effect of specular reflections and improves the visual performance of the depth estimation overall. However,\nusing regular OpenCV datatypes leads to unnecessary I/O operations to transfer data from Holoscan Tensors to the CPU and back.\nWe show in this application how to blend together Holoscan Tensors and OpenCV's `GPUMat` datatype to get rid of this issue in the [`CUDACLAHEOp`](./endoscopy_depth_estimation.py#L163) operator. \nCompare it to [`CPUCLAHEOp`](./endoscopy_depth_estimation.py#L123) for reference.\n\nTo achieve an end-to-end GPU accelerated pipeline / application, the pre-processing operators shall support accessing the GPU memory (Holoscan Tensor) \ndirectly without memory copy / movement in Holoscan SDK. This means that only libraries which implement the [`__cuda_array_interface__`](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html) \nand [DLPack](https://dmlc.github.io/dlpack/latest/) standards allow conversion from/to Holoscan Tensor, such as [cuCIM](https://github.com/rapidsai/cucim).\nOpenCV, however, does not implement neither the `__cuda_array_interface__` nor the standard DLPack, and a little work is needed yet to use this library.\n\nFirst, we convert CuPy arrays to GPUMat using a fix in OpenCV only available from 4.8.0 on. More information [here](https://github.com/opencv/opencv/pull/23371).\nThis is done in the [`gpumat_from_cp_array`](./endoscopy_depth_estimation.py#L32) function. With a `GPUMat`, we can now use any [OpenCV-CUDA operations](https://docs.opencv.org/2.4/modules/gpu/doc/gpu.html).\nOnce the `GPUMat` processing has finished, we have to convert it back to a CuPy tensor with [`gpumat_to_cupy`](./endoscopy_depth_estimation.py#L53). \n\n<hr/>\n\n**Important:** In order to run this application with CUDA acceleration, one must compile [OpenCV with CUDA support](https://docs.opencv.org/4.8.0/d2/dbc/cuda_intro.html).\nWe provide a sample [Dockerfile](./Dockerfile) to build a container based on Holoscan v2.1.0 with the latest version of OpenCV and CUDA support.\nIn case you use it, note that the variable [`CUDA_ARCH_BIN` ](./Dockerfile#L25) must be modified according to your specific GPU\nconfiguration. Refer to [this site](https://developer.nvidia.com/cuda-gpus) to find out your NVIDIA GPU architecture.\n\n<hr/>\n\n### Workflows\n\nThis application can be run with or without Histogram Equalization (CLAHE) by toggling the label `--clahe`.\n\n#### With CLAHE\n![](docs/workflow_depth_estimation_clahe.png)<br>\nFig. 1 Depth Estimation Application with CLAHE enabled\n\nThe pipeline uses a recorded endoscopy video file (generated by `convert_video_to_gxf_entities` script) for input frames. Each input frame in the file is loaded by [Video Stream Replayer](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators) and passed to the following two branches:\n- In the first branch (top), the input frames are passed to the [`CUDACLAHEOp`](./endoscopy_depth_estimation.py#L163), \nthen fed to the [Format Converter](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators)\nto convert their data type from `uint8` to `float32`, and finally fed to the [`InferenceOp`](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\nThe result is then ingested by the [`DepthPostProcessingOp`](./endoscopy_depth_estimation.py#L87), which converts the depth map\nto `uint8` and reorders its dimensions for rendering with [Holoviz](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\n- In the second branch (bottom), the input frames are passed to a [Format Converter](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators)\nthat resizes them. Its output is finally fed to the [`DepthPostProcessingOp`](./endoscopy_depth_estimation.py#L87) for \nrendering with [Holoviz](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\n\n\n#### Without CLAHE\n![](docs/workflow_depth_estimation_noclahe.png)<br>\nFig. 2 Depth Estimation Application with CLAHE disabled\n\nThe pipeline uses a recorded endoscopy video file (generated by `convert_video_to_gxf_entities` script) for input frames. Each input frame in the file is loaded by [Video Stream Replayer](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators)\nand passed to a branch that firstly converts its data type to `float32` and resizes it with a [Format Converter](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\nThen, the preprocessed frames are fed to the [`InferenceOp`](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators)\nand mixed with the original video in the custom [`DepthPostProcessingOp`](./endoscopy_depth_estimation.py#L87) for\nrendering with [Holoviz](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\n\n\n\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n \n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nThis application should **be run in the build directory of Holohub** in order to load the GXF extensions.\nAlternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of\nthe working directory.\n\nNext, run the command to run the application:\n\n```bash\ncd <HOLOHUB_BUILD_DIR>\npython3 <HOLOHUB_SOURCE_DIR>/applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py --data=<DATA_DIR> --model=<MODEL_DIR> --clahe\n```\n\n\n### Container Build & Rud Instructions\n\nBuild container using Holoscan 2.0.0 NGC container as base image and built OpenCV with CUDA ARCH 8.6, 8.7 and 8.9 support for IGX Orin and AGX Orin iGPU and Ampere and Ada Lovelace Architecture dGPUs.\n\n#### Change directory to Holohub source directory\n\n```bash\ncd <HOLOHUB_SOURCE_DIR>\n```\n#### Build container\n\n```bash\n./dev_container build --docker_file applications/endoscopy_depth_estimation/Dockerfile  --img  holohub:depth_estimation\n```\n\n#### Launch container\n\n```bash\n./dev_container  launch  --img holohub:depth_estimation\n```\n\n#### Build app\n\n```bash\n./run build endoscopy_depth_estimation\n```\n\n#### Launch app\n\n```bash\npython3 applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py  --data=data/endoscopy --model=data/endoscopy_depth/\n```\n\n",
        "application_name": "endoscopy_depth_estimation",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run endoscopy_depth_estimation --language python"
    },
    {
        "metadata": {
            "name": "VILA Live",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Large Multimodal Model",
                "Large Vision Model"
            ],
            "ranking": 1,
            "dependencies": {
                "Bootstrap": {
                    "source": "https://getbootstrap.com/docs/4.0/getting-started/contents/",
                    "version": "5.3.1"
                }
            },
            "run": {
                "command": "bash -c '<holohub_app_source>/run_vila_live.sh'",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# \ud83d\udcf7\ud83e\udd16 Holoscan VILA Live\n\nThis application demonstrates how to run [VILA 1.5](https://github.com/Efficient-Large-Model/VILA) models on live video feed with the possibility of changing the prompt in real time.\n\nVILA 1.5 is a family of Vision Language Models (VLM) created by NVIDIA & MIT. It uses SigLIP to encode images into tokens which are fed into an LLM with an accompanying prompt. This application collects video frames from the V4L2 operator and feeds them to an AWQ-quantized VILA 1.5 for inference using the [TinyChat](https://github.com/mit-han-lab/llm-awq/blob/main/tinychat/README.md) library. This allows users to interact with a Generative AI model that is \"watching\" a chosen video stream in real-time.\n\n![Holoscan VILA Live](./screenshot.png)\nNote: This demo currently uses [Llama-3-VILA1.5-8b-AWQ](https://huggingface.co/Efficient-Large-Model/Llama-3-VILA1.5-8b-AWQ), but any of the following AWQ-quantized models from the VILA 1.5 familty should work as long as the file names are changed in the [Dockerfile](./Dockerfile) and [run_vila_live.sh](./run_vila_live.sh):\n- [VILA1.5-3b-AWQ](https://huggingface.co/Efficient-Large-Model/VILA1.5-3b-AWQ)\n- [VILA1.5-3b-s2-AWQ](https://huggingface.co/Efficient-Large-Model/VILA1.5-3b-s2-AWQ)\n- [Llama-3-VILA1.5-8b-AWQ](https://huggingface.co/Efficient-Large-Model/Llama-3-VILA1.5-8b-AWQ)\n- [VILA1.5-13b-AWQ](https://huggingface.co/Efficient-Large-Model/VILA1.5-13b-AWQ)\n- [VILA1.5-40b-AWQ](https://huggingface.co/Efficient-Large-Model/VILA1.5-40b-AWQ)\n\n## \u2699\ufe0f Setup Instructions\nThe app defaults to using the video device at `/dev/video0`\n\n> Note: You can use a USB webcam as the video source, or an MP4 video by following the instructions for the [V4L2_Camera](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/v4l2_camera#use-with-v4l2-loopback-devices) example app.\n\nTo debug if this is the correct device download `v4l2-ctl`:\n```bash\nsudo apt-get install v4l-utils\n```\nTo check for your devices run:\n```bash\nv4l2-ctl --list-devices\n```\nThis command will output something similar to this:\n```bash\nNVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n```\nDetermine your desired video device and edit the source device in [vila_live.yaml](vila_live.yaml)\n\n## \ud83d\ude80 Build and Run Instructions\nFrom the Holohub main directory run the following command:\n```bash\n./dev_container build_and_run vila_live\n```\nNote: The first build will take **~1.5 hours** if you're on ARM64. This is largely due to building [Flash Attention 2](https://github.com/Dao-AILab/flash-attention) since pre-built wheels are not distributed for ARM64 platforms.\n\nOnce the main LMM-based app is running, you will see a link for the app at `http://127.0.0.1:8050`.\n\n## \ud83d\udcbb Supported Hardware\n- IGX w/ dGPU\n- IGX w/ iGPU\n- x86 w/ dGPU\n\n## \ud83d\ude4c Acknowledgements\n- Jetson AI Lab, [Live LLaVA](https://www.jetson-ai-lab.com/tutorial_live-llava.html): for the inspiration to create this app\n- [Jetson-Containers](https://github.com/dusty-nv/jetson-containers/tree/master/packages/llm/llamaspeak) repo: For the Flask web-app with WebSockets\n- [LLM-AWQ](https://github.com/mit-han-lab/llm-awq) repo: For the example code to create AWQ-powered LLM servers",
        "application_name": "vila_live",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run vila_live --language python"
    },
    {
        "metadata": {
            "name": "Body Pose Estimation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Computer Vision",
                "Human Body Pose Estimation"
            ],
            "ranking": 2,
            "dependencies": {
                "data": [
                    {
                        "model": "https://docs.ultralytics.com/tasks/pose/",
                        "video": "https://www.pexels.com/video/a-woman-showing-her-ballet-skill-in-turning-one-footed-5385885/"
                    }
                ],
                "hardware": [
                    {
                        "name": "camera",
                        "description": "This application requires a Video4Linux (V4L2) compatible device as input.",
                        "required": true
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/body_pose_estimation.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Body Pose Estimation App\n<div align=\"center\">\n    <img src=\"./docs/1.png\" width=\"300\" height=\"375\">\n    <img src=\"./docs/2.png\" width=\"300\" height=\"375\">\n    <img src=\"./docs/3.png\" width=\"300\" height=\"375\">\n</div>\n\nBody pose estimation is a computer vision task that involves recognizing specific points on the human body in images or videos.\nA model is used to infer the locations of keypoints from the source video which is then rendered by the visualizer. \n\n## Model\n\nThis application uses YOLOv8 pose model from [Ultralytics](https://docs.ultralytics.com/tasks/pose/) for body pose estimation.\nThe model is downloaded when building the application.\n\n## Data\n\nThis application downloads a pre-recorded video from [Pexels](https://www.pexels.com/video/a-woman-showing-her-ballet-skill-in-turning-one-footed-5385885/) when the application is built for use with this application.  Please review the [license terms](https://www.pexels.com/license/) from Pexels.\n\n> **_NOTE:_** The user is responsible for checking if the dataset license is fit for the intended purpose.\n\n## Input\n\nThis app currently supports three different input options:\n\n1. v4l2 compatible input device (default, see V4L2 Support below)\n2. pre-recorded video (see Video Replayer Support below)\n3. DDS video stream (see DDS Support below)\n\n## Run Instructions\n\n## V4L2 Support\n\nThis application supports v4l2 compatible devices as input.  To run this application with your v4l2 compatible device,\nplease plug in your input device and run:\n```sh\n./dev_container build_and_run body_pose_estimation\n```\n\nBy default, this application expects the input device to be mounted at `/dev/video0`.  If this is not the case, please update\n`applications/body_pose_estimation/body_pose_estimation.yaml` and set it to use the corresponding input device before\nrunning the application.  You can also override the default input device on the command line by running:\n```sh\n./dev_container build_and_run body_pose_estimation --run_args \"--video_device /dev/video0\"\n```\n\n## Video Replayer Support\n\nIf you don't have a v4l2 compatible device plugged in, you may also run this application on a pre-recorded video.\nTo launch the application using the Video Stream Replayer as the input source, run:\n\n```sh\n./dev_container build_and_run body_pose_estimation --run_args \"--source replayer\"\n```\n\n## DDS Support\n\nThis application supports using a DDS video stream as the input as well as\npublishing the output video stream back to DDS. To enable DDS, the application\nmust first be built with the DDS operators enabled. Only the subscriber or\npublisher operators need to be enabled for the sake of input or output video\nstreams, respectively, but to enable both use the following:\n\n```sh\n./run build body_pose_estimation --with \"dds_video_subscriber;dds_video_publisher\"\n```\n\nNote that building these operators requires [RTI Connext](https://content.rti.com/l/983311/2024-04-30/pz1wms)\nbe installed. See the [DDS Operator Documentation](../../operators/dds/README.md)\nfor more information on how to build the operators. If using a development\ncontainer, see the [additional instructions below](#using-a-development-container-with-dds-support).\n\nTo use a DDS video stream as the input to the application, use the `-s=dds`\nargument when running the application:\n\n```sh\n./run launch body_pose_estimation --extra_args -s=dds\n```\n\nTo publish the output result to DDS, edit the `body_pose_estimation.yaml`\nconfiguration file so that the `dds_publisher` `enable` option is `true`:\n\n```yaml\ndds_publisher:\n  enable: true\n```\n\nNote that the default DDS video stream IDs use by the application are `0` for\nthe input and `1` for the output. These can be changed using the `stream_id`\nsettings in the `dds_source` and `dds_publisher` sections of the configuration\nfile, respectively.\n\nTo produce the DDS input stream or to view the output stream generated by this\napplication, the [dds_video](../dds_video/README.md) application can be used.\nFor example, the following will use the `dds_video` application to capture\nvideo from the default V4L2 device and publish it to DDS so that it can be\nreceived as input by this application:\n\n```sh\n./run build dds_video\n./run launch dds_video --extra_args \"-p -i 0\"\n```\n\nAnd the following will use the `dds_video` application to receive and render\nthe output published by this application:\n\n\n```sh\n./run launch dds_video --extra_args \"-s -i 1\"\n```\n\n### Using a Development Container with DDS Support\n\nInstalling RTI Connext into the development container is not currently\nsupported, so enabling DDS support with this application requires RTI Connext\nbe installed onto the host and then mounted into the container at runtime.\nTo mount RTI Connext into the container, ensure that the `NDDSHOME` environment\nvariable is set to the path of the RTI Connext installation on the host and\nthen use the following:\n\n```sh\n./dev_container launch --img holohub:bpe --docker_opts \"-v $NDDSHOME:/opt/dds -e NDDSHOME=/opt/dds\"\n```\n",
        "application_name": "body_pose_estimation",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run body_pose_estimation --language python"
    },
    {
        "metadata": {
            "name": "Qt Video Replayer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Qt",
                "QML",
                "QtQuick",
                "Video",
                "UI",
                "Userinterface",
                "Interactive"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/qt_video_replayer",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Qt Video Replayer\n\n![](screenshot.png)<br>\n\nThis application demonstrates how to integrate Holoscan with a [Qt](https://www.qt.io/) application. It support displaying the video frames output by a Holoscan operator and changing operator properties using Qt UI elements.\n\n```mermaid\nflowchart LR\n    subgraph Holoscan application\n        A[(VideoFile)] --> VideostreamReplayerOp\n        VideostreamReplayerOp --> FormatConverterOp\n        FormatConverterOp --> NppFilterOp\n        NppFilterOp --> QtVideoOp\n    end\n    subgraph Qt Window\n        QtVideoOp <-.-> QtHoloscanVideo\n    end\n```\n\nThe application uses the VideostreamReplayerOp to read from a file on disk, the FormatConverterOp to convert the frames from RGB to RGBA, the [NppFilterOp](../../operators/npp_filter/README.md) to apply a filter to the frame and the [QtVideoOp](../../operators/qt_video/README.md) operator to display the video stream in a Qt window.\n\nThe [QtHoloscanApp](./qt_holoscan_app.hpp) class, which extends the `holoscan::Application` class, is used to expose parameters of Holoscan operators as Qt properties.\n\nFor example the application uses a [QML Checkbox](https://doc.qt.io/qt-6/qml-qtquick-controls-checkbox.html) is used the set the `realtime` property of the `VideostreamReplayerOp` operator.\n\n```\n    CheckBox {\n        id: realtime\n        text: \"Use Video Framerate\"\n        checked: holoscanApp.realtime\n        onCheckedChanged: {\n            holoscanApp.realtime = checked;\n        }\n    }\n```\n\nThe [QtHoloscanVideo](../../operators/qt_video/qt_video_op.hpp) is a QQuickItem which can be use in the QML file. Multiple `QtHoloscanVideo` items can be placed in a Qt window.\n\n```\nimport QtHoloscanVideo\nItem {\n    QtHoloscanVideo {\n        objectName: \"video\"\n    }\n}\n```\n\n## Run Instructions\n\nThis application requires [Qt](https://www.qt.io/).\nFor simplicity a DockerFile is available. To generate the container run:\n\n```bash\n./dev_container build --docker_file ./applications/qt_video_replayer/Dockerfile\n```\n\nThe application can then be built by launching this container and using the provided `run` script.\n\n```bash\n./dev_container launch\n./run build qt_video_replayer\n```\n\nOnce the application is build it can be launched with the `run` script.\n\n```bash\n./run launch qt_video_replayer\n```\n",
        "application_name": "qt_video_replayer",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run qt_video_replayer --language cpp"
    },
    {
        "metadata": {
            "name": "Florence-2",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Multimodal Model",
                "Vision Model"
            ],
            "ranking": 1,
            "dependencies": {
                "PySide6": {
                    "version": "6.7.2"
                },
                "flash-attn": {
                    "version": "2.5.9"
                }
            },
            "run": {
                "command": "python3 <holohub_app_source>/qt_app.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# \ud83d\udcf7\ud83e\udd16 Florence-2\n\nThis application demonstrates how to run the [Florence-2](https://arxiv.org/abs/2311.06242) models on a live video feed with the possibility of changing the task and optional prompt via a QT UI.\n\n\n\n<p align=\"center\">\n  <img src=\"./demo.gif\" alt=\"Holoscan VILA Live\">\n</p>\n\nNote: This demo currently uses [Florence-2-large-ft](https://huggingface.co/microsoft/Florence-2-large-ft), but any of the Florence-2 models should work as long as the correct URLs and names are used in [Dockerfile](./Dockerfile) and [config.yaml](./config.yaml):\n- [Florence-2-large-ft](https://huggingface.co/microsoft/Florence-2-large-ft)\n- [Florence-2-large](https://huggingface.co/microsoft/Florence-2-large)\n- [Florence-2-base-ft](https://huggingface.co/microsoft/Florence-2-base-ft)\n- [Florence-2-base](https://huggingface.co/microsoft/Florence-2-base)\n\n## \u2699\ufe0f Setup Instructions\nThe app defaults to using the video device at `/dev/video0`\n\n> Note: You can use a USB webcam as the video source, or an MP4 video by following the instructions for the [V4L2_Camera](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/v4l2_camera#use-with-v4l2-loopback-devices) example app.\n\nTo debug if this is the correct device download `v4l2-ctl`:\n```bash\nsudo apt-get install v4l-utils\n```\nTo check for your devices run:\n```bash\nv4l2-ctl --list-devices\n```\nThis command will output something similar to this:\n```bash\nNVIDIA Tegra Video Input Device (platform:tegra-camrtc-ca):\n        /dev/media0\n\nvi-output, lt6911uxc 2-0056 (platform:tegra-capture-vi:0):\n        /dev/video0\n\nDummy video device (0x0000) (platform:v4l2loopback-000):\n        /dev/video3\n```\nDetermine your desired video device and edit the source device in [config.yaml](config.yaml)\n\n## \ud83d\ude80 Build and Run Instructions\nFrom the Holohub main directory run the following command:\n```bash\n./dev_container build_and_run florence-2\n```\nNote: The first build will take **~1.5 hours** if you're on ARM64. This is largely due to building [Flash Attention 2](https://github.com/Dao-AILab/flash-attention) since pre-built wheels are not distributed for ARM64 platforms.\n\n## \ud83d\udcbb Supported Hardware\n- IGX w/ dGPU\n- x86 w/ dGPU\n> Note: iGPU is not yet supported. This app can be built on iGPU devices and the florence2_app.py will run, however, the full QT UI app is not yet supported for iGPU.",
        "application_name": "florence-2-vision",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run florence-2-vision --language python"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Tracking",
                "AJA"
            ],
            "ranking": 0,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/endoscopy_tool_tracking --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopy Tool Tracking\n\nBased on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\nThe provided applications are configured to either use capture cards for input stream, or a pre-recorded endoscopy video (replayer).\n\nFollow the [setup instructions from the user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\nRefer to the Deltacast documentation to use the Deltacast VideoMaster capture card.\n\nRefer to the Yuan documentation to use the Yuan QCap capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\nIn order to build with the Deltacast VideoMaster operator use ```./run build --with deltacast_videomaster```\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data <data_dir>/endoscopy\n    ```\n\n* Using a vtk_renderer instead of holoviz\n    ```bash\n    sed -i -e 's#^visualizer:.*#visualizer: \"vtk\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data <data_dir>/endoscopy\n    ```\n\n* Using a holoviz instead of vtk_renderer\n    ```bash\n    sed -i -e 's#^visualizer:.*#visualizer: \"holoviz\"#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data <data_dir>/endoscopy\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n    ```\n\n* Using a Deltacast card\n    ```bash\n    sed -i -e '/^#.*deltacast_videomaster/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    sed -i -e 's#^source:.*#source: deltacast#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n    ```\n* Using a Yuan card\n    ```bash\n    sed -i -e '/^#.*yuan_qcap/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    sed -i -e 's#^source:.*#source: yuan#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n    ```\n",
        "application_name": "endoscopy_tool_tracking",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run endoscopy_tool_tracking --language cpp"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Tracking",
                "AJA"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/endoscopy_tool_tracking.py --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopy Tool Tracking\n\nBased on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA or Yuan capture cards for input stream, or a pre-recorded endoscopy video (replayer). \nFollow the [setup instructions from the user guide](https://docs.nvidia.com/holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n \n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n \nNext, run the commands of your choice:\n\nThis application should **be run in the build directory of Holohub** in order to load the GXF extensions.\nAlternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of\nthe working directory.\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3 <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=replayer --data=<DATA_DIR>/endoscopy\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3  <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=aja\n    ```\n\n* Using a YUAN card\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3  <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=yuan\n    ```\n",
        "application_name": "endoscopy_tool_tracking",
        "source_folder": "applications",
        "build_and_run": "./dev_container build_and_run endoscopy_tool_tracking --language python"
    },
    {
        "metadata": {
            "name": "webrtc_client",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Client",
                "Browser",
                "Video"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    }
                ]
            }
        },
        "readme": "### WebRTC Client Operator\n\nThe `webrtc_client` operator receives video frames through a WebRTC connection. The application using this operator needs to call the `offer` method of the operator when a new WebRTC connection is available.\n\n### Methods\n\n- **`async def offer(self, sdp, type) -> (local_sdp, local_type)`**\n  Start a connection between the local computer and the peer.\n\n  **Parameters**\n  - **sdp** peer Session Description Protocol object\n  - **type** peer session type\n\n  **Return values**\n  - **sdp** local Session Description Protocol object\n  - **type** local session type\n\n### Outputs\n\n- **`output`**: Tensor with 8 bit per component RGB data\n  - type: `Tensor`\n",
        "application_name": "webrtc_client",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "npp_filter",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Filter",
                "NPP",
                "Gauss",
                "Sobel"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### NPP Filter\n\nThe `npp_filter` operator uses [NPP](https://developer.nvidia.com/npp) to apply a filters to a Tensor or VideBuffer.\n\n#### `holoscan::ops::NppFilter`\n\nOperator class to apply a filter of the [NPP library]() to a Tensor or VideBuffer.\n\n##### Parameters\n\n- **`filter`**: Name of the filter to apply (supported Gauss, SobelHoriz, SobelVert)\n  - type: `std::string`\n- **`mask_size`**: Filter mask size (supported values 3, 5, 7, 9, 11, 13)\n  - type: `uint32_t`\n- **`allocator`**: Allocator used to allocate the output data\n  - type: `std::shared_ptr<Allocator>`\n\n##### Inputs\n\n- **`input`**: Input frame data\n  - type: `nvidia::gxf::Tensor` or `nvidia::gxf::VideoBuffer`\n\n##### Outputs\n\n- **`input`**: Output frame data\n  - type: `nvidia::gxf::Tensor` or `nvidia::gxf::VideoBuffer`\n",
        "application_name": "npp_filter",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "emergent_source",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "emergent_source",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "mesh_to_usd",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "mesh",
                "OpenUSD",
                "STL"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "mesh_to_usd",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrTransformOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial Release",
                "1.0": "Update for Holoscan SDK v2.0.0"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### User interface Control Operator\n\nThe `XrTransformControlOp` maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.\n\n#### `holoscan::openxr::XrTransformControlOp`\n\n##### Inputs\n \nController state\n- **`trigger_click`**: trigger button state\n  - type: `bool`\n- **`shoulder_click`**: shoulder button state\n  - type: `bool`\n- **`trackpad_touch`**: trackpad state\n  - type: `bool`\n- **`trackpad`**: trackpad values [x,y]\n  - type: `std::array<float, 2>`\n- **`aim_pose`**: world space pose of the controller tip\n  - type: `nvidia::gxf::Pose3D`\n\nDevice state\n- **`head_pose`**: world space head pose of the device\n  - type: `nvidia::gxf::Pose3D`\n\nVolume state\n- **`extent`**: size of bounding box containing volume\n  - type: `std::array<float, 3>`\n\n##### Outputs\n\nUser interface widget state structures\n- **`ux_box`**: bounding box state structure\n  - type: `UxBoundingBox`\n- **`ux_cursor`**: cursor state structure\n  - type: `UxCursor`\n\nVolume rendering parameters\n- **`volume_pose`**: world pose of dataset \n  - type: `nvidia::gxf::Pose3D`\n- **`crop_box`**: axis aligned cropping planes in local coordinates\n  - type: `std::array<nvidia::gxf::Vector2f, 3>`",
        "application_name": "XrTransformOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrTransformRenderOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial Release",
                "1.0": "Update for Holoscan SDK v2.0.0"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### User interface Render Operator\n\nThe `XrTransformRenderOp` renders the mixed reality user interface of the volumetric rendering application. It consumes interface widget state structures as well as render buffers into which to overlay the interface widgets. The operator is application specific and will grow over time to include additional user interface widgets.\n\n#### `holoscan::openxr::XrTransformRenderOp`\n\n##### Parameters \n\n- **`display_width`**: pixel height of display\n  - type: `int`\n- **`display_height`**: pixel width of display\n  - type: `int`\n \n##### Inputs\n\nCamera state for stereo view\n- **`left_camera_pose`**: world space pose of the left eye\n  - type: `nvidia::gxf::Pose3D`\n- **`right_camera_pose`**: world space pose of the right eye\n  - type: `nvidia::gxf::Pose3D`\n- **`left_camera_model`**: camera model for the left eye\n  - type: `nvidia::gxf::CameraModel`\n- **`right_camera_model`**: camera model for the right eye\n  - type: `nvidia::gxf::CameraModel`\n- **`depth_range`**: depth range\n\nUser interface widget state structures\n- **`ux_box`**: bounding box state structure\n  - type: `UxBoundingBox`\n- **`ux_cursor`**: cursor state structure\n  - type: `UxCursor`\n\nRender buffers to be populated\n- **`Collor buffer_in`**: color buffer\n  - type: `holoscan::gxf::VideoBuffer`\n- **`Depth buffer_in`**: depth buffer\n  - type: `holoscan::gxf::VideoBuffer`\n\n##### Outputs\n\nRender buffers including interface widgets\n- **`color_buffer_out`**: color buffer\n  - type: `holoscan::gxf::VideoBuffer`\n- **`depth_buffer_out`**: depth buffer\n  - type: `holoscan::gxf::VideoBuffer`\n",
        "application_name": "XrTransformRenderOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrTransformControlOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial Release",
                "1.0": "Update for Holoscan SDK v2.0.0"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### User interface Control Operator\n\nThe `XrTransformControlOp` maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.\n\n#### `holoscan::openxr::XrTransformControlOp`\n\n##### Inputs\n \nController state\n- **`trigger_click`**: trigger button state\n  - type: `bool`\n- **`shoulder_click`**: shoulder button state\n  - type: `bool`\n- **`trackpad_touch`**: trackpad state\n  - type: `bool`\n- **`trackpad`**: trackpad values [x,y]\n  - type: `std::array<float, 2>`\n- **`aim_pose`**: world space pose of the controller tip\n  - type: `nvidia::gxf::Pose3D`\n\nDevice state\n- **`head_pose`**: world space head pose of the device\n  - type: `nvidia::gxf::Pose3D`\n\nVolume state\n- **`extent`**: size of bounding box containing volume\n  - type: `std::array<float, 3>`\n\n##### Outputs\n\nUser interface widget state structures\n- **`ux_box`**: bounding box state structure\n  - type: `UxBoundingBox`\n- **`ux_cursor`**: cursor state structure\n  - type: `UxCursor`\n\nVolume rendering parameters\n- **`volume_pose`**: world pose of dataset \n  - type: `nvidia::gxf::Pose3D`\n- **`crop_box`**: axis aligned cropping planes in local coordinates\n  - type: `std::array<nvidia::gxf::Vector2f, 3>`",
        "application_name": "XrTransformControlOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "lstm_tensor_rt_inference",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "LSTM",
                "TensorRT"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Custom LSTM Inference\n\nThe `lstm_tensor_rt_inference` extension provides LSTM (Long-Short Term Memory) stateful inference module using TensorRT.\n\n#### `nvidia::holoscan::lstm_tensor_rt_inference::TensorRtInference`\n\nCodelet, taking input tensors and feeding them into TensorRT for LSTM inference.\n\nThis implementation is based on `nvidia::gxf::TensorRtInference`.\n`input_state_tensor_names` and `output_state_tensor_names` parameters are added to specify tensor names for states in LSTM model.\n\n##### Parameters\n\n- **`model_file_path`**: Path to ONNX model to be loaded\n  - type: `std::string`\n- **`engine_cache_dir`**: Path to a directory containing cached generated engines to be serialized and loaded from\n  - type: `std::string`\n- **`plugins_lib_namespace`**: Namespace used to register all the plugins in this library (default: `\"\"`)\n  - type: `std::string`\n- **`force_engine_update`**: Always update engine regard less of existing engine file. Such conversion may take minutes (default: `false`)\n  - type: `bool`\n- **`input_tensor_names`**: Names of input tensors in the order to be fed into the model\n  - type: `std::vector<std::string>`\n- **`input_state_tensor_names`**: Names of input state tensors that are used internally by TensorRT\n  - type: `std::vector<std::string>`\n- **`input_binding_names`**: Names of input bindings as in the model in the same order of what is provided in input_tensor_names\n  - type: `std::vector<std::string>`\n- **`output_tensor_names`**: Names of output tensors in the order to be retrieved from the model\n  - type: `std::vector<std::string>`\n- **`input_state_tensor_names`**: Names of output state tensors that are used internally by TensorRT\n  - type: `std::vector<std::string>`\n- **`output_binding_names`**: Names of output bindings in the model in the same order of of what is provided in output_tensor_names\n  - type: `std::vector<std::string>`\n- **`pool`**: Allocator instance for output tensors\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`cuda_stream_pool`**: Instance of gxf::CudaStreamPool to allocate CUDA stream\n  - type: `gxf::Handle<gxf::CudaStreamPool>`\n- **`max_workspace_size`**: Size of working space in bytes (default: `67108864l` (64MB))\n  - type: `int64_t`\n- **`dla_core`**: DLA Core to use. Fallback to GPU is always enabled. Default to use GPU only (`optional`)\n  - type: `int64_t`\n- **`max_batch_size`**: Maximum possible batch size in case the first dimension is dynamic and used as batch size (default: `1`)\n  - type: `int32_t`\n- **`enable_fp16_`**: Enable inference with FP16 and FP32 fallback (default: `false`)\n  - type: `bool`\n- **`verbose`**: Enable verbose logging on console (default: `false`)\n  - type: `bool`\n- **`relaxed_dimension_check`**: Ignore dimensions of 1 for input tensor dimension check (default: `true`)\n  - type: `bool`\n- **`clock`**: Instance of clock for publish time (`optional`)\n  - type: `gxf::Handle<gxf::Clock>`\n- **`rx`**: List of receivers to take input tensors\n  - type: `std::vector<gxf::Handle<gxf::Receiver>>`\n- **`tx`**: Transmitter to publish output tensors\n  - type: `gxf::Handle<gxf::Transmitter>`\n",
        "application_name": "lstm_tensor_rt_inference",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "orsi_segmentation_preprocessor",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "preprocessor"
            ],
            "ranking": 3,
            "dependencies": {}
        },
        "readme": "",
        "application_name": "orsi_segmentation_preprocessor",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "orsi_format_converter",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "converter"
            ],
            "ranking": 3,
            "dependencies": {}
        },
        "readme": "",
        "application_name": "orsi_format_converter",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "orsi_segmentation_postprocessor",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "postprocessor"
            ],
            "ranking": 3,
            "dependencies": {}
        },
        "readme": "",
        "application_name": "orsi_segmentation_postprocessor",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "orsi_visualizer",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "visualizer"
            ],
            "ranking": 3,
            "dependencies": {}
        },
        "readme": "",
        "application_name": "orsi_visualizer",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_encoder_request",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "encoder",
                "request"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videoencoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videoencoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Encoder Request\n\nThe `video_encoder_request` handles the input for encoding YUV frames to H264 bit stream.\n\n#### `holoscan::ops::VideoEncoderOp`\n\nOperator class to handle the input for encoding YUV frames to H264 bit stream.\n\nThis implementation is based on `nvidia::gxf::VideoEncoderRequest`.\n\n##### Parameters\n\n- **`input_frame`**: Receiver to get the input frame.\n  - type: `holoscan::IOSpec*`\n- **`videoencoder_context`**: Encoder context Handle.\n  - type: `std::shared_ptr<holoscan::ops::VideoEncoderContext>`\n- **`inbuf_storage_type`**: Input Buffer storage type, 0: kHost, 1: kDevice. Default: 1\n  - type: `uint32_t`\n- **`codec`**: Video codec to use,  0: H264, only H264 supported. Default: 0.\n  - type: `int32_t`\n- **`input_height`**: Input frame height.\n  - type: `uint32_t`\n- **`input_width`**: Input image width.\n  - type: `uint32_t`\n- **`input_format`**: Input color format, nv12,nv24,yuv420planar. Default: nv12.\n  - type: `nvidia::gxf::EncoderInputFormat`\n- **`profile`**: Encode profile, 0: Baseline Profile, 1: Main, 2: High. Default: 2.\n  - type: `int32_t`\n- **`bitrate`**: Bitrate of the encoded stream, in bits per second. Default: 20000000.\n  - type: `int32_t`\n- **`framerate`**: Frame Rate, frames per second. Default: 30.\n  - type: `int32_t`\n- **`qp`**: Encoder constant QP value. Default: 20.\n  - type: `uint32_t`\n- **`level`**: Video H264 level. Maximum data rate and resolution, select from 0 to 14. Default: 14.\n  - type: `int32_t`\n- **`iframe_interval`**: I Frame Interval, interval between two I frames. Default: 30.\n  - type: `int32_t`\n- **`rate_control_mode`**: Rate control mode, 0: CQP[RC off], 1: CBR, 2: VBR. Default: 1.\n  - type: `int32_t`\n- **`config`**: Preset of parameters, select from pframe_cqp, iframe_cqp, custom. Default: custom.\n  - type: `nvidia::gxf::EncoderConfig`\n\n",
        "application_name": "video_encoder_request",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "dds_shapes",
            "description": "This operator subscribes to DDS topics published by the RTI Connext shapes demo.",
            "authors": [
                {
                    "name": "Ian Stewart",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DDS",
                "RTI Connext",
                "Shapes"
            ],
            "ranking": 2,
            "dependencies": {
                "packages": [
                    {
                        "name": "RTI Connext",
                        "author": "Real-Time Innovations",
                        "license": "Closed",
                        "version": "6.1.2",
                        "url": "https://www.rti.com/products"
                    }
                ]
            }
        },
        "readme": "### DDS Shape Subscriber Operator\n\nThe DDS Shape Subscriber Operator subscribes to and reads from the `Square`, `Circle`, and\n`Triangle` shape topics as used by the [RTI Shapes Demo](https://www.rti.com/free-trial/shapes-demo).\nIt will then translate the received shape data to an internal `Shape` datatype for output\nto downstream operators.\n\nThis operator requires an installation of [RTI Connext](https://content.rti.com/l/983311/2024-04-30/pz1wms)\nto provide access to the DDS domain, as specified by the [OMG Data-Distribution Service](https://www.omg.org/omg-dds-portal/)\n\n#### `holoscan::ops::DDSShapesSubscriberOp`\n\nOperator class for the DDS Shapes Subscriber.\n\nThis operator also inherits the parameters from [DDSOperatorBase](../base/README.md).\n\n##### Parameters\n\n- **`reader_qos`**: The name of the QoS profile to use for the DDS DataReader\n  - type: `std::string`\n\n##### Outputs\n\n- **`output`**: Output shapes, translated from those read from DDS\n  - type: `holoscan::ops::DDSShapesSubscriberOp::Shape`\n",
        "application_name": "dds_shapes_subscriber",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "dds_base",
            "description": "A base DDS operator that can be derived to create custom DDS operators.",
            "authors": [
                {
                    "name": "Ian Stewart",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DDS",
                "RTI Connext",
                "Shapes"
            ],
            "ranking": 2,
            "dependencies": {
                "packages": [
                    {
                        "name": "RTI Connext",
                        "author": "Real-Time Innovations",
                        "license": "Closed",
                        "version": "6.1.2",
                        "url": "https://www.rti.com/products"
                    }
                ]
            }
        },
        "readme": "### DDS Base Operator\n\nThe DDS Base Operator provides a base class which can be inherited by any\noperator class which requires access to a DDS domain.\n\nThis operator requires an installation of [RTI Connext](https://content.rti.com/l/983311/2024-04-30/pz1wms)\nto provide access to the DDS domain, as specified by the [OMG Data-Distribution Service](https://www.omg.org/omg-dds-portal/)\n\n#### `holoscan::ops::DDSOperatorBase`\n\nBase class which provides the parameters and members required to access a\nDDS domain.\n\nFor more documentation about how these parameters (and other similar\ninheriting-class parameters) are used, see the\n[RTI Connext Documentation](https://community.rti.com/documentation).\n\n##### Parameters\n\n- **`qos_provider`**: URI for the DDS QoS Provider\n  - type: `std::string`\n- **`participant_qos`**: Name of the QoS profile to use for the DDS DomainParticipant\n  - type: `std::string`\n- **`domain_id`**: The ID of the DDS domain to use\n  - type: `uint32_t`\n",
        "application_name": "base",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "dds_video_publisher",
            "description": "Publishes video frames as a DDS topic",
            "authors": [
                {
                    "name": "Ian Stewart",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DDS",
                "RTI Connext",
                "Video"
            ],
            "ranking": 2,
            "dependencies": {
                "packages": [
                    {
                        "name": "RTI Connext",
                        "author": "Real-Time Innovations",
                        "license": "Closed",
                        "version": "6.1.2",
                        "url": "https://www.rti.com/products"
                    }
                ]
            }
        },
        "readme": "### DDS Video Operators\n\nThe DDS Video Operators allow applications to read or write video buffers\nto a DDS databus, enabling communication with other applications via the\n[VideoFrame](VideoFrame.idl) DDS topic.\n\nThis operator requires an installation of [RTI Connext](https://content.rti.com/l/983311/2024-04-30/pz1wms)\nto provide access to the DDS domain, as specified by the [OMG Data-Distribution Service](https://www.omg.org/omg-dds-portal/)\n\n#### `holoscan::ops::DDSVideoPublisherOp`\n\nOperator class for the DDS video publisher. This operator accepts `VideoBuffer` objects\nas input and publishes each buffer to DDS as a [VideoFrame](VideoFrame.idl).\n\nThis operator also inherits the parameters from [DDSOperatorBase](../base/README.md).\n\n##### Parameters\n\n- **`writer_qos`**: The name of the QoS profile to use for the DDS DataWriter\n  - type: `std::string`\n- **`stream_id`**: The ID to use for the video stream\n  - type: `uint32_t`\n\n##### Inputs\n\n- **`input`**: Input video buffer\n  - type: `nvidia::gxf::VideoBuffer`\n\n#### `holoscan::ops::DDSVideoSubscriberOp`\n\nOperator class for the DDS video subscriber. This operator reads from the\n[VideoFrame](VideoFrame.idl) DDS topic and outputs each received frame as\n`VideoBuffer` objects.\n\nThis operator also inherits the parameters from [DDSOperatorBase](../base/README.md).\n\n##### Parameters\n\n- **`reader_qos`**: The name of the QoS profile to use for the DDS DataReader\n  - type: `std::string`\n- **`stream_id`**: The ID of the video stream to filter for\n  - type: `uint32_t`\n- **`allocator`**: Allocator used to allocate the output data\n  - type: `std::shared_ptr<Allocator>`\n\n##### Outputs\n\n- **`output`**: Output video buffer\n  - type: `nvidia::gxf::VideoBuffer`\n",
        "application_name": "dds_video_publisher",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "dds_video_subscriber",
            "description": "This operator subscribes to a DDS topic that contains video data.",
            "authors": [
                {
                    "name": "Ian Stewart",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DDS",
                "RTI Connext",
                "Video"
            ],
            "ranking": 2,
            "dependencies": {
                "packages": [
                    {
                        "name": "RTI Connext",
                        "author": "Real-Time Innovations",
                        "license": "Closed",
                        "version": "6.1.2",
                        "url": "https://www.rti.com/products"
                    }
                ]
            }
        },
        "readme": "### DDS Video Operators\n\nThe DDS Video Operators allow applications to read or write video buffers\nto a DDS databus, enabling communication with other applications via the\n[VideoFrame](VideoFrame.idl) DDS topic.\n\nThis operator requires an installation of [RTI Connext](https://content.rti.com/l/983311/2024-04-30/pz1wms)\nto provide access to the DDS domain, as specified by the [OMG Data-Distribution Service](https://www.omg.org/omg-dds-portal/)\n\n#### `holoscan::ops::DDSVideoPublisherOp`\n\nOperator class for the DDS video publisher. This operator accepts `VideoBuffer` objects\nas input and publishes each buffer to DDS as a [VideoFrame](VideoFrame.idl).\n\nThis operator also inherits the parameters from [DDSOperatorBase](../base/README.md).\n\n##### Parameters\n\n- **`writer_qos`**: The name of the QoS profile to use for the DDS DataWriter\n  - type: `std::string`\n- **`stream_id`**: The ID to use for the video stream\n  - type: `uint32_t`\n\n##### Inputs\n\n- **`input`**: Input video buffer\n  - type: `nvidia::gxf::VideoBuffer`\n\n#### `holoscan::ops::DDSVideoSubscriberOp`\n\nOperator class for the DDS video subscriber. This operator reads from the\n[VideoFrame](VideoFrame.idl) DDS topic and outputs each received frame as\n`VideoBuffer` objects.\n\nThis operator also inherits the parameters from [DDSOperatorBase](../base/README.md).\n\n##### Parameters\n\n- **`reader_qos`**: The name of the QoS profile to use for the DDS DataReader\n  - type: `std::string`\n- **`stream_id`**: The ID of the video stream to filter for\n  - type: `uint32_t`\n- **`allocator`**: Allocator used to allocate the output data\n  - type: `std::shared_ptr<Allocator>`\n\n##### Outputs\n\n- **`output`**: Output video buffer\n  - type: `nvidia::gxf::VideoBuffer`\n",
        "application_name": "dds_video_subscriber",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "openigtlink",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Streaming",
                "Ethernet",
                "3DSlicer"
            ],
            "ranking": 2,
            "dependencies": {
                "SDK": "OpenIGTLink"
            }
        },
        "readme": "### OpenIGTLink operator\n\nThe `openigtlink` operator provides a way to send and receive imaging data using the [OpenIGTLink](http://openigtlink.org/) library. The `openigtlink` operator contains separate operators for transmit and receive. Users may choose one or the other, or use both in applications requiring bidirectional traffic.\n\nThe `openigtlink` operators use class names: `OpenIGTLinkTxOp` and `OpenIGTLinkRxOp`\n\n#### `nvidia::holoscan::openigtlink`\n\nOperator class to send and transmit data using the OpenIGTLink protocol.\n\n##### Receiver Configuration Parameters\n\n- **`port`**: Port number of server\n  - type: `integer`\n- **`out_tensor_name`**: Name of output tensor\n  - type: `string`\n- **`flip_width_height`**: Flip width and height (necessary for receiving from 3D Slicer)\n  - type: `bool`\n\n##### Transmitter Configuration Parameters\n\n- **`device_name`**: OpenIGTLink device name\n  - type: `string`\n- **`input_names`**: Names of input messages\n  - type: `std::vector<std::string>`\n- **`host_name`**: Host name\n  - type: `string`\n- **`port`**: Port number of server\n  - type: `integer`",
        "application_name": "openigtlink",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "Intel RealSense Camera Operator",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "0.1.0",
            "changelog": {
                "0.1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.1.0",
                "tested_versions": [
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "# Intel RealSense Camera Operator\n\n## Overview\n\nCaptures frames from an Intel RealSense camera.\n",
        "application_name": "realsense_camera",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "yuan_qcap",
            "authors": [
                {
                    "name": "David Su",
                    "affiliation": "Yuan"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Yuan"
            ],
            "ranking": 2,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "yuan_qcap",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "yuan_qcap",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "visualizer_icardio",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Visualizer iCardio\n\nThe `visualizer_icardio` extension generates the visualization components from the processed results of the plax chamber model.\n\n#### `nvidia::holoscan::multiai::VisualizerICardio`\n\nVisualizer iCardio extension ingests the processed results of the plax chamber model and generates the key points, the key areas and the lines that are transmitted to the HoloViz codelet.\n\n##### Parameters\n\n- **`in_tensor_names_`**: Input tensor names\n  - type: `std::vector<std::string>`\n- **`out_tensor_names_`**: Output tensor names\n  - type: `std::vector<std::string>`\n- **`allocator_`**: Memory allocator\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`receivers_`**: Vector of input receivers. Multiple receivers supported.\n  - type: `HoloInfer::GXFReceivers`\n- **`transmitter_`**: Output transmitter. Single transmitter supported.\n  - type: `HoloInfer::GXFTransmitters`\n",
        "application_name": "visualizer_icardio",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "advanced_network",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.2",
            "changelog": {
                "1.3": "Plugin support",
                "1.2": "TX GPUDirect",
                "1.1": "GPUDirect updates",
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "DPDK",
                "UDP",
                "Ethernet",
                "IP",
                "GPUDirect",
                "RDMA"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "advanced_network",
                        "version": "1.3"
                    }
                ]
            }
        },
        "readme": "### Advanced Network Operator\n\nThe Advanced Network Operator provides a way for users to achieve the highest throughput and lowest latency\nfor transmitting and receiving Ethernet frames out of and into their operators. Direct access to the NIC hardware\nis available in userspace using this operator, thus bypassing the kernel's networking stack entirely. With a\nproperly tuned system the advanced network operator can achieve hundreds of Gbps with latencies in the low\nmicroseconds. Performance is highly dependent on system tuning, packet sizes, batch sizes, and other factors.\nThe data may optionally be sent to the GPU using GPUDirect to prevent extra copies to and from the CPU.\n\nSince the kernel's networking stack is bypassed, the user is responsible for defining the protocols used\nover the network. In most cases Ethernet, IP, and UDP are ideal for this type of processing because of their\nsimplicity, but any type of protocol can be implemented or used. The advanced network operator\ngives the option to use several primitives to remove the need for filling out these headers for basic packet types,\nbut raw headers can also be constructed.\n\n#### Requirements\n\n- Linux\n- A DPDK-compatible network card. For GPUDirect only NVIDIA NICs are supported\n- System tuning as described below\n- DPDK 22.11\n- MOFED 5.8-1.0.1.1 or later\n- DOCA 2.7 or later\n\n#### Features\n\n- **High Throughput**: Hundreds of gigabits per second is possible with the proper hardware\n- **Low Latency**: With direct access to the NIC's ring buffers, most latency incurred is only PCIe latency\n- **GPUDirect**: Optionally send data directly from the NIC to GPU, or directly from the GPU to NIC. GPUDirect has two modes:\n  - Header-data split: Split the header portion of the packet to the CPU and the rest (payload) to the GPU. The split point is\n    configurable by the user. This option should be the preferred method in most cases since it's easy to use and still\n    gives near peak performance.\n  - Batched GPU: Receive batches of whole packets directly into the GPU memory. This option requires the GPU kernel to inspect\n    and determine how to handle packets. While performance may increase slightly over header-data split, this method\n    requires more effort and should only be used for advanced users.\n- **GPUComms**: Optionally control the send or receive communications from the GPU through the GPUDirect Async Kernel-Initiated network technology (enabled with the [DOCA GPUNetIO](https://docs.nvidia.com/doca/sdk/doca+gpunetio/index.html) transport layer only).\n- **Flow Configuration**: Configure the NIC's hardware flow engine for configurable patterns. Currently only UDP source\n    and destination are supported.\n\n#### Limitations\n\nThe limitations below will be removed in a future release.\n\n- Only UDP fill mode is supported\n\n#### Implementation\n\nInternally the advanced network operator can be implemented by different transport layers, each offering different features.\nThe network transport layer must be specified at the beginning of the application using the ANO API.\n\n##### DPDK\n\nDPDK is an open-source userspace packet processing library supported across platforms and vendors.\nWhile the DPDK interface is abstracted away from users of the advanced network operator,\nthe method in which DPDK integrates with Holoscan is important for understanding how to achieve the highest performance and for debugging.\n\nWhen the advanced network operator is compiled/linked against a Holoscan application, an instance of the DPDK manager\nis created, waiting to accept configuration. When either an RX or TX advanced network operator is defined in a\nHoloscan application, their configuration is sent to the DPDK manager. Once all advanced network operators have initialized,\nthe DPDK manager is told to initialize DPDK. At this point the NIC is configured using all parameters given by the operators.\nThis step allocates all packet buffers, initializes the queues on the NIC, and starts the appropriate number of internal\nthreads. The job of the internal threads is to take packets off or put packets onto the NIC as fast as possible. They\nact as a proxy between the advanced network operators and DPDK by handling packets faster than the operators may be\nable to.\n\nTo achieve zero copy throughout the whole pipeline only pointers are passed between each entity above. When the user\nreceives the packets from the network operator it's using the same buffers that the NIC wrote to either CPU or GPU\nmemory. This architecture also implies that the user must explicitly decide when to free any buffers it's owning.\nFailure to free buffers will result in errors in the advanced network operators not being able to allocate buffers.\n\n\n##### DOCA\n\nNVIDIA DOCA brings together a wide range of powerful APIs, libraries, and frameworks for programming and accelerating modern data center infrastructures\u200b. The DOCA SDK composed by a variety of C/C++ API for different purposes\u200b, exposing all the features supported by NVIDIA hardware and platforms. [DOCA GPUNetIO](https://docs.nvidia.com/doca/sdk/doca+gpunetio/index.html) is one of the libraries included in the SDK and it enables the GPU to control, from a CUDA kernel, network communications directly interacting with the network card and completely removing the CPU from the critical data path.\n\nIf the application wants to enable GPU communications, it must chose DOCA as transport layer. The behaviour of the DOCA transport layer is similar to the DPDK one except that the receive and send are executed by CUDA kernels. Specifically:\n- Receive: a persistent CUDA kernel is running on a dedicated stream and keeps receiving packets, providing packets' info to the application level. Due to the nature of the operator, the CUDA receiver kernel now is responsible only to receive packets but in a real-world application, it can be extended to receive and process in real-time network packets (DPI, filtering, decrypting, byte modification, etc..) before forwarding packets to the application.\n- Send: every time the application wants to send packets it launches one or more CUDA kernels to prepare data and create Ethernet packets and then (without the need of synchronizing) forward the send request to the operator. The operator then launches another CUDA kernel that in turn sends the packets (still no need to synchronize with the CPU). The whole pipeline is executed on the GPU. Due to the nature of the operator, the packets' creation and packets' send must be split in two CUDA kernels but in a real-word application, they can be merged into a single CUDA kernel responsible for both packet processing and packet sending.\n\nPlease refer to the [DOCA GPUNetIO](https://docs.nvidia.com/doca/sdk/doca+gpunetio/index.html) programming guide to correctly configure your system before using this transport layer.\n\nDOCA transport layer doesn't support the `split-boundary` option.\n\nTo build and run the ANO Dockerfile with DOCA support, please follow the steps below:\n\n```\n# To build Docker image\n./dev_container build --docker_file operators/advanced_network/Dockerfile --img holohub-doca:doca-27-ubuntu2204 --no-cache\n\n# Launch DOCA container\n./operators/advanced_network/run_doca.sh\n\n# To build operator + app from main dir\n./run build adv_networking_bench\n\n# Run app\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_doca_tx_rx.yaml\n```\n\n<mark>Receiver side, CUDA Persistent kernel note</mark>\nTo get the best performance on the receive side, the Advanced Network Operator must be built with with `RX_PERSISTENT_ENABLED` set to 1 which enables the CUDA receiver kernel to run persistently for the whole execution. For Holoscan internal reasons (not related to the DOCA library), a persistent CUDA kernel may cause issues on some applications on the receive side. This issue is still under investigation.\nIf this happens, there are two options:\n- build the Advanced Network Operator with `RX_PERSISTENT_ENABLED` set to 0\n- keep the `RX_PERSISTENT_ENABLED` set to 1 and enable also MPS setting `MPS_ENABLED` to 1. Then, MPS should be enabled on the system:\n```\nexport CUDA_MPS_PIPE_DIRECTORY=/var\nexport CUDA_MPS_LOG_DIRECTORY=/var\nsudo -E nvidia-cuda-mps-control -d\nsudo -E echo start_server -uid 0 | sudo -E nvidia-cuda-mps-control\n```\n\nThis should solve all problems. Both `RX_PERSISTENT_ENABLED` and `MPS_ENABLED` are defined in `operators/advanced_network/managers/doca/adv_network_doca_mgr.h`.\n\n\n\n#### System Tuning\n\nFrom a high level, tuning the system for a low latency workload prevents latency spikes large enough to cause anomalies\nin the application. This section details how to perform the basic tuning steps needed on both a Clara AGX and Orin IGX systems.\n\n##### Create Hugepages\n\nHugepages give the kernel access to a larger page size than the default (usually 4K) which reduces the number of memory\ntranslations that have to be actively maintained in MMUs. 1GB hugepages are ideal, but 2MB may be used as well if 1GB is not\navailable. To configure 1GB hugepages:\n\n```\nsudo mkdir /mnt/huge\nsudo mount -t hugetlbfs nodev /mnt/huge\nsudo sh -c \"echo nodev /mnt/huge hugetlbfs pagesize=1GB 0 0 >> /etc/fstab\"\n```\n\n##### Linux Boot Command Line\n\nThe Linux boot command line allows configuration to be injected into Linux before booting. Some configuration options are\nonly available at the boot command since they must be provided before the kernel has started. On the Clara AGX and Orin IGX\nediting the boot command can be done with the following configuration:\n\n```\nsudo vim /boot/extlinux/extlinux.conf\n# Find the line starting with APPEND and add the following\n\n# For Orin IGX:\nisolcpus=6-11 nohz_full=6-11 irqaffinity=0-5 rcu_nocbs=6-11 rcu_nocb_poll tsc=reliable audit=0 nosoftlockup default_hugepagesz=1G hugepagesz=1G hugepages=2\n\n# For Clara AGX:\nisolcpus=4-7 nohz_full=4=7 irqaffinity=0-3 rcu_nocbs=4-7 rcu_nocb_poll tsc=reliable audit=0 nosoftlockup default_hugepagesz=1G hugepagesz=1G hugepages=2\n```\n\nThe settings above isolate CPU cores 6-11 on the Orin and 4-7 on the Clara, and turn 1GB hugepages on.\n\nFor non-IGX or AGX systems please look at the documentation for your system to change the boot command.\n\n##### Setting the CPU governor\n\nThe CPU governor reduces power consumption by decreasing the clock frequency of the CPU when cores are idle. While this is useful\nin most environments, increasing the clocks from an idle period can cause long latency stalls. To disable frequency scaling:\n\n```\nsudo apt install cpufrequtils\nsudo sed -i 's/^GOVERNOR=.*/GOVERNOR=\"performance\"/' /etc/init.d/cpufrequtils\n```\n\nReboot the system after these changes.\n\n##### Permissions\n\nDPDK typically requires running as a root user. If you wish to run as a non-root user, you may follow the directions here:\nhttp://doc.dpdk.org/guides/linux_gsg/enable_func.html\n\nIf running in a container, you will need to run in privileged container, and mount your hugepages mount point from above into the container. This\ncan be done as part of the `docker run` command by adding the following flags:\n\n```\n-v /mnt/huge:/mnt/huge \\\n--privileged \\\n```    \n\n#### Configuration Parameters\n\nThe advanced network operator contains a separate operator for both transmit and receive. This allows applications to choose\nwhether they need to handle bidirectional traffic or only unidirectional. Transmit and receive are configured separately in\na YAML file, and a common configuration contains items used by both directions. Each configuration section is described below.\n\n##### Common Configuration\n\nThe common configuration container parameters are used by both TX and RX:\n\n- **`version`**: Version of the config. Only 1 is valid currently.\n  - type: `integer`\n- **`master_core`**: Master core used to fork and join network threads. This core is not used for packet processing and can be\nbound to a non-isolated core\n  - type: `integer`\n\n##### Receive Configuration\n\n- **`if_name`**: Name of the interface or PCIe BDF to use\n  - type: `string`\n- **`queues`**: Array of queues\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`\n- **`gpu_direct`**: GPUDirect is enabled on the queue\n  - type: `boolean`\n- **`batch_size`**: Number of packets in a batch that is passed between the advanced network operator and the user's operator. A\nlarger number increases throughput and latency by requiring fewer messages between operators, but takes longer to populate a single\nbuffer. A smaller number reduces latency and bandwidth by passing more messages.\n- **`num_concurrent_batches`**: Number of batches that can be outstanding (not freed) at any given time. This value directly affects\nthe amount of memory needed for receiving packets. A value too small and packets will be dropped, while a value too large will\nunnecessarily use excess CPU and/or GPU memory.\n  - type: `integer`\n- **`max_packet_size`**: Largest packet size expected\n  - type: `integer`\n- **`split_boundary`**: Split point in bytes where any byte before this value is sent to CPU, and anything after to GPU\n  - type: `integer`\n- **`gpu_device`**: GPU device number if using GPUDirect\n  - type: `integer`\n- **`cpu_cores`**: List of CPU cores from the isolated set used by the operator for receiving\n  - type: `string`\n- **`flows`**: Array of flows\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`\n- **`action`**: Action section of flow\n  - type: `sequence`\n- **`type`**: Type of action. Only \"queue\" is supported currently.\n  - type: `string`\n- **`id`**: ID of queue to steer to\n  - type: `integer`\n- **`match`**: Match section of flow\n  - type: `sequence`\n- **`udp_src`**: UDP source port\n  - type: `integer`\n- **`udp_dst`**: UDP destination port\n  - type: `integer`\n\n##### Transmit Configuration\n\n- **`if_name`**: Name of the interface or PCIe BDF to use\n  - type: `string`\n- **`accurate_send`**: Boolean flag to turn on accurate TX scheduling\n  - type: `boolean`\n- **`queues`**: Array of queues\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`\n- **`id`**: ID of queue to steer to\n  - type: `integer`\n- **`gpu_direct`**: GPUDirect is enabled on the queue\n  - type: `boolean`\n- **`batch_size`**: Number of packets in a batch that is passed between the advanced network operator and the user's operator. A\nlarger number increases throughput and latency by requiring fewer messages between operators, but takes longer to populate a single\nbuffer. A smaller number reduces latency and bandwidth by passing more messages.\n  - type: `integer`\n- **`max_payload_size`**: Largest payload size expected\n  - type: `integer`\n- **`layer_fill`**: Layer(s) that the advanced network operator should populate in the packet. Anything higher than the layer\nspecified must be populated by the user. For example, if `ethernet` is specified, the user is responsible for populating values of\nany item above that layer (IP, UDP, etc...). Valid values are `raw`, `ethernet`, `ip`, and `udp`\n  - type: `string`\n- **`eth_dst_addr`**: Destination ethernet MAC address. Only used for `ethernet` layer_fill mode or above\n  - type: `string`\n- **`ip_src_addr`**: Source IP address to send packets from. Only used for `ip` layer_fill and above\n  - type: `string`\n- **`ip_dst_addr`**: Destination IP address to send packets to. Only used for `ip` layer_fill and above\n  - type: `string`\n- **`udp_dst_port`**: UDP destination port. Only used for `udp` layer_fill and above\n  - type: `integer`\n- **`udp_src_port`**: UDP source port. Only used for `udp` layer_fill and above\n  - type: `integer`\n- **`cpu_cores`**: List of CPU cores for transmitting\n  - type: `string`\n\n  #### API Structures\n\n  Both the transmit and receive operators use a common structure named `AdvNetBurstParams` to pass data to/from other operators.\n  `AdvNetBurstParams` provides pointers to all packets on the CPU and GPU, and contains metadata needed by the operator to track\n  allocations. Since the advanced network operator utilizes a generic interface that does not expose the underlying low-level network\n  card library, interacting with the `AdvNetBurstParams` is mostly done with the helper functions described below. A user should\n  never modify any members of `AdvNetBurstParams` directly as this may break in future versions. The `AdvNetBurstParams` is described\n  below:\n\n  ```\n  struct AdvNetBurstParams {\n    union {\n        AdvNetBurstParamsHdr hdr;\n        uint8_t buf[HS_NETWORK_HEADER_SIZE_BYTES];\n    };\n\n    void **cpu_pkts;\n    void **gpu_pkts;\n};\n```\n\nStarting from the top, the `hdr` field contains metadata about the batch of packets. `buf` is a placeholder for future expansion\nof fields. `cpu_pkts` contains pointers to CPU packets, while `gpu_pkts` contains pointers to the GPU packets. As mentioned above,\nthe `cpu_pkts` and `gpu_pkts` are opaque pointers and should not be access directly. See the next section for information on interacting\nwith these fields.\n\n#### Example API Usage\n\nFor an entire list of API functions, please see the `adv_network_common.h` header file.\n\n##### Receive\n\nThe section below describes a workflowusing GPUDirect to receive packets using header-data split. The job of the user's operator(s)\nis to process and free the buffers as quickly as possible. This might be copying to interim buffers or freeing before the entire\npipeline is done processing. This allows the networking piece to use relatively few buffers while still achieving very high rates.\n\nThe first step in receiving from the advanced network operator is to tie your operator's input port to the output port of the RX\nnetwork operator's `burst_out` port.\n\n```\nauto adv_net_rx    = make_operator<ops::AdvNetworkOpRx>(\"adv_network_rx\", from_config(\"adv_network_common\"), from_config(\"adv_network_rx\"), make_condition<BooleanCondition>(\"is_alive\", true));\nauto my_receiver   = make_operator<ops::MyReceiver>(\"my_receiver\", from_config(\"my_receiver\"));\nadd_flow(adv_net_rx, my_receiver, {{\"burst_out\", \"burst_in\"}});\n```\n\nOnce the ports are connected, inside the `compute()` function of your operator you will receive a `AdvNetBurstParams` structure\nwhen a batch is complete:\n\n```\nauto burst = op_input.receive<std::shared_ptr<AdvNetBurstParams>>(\"burst_in\").value();\n```\n\nThe packets arrive in scattered packet buffers. Depending on the application, you may need to iterate through the packets to\naggregate them into a single buffer. Alternatively the operator handling the packet data can operate on a list of packet\npointers rather than a contiguous buffer. Below is an example of aggregating separate GPU packet buffers into a single GPU\nbuffer:\n\n```\n  for (int p = 0; p < adv_net_get_num_pkts(burst); p++) {\n    h_dev_ptrs_[aggr_pkts_recv_ + p]   = adv_net_get_cpu_pkt_ptr(burst, p);\n    ttl_bytes_in_cur_batch_           += adv_net_get_gpu_packet_len(burst, p) + sizeof(UDPPkt);\n  }\n\n  simple_packet_reorder(buffer, h_dev_ptrs, packet_len, burst->hdr.num_pkts);\n```\n\nFor this example we are tossing the header portion (CPU), so we don't need to examine the packets. Since we launched a reorder\nkernel to aggregate the packets in GPU memory, we are also done with the GPU pointers. All buffers may be freed to the\nadvanced network operator at this point:\n\n```\nadv_net_free_all_burst_pkts_and_burst(burst_bufs_[b]);\n```\n\n##### Transmit\n\nTransmitting packets works similar to the receive side, except the user is tasked with filling out the packets as much as it\nneeds to. As mentioned above, helper functions are available to fill in most boilerplate header information if that doesn't\nchange often.\n\nSimilar to the receive, the transmit operator needs to connect to `burst_in` on the advanced network operator transmitter:\n\n```\nauto my_transmitter  = make_operator<ops::MyTransmitter>(\"my_transmitter\", from_config(\"my_transmitter\"), make_condition<BooleanCondition>(\"is_alive\", true));  \nauto adv_net_tx       = make_operator<ops::AdvNetworkOpTx>(\"adv_network_tx\", from_config(\"adv_network_common\"), from_config(\"adv_network_tx\"));\nadd_flow(my_transmitter, adv_net_tx, {{\"burst_out\", \"burst_in\"}});\n```\n\nBefore sending packets, the user's transmit operator must request a buffer from the advanced network operator pool:\n\n```\nauto msg = std::make_shared<AdvNetBurstParams>();\nmsg->hdr.num_pkts = num_pkts;\nif ((ret = adv_net_get_tx_pkt_burst(msg.get())) != AdvNetStatus::SUCCESS) {\n  HOLOSCAN_LOG_ERROR(\"Error returned from adv_net_get_tx_pkt_burst: {}\", static_cast<int>(ret));\n  return;\n}\n```\n\nThe code above creates a shared `AdvNetBurstParams` that will be passed to the advanced network operator, and uses\n`adv_net_get_tx_pkt_burst` to populate the burst buffers with valid packet buffers. On success, the buffers inside the\nburst structure will be allocate and are ready to be filled in. Each packet must be filled in by the user. In this\nexample we loop through each packet and populate a buffer:\n\n```\nfor (int num_pkt = 0; num_pkt < msg->hdr.num_pkts; num_pkt++) {\n  void *payload_src = data_buf + num_pkt * nom_pkt_size;\n  if (adv_net_set_udp_payload(msg->cpu_pkts[num_pkt], payload_src, nom_pkt_size) != AdvNetStatus::SUCCESS) {\n    HOLOSCAN_LOG_ERROR(\"Failed to create packet {}\", num_pkt);\n  }\n}\n```\n\nThe code iterates over `msg->hdr.num_pkts` (defined by the user) and passes a pointer to the payload and the packet\nsize to `adv_net_set_udp_payload`. In this example our configuration is using `fill_mode` \"udp\" on the transmitter, so\n`adv_net_set_udp_payload` will populate the Ethernet, IP, and UDP headers. The payload pointer passed by the user\nis also copied into the buffer. Alternatively a user could use the packet buffers directly as output from a previous stage\nto avoid this extra copy.\n\nWith the `AdvNetBurstParams` populated, the burst can be sent off to the advanced network operator for transmission:\n\n```\nop_output.emit(msg, \"burst_out\");\n```\n",
        "application_name": "advanced_network",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "cvcuda_holoscan_interop",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "CV-CUDA",
                "Computer Vision",
                "CV"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Holoscan Sample App Data for AI-based Endoscopy Tool Tracking",
                        "description": "This resource contains the convolutional LSTM model for tool tracking in laparoscopic videos by Nwoye et. al [1], and a sample surgical video.",
                        "url": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                        "version": "20230222"
                    }
                ],
                "libraries": [
                    {
                        "name": "cvcuda",
                        "version": "0.3.1-beta"
                    }
                ]
            }
        },
        "readme": "### CVCUDA Holoscan Interoperability Operators\n\nThis directory contains two operators to enable interoperability between the [CVCUDA](https://github.com/CVCUDA/CV-CUDA) and Holoscan\ntensors: `holoscan::ops::CvCudaToHoloscan` and `holoscan::ops::HoloscanToCvCuda`.\n\n#### `holoscan::ops::CvCudaToHoloscan`\n\nOperator class to convert a `nvcv::Tensor` to a `holoscan::Tensor`.\n\n##### Inputs\n\n- **`input`**: a CV-CUDA tensor\n  - type: `nvcv::Tensor`\n\n##### Outputs\n\n- **`output`**: a Holoscan tensor as `holoscan::Tensor` in `holoscan::TensorMap`\n  - type: `holoscan::TensorMap`\n\n#### `holoscan::ops::HoloscanToCvCuda`\n\n##### Inputs\n\n- **`input`**: a `gxf::Entity` containing a Holoscan tensor as `holoscan::Tensor`\n  - type: `gxf::Entity`\n\n##### Outputs\n\n- **`output`**: a CV-CUDA tensor\n  - type: `nvcv::Tensor`",
        "application_name": "cvcuda_holoscan_interop",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "tensor_to_video_buffer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Tensor",
                "Video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### GXF Tensor to VideoBuffer Converter\n\nThe `tensor_to_video_buffer` converts GXF Tensor to VideoBuffer.\n\n#### `holoscan::ops::TensorToVideoBufferOp`\n\nOperator class to convert GXF Tensor to VideoBuffer. This operator is required\nfor data transfer  between Holoscan operators that output GXF Tensor and\nthe other Holoscan Wrapper Operators that understand only VideoBuffer.\nIt receives GXF Tensor as input and outputs GXF VideoBuffer created from it.\n\n##### Parameters\n\n- **`data_in`**: Data in GXF Tensor format\n - type: `holoscan::IOSpec*`\n- **`data_out`**: Data in GXF VideoBuffer format\n - type: `holoscan::IOSpec*`\n- **`in_tensor_name`**: Name of the input tensor\n  - type: `std::string`\n- **`video_format`**: The video format, supported values: \"yuv420\", \"rgb\"\n  - type: `std::string`\n",
        "application_name": "tensor_to_video_buffer",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XR Basic Rendering Operator",
            "description": "Operator to demonstrate basic rendering using OpenXR",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "NVIDIA Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Gaze tracking",
                "OpenXR",
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### XR Basic Rendering Operator\n\nThe `XrBasicRenderOp` defines and renders a basic scene to demonstrate OpenXR compatibility. It provides visuals for static primitives, controller tracking, and an ImGui window.\n\nSee the [`xr_hello_holoscan` application](/applications/xr_hello_holoscan/) for a complete example demonstrating `XrBasicRenderOp` in a Holoscan SDK pipeline.\n",
        "application_name": "xr_basic_render",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "videomaster",
            "authors": [
                {
                    "name": "Laurent Radoux",
                    "affiliation": "Deltacast"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Deltacast"
            ],
            "ranking": 2,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videomaster",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "# VideoMaster GXF Operator\n\nThis library contains two operators:\n- videomaster_source: get signal from capture card\n- videomaster_transmitter: generate signal\n\nThese operators wrap the GXF extension to provide support for VideoMaster SDK.\n\n## Requirements\n\nThis operator requires the VideoMaster SDK from Deltacast.\n\n## Building the operator\n\nAs part of Holohub, running CMake on Holohub and point to Holoscan SDK install tree.\n\nThe path to the VideoMaster SDK is also mandatory and can be given through the VideoMaster_SDK_DIR parameter.\n",
        "application_name": "deltacast_videomaster",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "webrtc_server",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Server",
                "Browser",
                "Video"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    }
                ]
            }
        },
        "readme": "### WebRTC Server Operator\n\nThe `webrtc_server` operator sends video frames through a WebRTC connection. The application using this operator needs to call the `offer` method of the operator when a new WebRTC connection is available.\n\n### Methods\n\n- **`async def offer(self, sdp, type) -> (local_sdp, local_type)`**\n  Start a connection between the local computer and the peer.\n\n  **Parameters**\n  - **sdp** peer Session Description Protocol object\n  - **type** peer session type\n\n  **Return values**\n  - **sdp** local Session Description Protocol object\n  - **type** local session type\n\n### Inputs\n\n- **`input`**: Tensor or numpy array with 8 bit per component RGB data\n  - type: `Tensor`\n",
        "application_name": "webrtc_server",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "basic_network",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "UDP",
                "Ethernet",
                "IP",
                "TCP"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "\n### Basic networking operator\n\nThe `basic_network_operator` operator provides a way to send and receive data over Linux sockets. The\ndestination can be on the same machine or over a network. The basic network operator contains separate\noperators for transmit and receive. Users may choose one or the other, or use both in applications \nrequiring bidirectional traffic.\n\nFor TCP sockets the basic network operator only supports a single stream currently. Future versions\nmay expand this to launch multiple threads to listen on different streams.\n\nThe basic networking operators use class names: `BasicNetworkOpTx` and `BasicNetworkOpRx`\n\n#### `nvidia::holoscan::basic_network_operator`\n\nBasic networking operator\n\n##### Receiver Configuration Parameters\n\n- **`batch_size`**: Bytes in batch\n  - type: `integer`\n- **`max_payload_size`**: Maximum payload size for a single packet\n  - type: `integer`\n- **`udp_dst_port`**: UDP destination port for packets\n  - type: `integer`\n- **`l4_proto`**: Layer 4 protocol\n  - type: `string` (`udp`/`tcp`)\n- **`ip_addr`**: Destination IP address\n  - type: `string`    \n\n##### Transmitter Configuration Parameters\n\n- **`max_payload_size`**: Maximum payload size for a single packet\n  - type: `integer`\n- **`udp_dst_port`**: UDP destination port for packets\n  - type: `integer`\n- **`l4_proto`**: Layer 4 protocol\n  - type: `string` (`udp`/`tcp`)\n- **`ip_addr`**: Destination IP address\n  - type: `string`    \n- **`min_ipg_ns`**: Minimum inter-packet gap in nanoseconds\n  - type: `integer`  \n\n\n##### Transmitter and Receiver Operator Parameters\n\nThe transmitter and receiver operator both use the `NetworkOpBurstParams` structure as input\nand output to their ports, respectively. `NetworkOpBurstParams` contains the following fields:\n\n- **`data`**: Pointer to batch of packet data\n  - type: `uint8_t *`\n- **`len`**: Length of total buffer in bytes\n  - type: `integer`\n- **`num_pkts`**: Number of packets in batch\n  - type: `integer`\n\nTo receive messages from the Receive operator use the output port `burst_out`.\nTo send messages to the Transmit operator use the input port `burst_in`.",
        "application_name": "basic_network",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "prohawk_video_processing",
            "authors": [
                {
                    "name": "Tim Wooldridge",
                    "affiliation": "Prohawk Technology Group"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Added watermark to the prohawk restoration engine"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.1",
                "tested_versions": [
                    "0.5.1",
                    "0.6.0"
                ]
            },
            "platforms": [
                "arm64"
            ],
            "tags": [
                "Video processing",
                "Prohawk"
            ],
            "ranking": 4,
            "dependencies": {
                "SDK": "Prohawk runtime"
            }
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "prohawk_video_processing",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "Velodyne VLP-16 Lidar Operator",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "nvMap Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "nvMap Embedded Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Tom Birdsong",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Julien Jomier",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Jiahao Yin",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Marlene Wan",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Lidar",
                "Velodyne",
                "Point Cloud",
                "Sensor"
            ],
            "ranking": 4,
            "dependencies": {}
        },
        "readme": "# Velodyne Lidar Operator\n\n## Overview\n\nA Holoscan operator to convert packets from the Velodyne VLP-16 Lidar sensor\nto a point cloud tensor format.\n\n## Description\n\nThis operator receives packets from a Velodyne VLP-16 lidar and\nprocesses them into a point cloud of fixed size in Cartesian space.\n\nThe operator performs the following steps:\n1. Interpret a fixed-size UDP packet as a Velodyne VLP-16 lidar packet,\n   which contains 12 data blocks (azimuths) and 32 spherical data points per block.\n2. Transform the spherical data points into Cartesian coordinates (x, y, z)\n   and add them to the output point cloud tensor, overwriting a previous cloud segment.\n3. Output the point cloud tensor and update the tensor insertion pointer to prepare\n   for the next incoming packet.\n\nWe recommend relying on HoloHub networking operators to receive Velodyne VLP-16 lidar packets\nover UDP/IP and forward them to this operator.\n\n## Requirements\n\nHardware requirements:\n- Holoscan supported platform (x64 or NVIDIA IGX devkit);\n- Velodyne VLP-16 Lidar sensor\n\n## Example Usage\n\nSee the [HoloHub Lidar Sample Application](../../../applications/velodyne_lidar_app/cpp/) to get started.\n\n## Acknowledgements\n\nThis operator was developed in part with support from the NVIDIA nvMap team and adapts portions\nof the NVIDIA DeepMap SDK.\n",
        "application_name": "velodyne_lidar",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "vtk_renderer",
            "authors": [
                {
                    "name": "Kitware Team",
                    "affiliation": "Kitware Inc"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "visualization",
                "tool tracking"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### vtk_renderer operator\n\nThe `vtk_renderer` extension takes the output of the source video player and the\noutput of the `tool_tracking_postprocessor` operator and renders the video\nstream with an overlay annotation of the label using VTK.\n\nVTK can be a useful addition to holohub stack since VTK is a industry leading\nvisualization toolkit. It is important to mention that this renderer operator\nneeds to copy the input from device memory to host due to limitations of VTK.\nWhile this is a strong limitation for VTK we believe that VTK can still be a\ngood addition and VTK is an evolving project. Perhaps in the future we could\novercome this limitation.\n\n#### How to build this operator\n\nBuild the HoloHub container as described at the root [README.md](../../README.md)\n\nYou need to create a docker image which includes VTK with the provided\n`vtk.Dockerfile`:\n\n```bash\ndocker build -t vtk:latest -f vtk.Dockerfile .\n```\n\nThen, you can build the tool tracking application with the provided\n`Dockerfile`:\n\n```bash\n./dev_container launch --img vtk:latest\n```\n\nInside the container you can build the holohub application with:\n\n```bash\n./run build <application> --with vtk_renderer\n```\n\n##### Parameters\n\n- **`videostream`**: Input channel for the videostream, type `gxf::Tensor`\n  - type: `gxf::Handle<gxf::Receiver>`\n- **`annotations`**: Input channel for the annotations, type `gxf::Tensor`\n  - type: `gxf::Handle<gxf::Receiver>`\n- **`window_name`**: Compositor window name.\n  - type: `std::string`\n- **`width`**: width of the renderer window.\n  - type: `int`\n- **`height`**: height of the renderer window.\n  - type: `int`\n- **`labels`**: labels to be displayed on the rendered image.\n  - type: `std::vector<std::string>>`\n",
        "application_name": "vtk_renderer",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrFrameOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial Release",
                "1.0": "Update for Holoscan SDK v2.0.0"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### XrFrame Operator\n\nThe `XrFrameOp` directory contains the `XrBeginFrameOp` and the `XrEndFrameOp`. `XrBeginFrameOp` operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to `XrEndFrameOp` in order to deliver the frame back to the OpenXR runtime. Note that a single connection xr_frame from `XrBeginFrameOp` to `XrEndFrameOp` is required to synchronize the OpenXR calls issued by the two operators.",
        "application_name": "XrFrameOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrEndFrameOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial Release",
                "1.0": "Update for Holoscan SDK v2.0.0"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### XrEndFrame Operator\n\nThe `XrEndFrameOp` operator completes the rendering of a single OpenXR frame by passing populated color and depth buffer for the left and right eye to the OpenXR device. Note that a single connection `xr_frame` from `XrBeginFrameOp` to `XrEndFrameOp` is required to synchronize the OpenXR calls issued by the two operators.\n\n#### `holoscan::openxr::XrEndFrameOp`\n\n##### Parameters \n\n- **`XrSession`**: A class that encapsulates a single OpenXR session\n  - type: `holoscan::openxr::XrSession`\n\n##### Inputs\n \nRender buffers populated by application\n- **`color_buffer`**: color buffer\n  - type: `holoscan::gxf::VideoBuffer`\n- **`depth_buffer`**: depth buffer\n  - type: `holoscan::gxf::VideoBuffer`\n\nOpenXR synchronization\n- **`XrFrame`**: connection to synchronize `XrBeginFrameOp` and `XrEndFrameOp`\n  - type: `XrFrame`\n\n\n\nNote:\n\n- **`XrCudaInteropSwapchain`**: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR",
        "application_name": "end_frame",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrBeginFrameOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "0.0": "Initial Release",
                "1.0": "Update for Holoscan SDK v2.0.0"
            },
            "holoscan_sdk": {
                "minimum_required_version": "2.0.0",
                "tested_versions": [
                    "2.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### XRBeginFrame Operator\n\nThe `XrBeginFrameOp` operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to `XrEndFrameOp` in order to deliver the frame back to the OpenXR runtime. Note that a single arc xr_frame from `XrBeginFrameOp` to `XrEndFrameOp` is required to synchronize the OpenXR calls issued by the two operators.\n\n#### `holoscan::openxr::XrBeginFrameOp`\n\n##### Outputs\n\nOutput for camera state \n- **`left_camera_pose`**: camera pose for the left eye\n  - type: `nvidia::gxf::Pose3D`\n- **`right_camera_pose`**: camera pose for the right eye\n  - type: `nvidia::gxf::Pose3D`\n- **`left_camera_model`**: camera model for the left eye\n  - type: `nvidia::gxf::CameraModel`\n- **`right_camera_model`**: camera model for the right eye\n  - type: `nvidia::gxf::CameraModel`\n- **`depth_range`**: depth range\n  - type: `nvidia::gxf::Vector2f`\n\nOutput for input state \n- **`trigger_click`**: trigger click , values true/false\n  - type: `bool`\n- **`shoulder_click`**: shoulder click , values true/false\n  - type: `bool`\n- **`trackpad_touch`**: trackpad touch , values true/false\n  - type: `bool`\n- **`trackpad`**: trackpad values [x.y]\n  - type: `std::array<float, 2>`\n- **`aim_pose`**: aim pose for the controller specific for the right hand\n  - type: `nvidia::gxf::Pose3D`\n- **`head_pose`**: head pose \n  - type: `nvidia::gxf::Pose3D`\n- **`color_buffer`**: color buffer\n  - type: `holoscan::gxf::Entity`\n- **`depth_buffer`**: depth buffer\n  - type: `holoscan::gxf::Entity`\n\n\n##### Parameters \n\n- **`XrSession`**: A class that encapsulates a single OpenXR session\n  - type: `holoscan::openxr::XrSession`\n \n\nNote:\n\n- **`XrCudaInteropSwapchain`**: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR\n  \n \n",
        "application_name": "begin_frame",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "Convert Depth to Screen Space",
            "authors": [
                {
                    "name": "Magic Leap team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6",
                "tested_versions": [
                    "0.6",
                    "2.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Convert",
                "Depth",
                "Screen"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### Convert Depth To Screen Space Operator\n\nThe `ConvertDepthToScreenSpaceOp` operator remaps the depth buffer from Clara Viz to an OpenXR specific range. The depth buffer is converted in place.\n\n#### `holoscan::openxr::ConvertDepthToScreenSpaceOp`\n\nConverts a depth buffer from linear world units to screen space ([0,1])\n\n##### Inputs\n\n- **`depth_buffer_in`**: input depth buffer to be remapped\n  - type: `holoscan::gxf::VideoBuffer`\n- **`depth_range`**: Allocator used to allocate the volume data\n  - type: `nvidia::gxf::Vector2f`\n\n##### Outputs\n- **`depth_buffer_out`**: output depth buffer \n  - type: `holoscan::gxf::Entity`\n",
        "application_name": "convert_depth",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "tool_tracking_postprocessor",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "visualization",
                "tool tracking"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "\n### Tool tracking postprocessor\n\nThe `tool_tracking_postprocessor` extension provides a codelet that converts inference output of `lstm_tensor_rt_inference` used in the endoscopy tool tracking pipeline to be consumed by the `holoviz` codelet.\n\n#### `nvidia::holoscan::tool_tracking_postprocessor`\n\nTool tracking postprocessor codelet\n\n##### Parameters\n\n- **`in`**: Input channel, type `gxf::Tensor`\n  - type: `gxf::Handle<gxf::Receiver>`\n- **`out`**: Output channel, type `gxf::Tensor`\n  - type: `gxf::Handle<gxf::Transmitter>`\n- **`min_prob`**: Minimum probability, (default: 0.5)\n  - type: `float`\n- **`overlay_img_colors`**: Color of the image overlays, a list of RGB values with components between 0 and 1, (default: 12 qualitative classes color scheme from colorbrewer2)\n  - type: `std::vector<std::vector<float>>`\n- **`host_allocator`**: Output Allocator\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`device_allocator`**: Output Allocator\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`cuda_stream_pool`**: Instance of gxf::CudaStreamPool\n  - type: `gxf::Handle<gxf::CudaStreamPool>`\n",
        "application_name": "tool_tracking_postprocessor",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "volume_loader",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0",
                    "1.0.3",
                    "2.0.0",
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Load",
                "MHD",
                "NIFTI",
                "NRRD"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "# Volume Loader\n\nThe `volume_loader` operator reads 3D volumes from the specified input file.\n\n## Supported Formats\n\nThe operator supports these medical volume file formats:\n* [MHD (MetaImage)](https://itk.org/Wiki/ITK/MetaIO/Documentation)\n  * Detached-header format only (`.mhd` + `.raw`)\n* [NIFTI](https://nifti.nimh.nih.gov/)\n* [NRRD (Nearly Raw Raster Data)](https://teem.sourceforge.net/nrrd/format.html)\n  * [Attached-header format](https://teem.sourceforge.net/nrrd/format.html) (`.nrrd`)\n  * [Detached-header format](https://teem.sourceforge.net/nrrd/format.html#detached) (`.nhdr` + `.raw`)\n\nYou must convert your data to one of these formats to load it with `VolumeLoaderOp`. Some third party open source\ntools for volume file format conversion include:\n- Command Line Tools\n  - the [Insight Toolkit (ITK)](https://itk.org/) ([PyPI](https://pypi.org/project/itk/), [Image IO Examples](https://examples.itk.org/src/io/imagebase/))\n  - [SimpleITK](https://simpleitk.org/) ([PyPI](https://pypi.org/project/SimpleITK/))\n  - [Utah NRRD Utilities (unu)](https://teem.sourceforge.net/unrrdu/)\n- GUI Applications\n  - [3D Slicer](https://www.slicer.org/)\n  - [ImageJ](https://imagej.net/)\n\n## API\n\n#### `holoscan::ops::VolumeLoaderOp`\n\nOperator class to read a volume.\n\n##### Parameters\n\n- **`file_name`**: Volume data file name\n  - type: `std::string`\n- **`allocator`**: Allocator used to allocate the volume data\n  - type: `std::shared_ptr<Allocator>`\n\n##### Outputs\n\n- **`volume`**: Output volume data\n  - type: `nvidia::gxf::Tensor`\n- **`spacing`**: Physical size of each volume element\n  - type: `std::array<float, 3>`\n- **`permute_axis`**: Volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}\n  - type: `std::array<uint32_t, 3>`\n- **`flip_axes`**: Volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}\n  - type: `std::array<bool, 3>`\n- **`extent`**: Physical size of the the volume in world space\n  - type: `std::array<float, 3>`\n",
        "application_name": "volume_loader",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "qt_video",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Qt",
                "QML",
                "QtQuick",
                "Video",
                "UI",
                "Userinterface",
                "Interactive"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Qt Video Operator\n\nThe `qt_video` operator is used to display a video in a [QtQuick](https://doc.qt.io/qt-6/qtquick-index.html) application.\n\nFor more information on how to use this operator in an application see [Qt video replayer example](../../applications/qt_video_replayer/README.md).\n\n#### `holoscan::ops::QtVideoOp`\n\nOperator class.\n\n##### Parameters\n\n- **`QtHoloscanVideo`**: Instance of QtHoloscanVideo to be used\n      - type: `QtHoloscanVideo\n\n##### Inputs\n\n- **`input`**: Input frame data\n  - type: `nvidia::gxf::Tensor` or `nvidia::gxf::VideoBuffer`\n",
        "application_name": "qt_video",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "volume_renderer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "2.0.0",
            "changelog": {
                "1.0": "Initial Release",
                "2.0": "Updates for compatibility with XR operators"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3",
                    "2.0.0",
                    "2.1.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Render",
                "ClaraViz"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "# Volume Renderer\n\nThe `volume_renderer` operator renders a volume using ClaraViz (https://github.com/NVIDIA/clara-viz).\n\n## `holoscan::ops::VolumeRenderer`\n\nOperator class to render a volume.\n\n### Parameters\n\n- **`config_file`**: Config file path. The content of the file is passed to `clara::viz::JsonInterface::SetSettings()` at initialization time. See [Configuration](#configuration) for details.\n  - type: `std::string`\n- **`allocator`**: Allocator used to allocate render buffer outputs when no pre-allocated color or depth buffer is passed to `color_buffer_in` or `depth_buffer_in`. Allocator needs to be capable to allocate device memory.\n  - type: `std::shared_ptr<Allocator>`\n- **`alloc_width`**: Width of the render buffer to allocate when no pre-allocated buffers are provided.\n  - type: `uint32_t`\n- **`alloc_height`**: Height of the render buffer to allocate when no pre-allocated buffers are provided.\n  - type: `uint32_t`\n\n### Inputs\n\nAll inputs are optional.\n\n- **`volume_pose`**: Transform the volume.\n  - type: `nvidia::gxf::Pose3D`\n- **`crop_box`**: Volume crop box. Each `nvidia::gxf::Vector2f` contains the min and max values in range `[0, 1]` of the x, y and z axes of the volume.\n  - type: `std::array<nvidia::gxf::Vector2f, 3>`\n- **`depth_range`**: The distance to the near and far frustum planes.\n  - type: `nvidia::gxf::Vector2f`\n- **`left_camera_pose`**: Camera pose for the left camera when rendering in stereo mode.\n  - type: `nvidia::gxf::Pose3D`\n- **`right_camera_pose`**: Camera pose for the right camera when rendering in stereo mode.\n  - type: `nvidia::gxf::Pose3D`\n- **`left_camera_model`**: Camera model for the left camera when rendering in stereo mode.\n  - type: `nvidia::gxf::CameraModel`\n- **`right_camera_model`**: Camera model for the right camera when rendering in stereo mode.\n  - type: `nvidia::gxf::CameraModel`\n- **`camera_matrix`**: Camera pose when not rendering in stereo mode.\n  - type: `std::array<float, 16>`\n- **`color_buffer_in`**: Buffer to store the rendered color data to, format needs to be 8 bit per component RGBA and buffer needs to be in device memory.\n  - type: `nvidia::gxf::VideoBuffer`\n- **`depth_buffer_in`**: Buffer to store the rendered depth data to, format needs to be 32 bit float single component buffer needs to be in device memory.\n  - type: `nvidia::gxf::VideoBuffer`\n- **`density_volume`**: Density volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.\n  - type: `nvidia::gxf::Tensor`\n- **`density_spacing`**: Physical size of each density volume element.\n  - type: `std::array<float, 3>`\n- **`density_permute_axis`**: Density volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.\n  - type: `std::array<uint32_t, 3>`\n- **`density_flip_axes`**: Density volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.\n  - type: `std::array<bool, 3>`\n- **`mask_volume`**: Mask volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.\n  - type: `nvidia::gxf::Tensor`\n- **`mask_spacing`**: Physical size of each mask volume element.\n  - type: `std::array<float, 3>`\n- **`mask_permute_axis`**: Mask volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.\n  - type: `std::array<uint32_t, 3>`\n- **`mask_flip_axes`**: Mask volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.\n  - type: `std::array<bool, 3>`\n\n### Outputs\n\n- **`color_buffer_out`**: Buffer with rendered color data, format is 8 bit per component RGBA and buffer is in device memory.\n  - type: `nvidia::gxf::VideoBuffer`\n- **`depth_buffer_out`**: Buffer with rendered depth data, format is be 32 bit float single component and buffer is in device memory.\n  - type: `nvidia::gxf::VideoBuffer`\n\n## Configuration\n\nThe renderer accepts a [ClaraViz](https://github.com/NVIDIA/clara-viz) JSON configuration file at startup to control rendering settings, including\n- camera parameters;\n- transfer functions;\n- lighting;\n- and more.\n\nThe ClaraViz JSON configuration file exists in addition to and independent of a Holoscan SDK `.yaml` configuration file that may be passed to an application.\n\nSee the [`volume_rendering_xr` application](../../applications/volume_rendering_xr/configs) for a sample configuration file. Visit the [ClaraViz `render_server.proto` gRPC specification](https://github.com/NVIDIA/clara-viz/blob/main/src/protos/nvidia/claraviz/cinematic/v1/render_server.proto) for insight into configuration file field values.\n\n### Transfer Functions\n\nUsually CT datasets are stored in [Hounsfield scale](https://en.wikipedia.org/wiki/Hounsfield_scale). The renderer maps these values in Hounsfield scale to opacity in order to display the volume. These mappings are called transfer functions. Multiple transfer functions for different input value regions can be defined. Transfer functions also include material properties like diffuse, specular and emissive color. The range of input values the transfer function is applied to is in normalized input range `[0, 1]`.\n\n### Segmentation (Mask) Volume\n\nDifferent organs often have very similar Hounsfield values, therefore additionally an segmentation volume is supported. The segmentation volume contains an integer index for each element of the volume. Transfer functions can be restricted on specific segmentation indices. The segmentation volume can, for example, be generated using [TotalSegmentator](https://github.com/wasserth/TotalSegmentator).\n\n### Creating a Configuration File\n\nConfiguration files are typically specific to a given dataset or modality, and are tailored to a specific voxel intensity range.\nIt may be necessary to create a new configuration file when working with a new dataset in order to produce a meaningful rendering.\n\nThere are two options to create a configuration file for a new dataset:\n- Copy from an existing configuration file as a reference and modify parameters manually. An example configuration file is available in the [`volume_rendering_xr` application config folder](../../applications/volume_rendering_xr/configs/).\n- Use `VolumeRendererOp` to deduce settings for the input dataset. Follow these steps:\n  1. Use the HoloHub [`volume_rendering` app](../../applications/volume_rendering/) or a similar application that will load an input dataset and pass it to `VolumeRendererOp`.\n  2. Configure application settings via a Holoscan SDK YAML file or command line settings to run with the following values:\n    - Set the `VolumeRendererOp` `config_file` parameter to an empty string to indicate no default config file is present;\n    - Set the `VolumeRendererOp` `write_config_file` parameter to the desired output JSON configuration filepath.\n  3. Run the application with the desired input volume. The operator will deduce settings and write out the JSON file to reuse on subsequent runs via the `config_file` parameter.\n",
        "application_name": "volume_renderer",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "CUDA MPS Tutorial",
            "description": "This tutorial describes the steps to enable CUDA MPS and demonstrate few performance benefits of using it.",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "0.1.0",
            "changelog": {
                "0.1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Acceleration",
                "Benchmarking",
                "CUDA",
                "MPS"
            ],
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "CUDA Multi-Process Service (MPS)",
                        "version": ">=vR525",
                        "url": "https://docs.nvidia.com/deploy/mps/index.html"
                    }
                ]
            }
        },
        "readme": "# CUDA MPS Tutorial for Holoscan Applications\n\nCUDA MPS is NVIDIA's [Multi-Process Service](https://docs.nvidia.com/deploy/mps/index.html) for CUDA\napplications. It allows multiple CUDA applications to share a single GPU, which can be useful for\nrunning more than one Holoscan application on a single machine featuring one or more GPUs. This\ntutorial describes the steps to enable CUDA MPS and demonstrate few performance benefits of using it.\n\n## Steps to enable CUDA MPS\n\nBefore enabling CUDA MPS, please [check](https://docs.nvidia.com/deploy/mps/index.html#topic_3_3)\nwhether your system supports CUDA MPS.\n\nCUDA MPS can be enabled by running the `nvidia-cuda-mps-control -d` command and stopped by running \n`echo quit | nvidia-cuda-mps-control` command. More control commands are described \n[here](https://docs.nvidia.com/deploy/mps/index.html#topic_5_1). \n\nCUDA MPS does not require any changes to\nan existing Holoscan application; even an already compiled application binary works as it is.\nTherefore, a Holoscan application can work with CUDA MPS without any \nchanges to its source code or binary.\nHowever, a machine learning model like a TRT engine file may need to be recompiled \nfor the first time after enabling CUDA MPS.\n\nWe have included a helper script in this tutorial `start_mps_daemon.sh` to enable \nCUDA MPS with necessary environment variables.\n\n```bash\n./start_mps_daemon.sh\n```\n\n## Customization\n\nCUDA MPS provides many options to customize resource allocation for MPS clients. For example, it has\nan option to limit the maximum number of GPU threads that can \nbe used by every MPS client. \nThe `CUDA_MPS_ACTIVE_THREAD_PERCENTAGE` environment variable can be used to control this limit\nsystem-wide. This limit can also be configured by communicating the active thread percentage to the control daemon with  \n`echo \"set_default_active_thread_percentage <Thread Percentage>\" | nvidia-cuda-mps-control`.\nOur `start_mps_daemon.sh` script takes this percentage as the first argument as well.\n\n```bash\n./start_mps_daemon.sh <Active Thread Percentage>\n```\n\nFor different applications, one may want to set different limits on the number of GPU threads\navailable to each of them. This can be done by setting the `CUDA_MPS_ACTIVE_THREAD_PERCENTAGE`\nenvironment variable separately for each application. It is elaborated in details [here](https://docs.nvidia.com/deploy/mps/index.html#topic_5_2_5).\n\nThere are other customizations available in CUDA MPS as well. Please refer to the CUDA MPS\n[documentation](https://docs.nvidia.com/deploy/mps/index.html#topic_5_1_1) to know more about them.\nPlease note that concurrently running Holoscan applications may increase the GPU device memory\nfootprint. Therefore, one needs to be careful about hitting the GPU memory size and [potential\ndelay due to page faults](https://developer.nvidia.com/blog/improving-gpu-memory-oversubscription-performance/).\n\n## Performance Benefits\n\nCUDA MPS improves the performance for concurrently running Holoscan applications. \nSince multiple applications can simultaneously execute more than one CUDA compute tasks with CUDA\nMPS, it can also improve the overall GPU utilization.\n\nSuppose, we want to run the endoscopy tool tracking and ultrasound segmentation applications\nconcurrently on an x86 workstation with RTX A6000 GPU. The below table shows the maximum end-to-end latency performance\nwithout and with CUDA MPS, where the active thread percentage is set to 40\\% for each application.\nIt demonstrates 18% and 50% improvement in the maximum end-to-end latency for the\nendoscopy tool tracking and ultrasound segmentation applications, respectively.\n\n| Application | Without MPS (ms) | With MPS (ms) |\n| ----------- | ---------------- | ------------- |\n| Endoscopy Tool Tracking | 115.38 | 94.20 |\n| Ultrasound Segmentation | 121.48 | 60.94 |\n\nIn another set of experiments, we concurrently run multiple instances of the endoscopy tool tracking\napplication in different processes. We set the active thread percentage to be 20\\% for each MPS client. The below graph shows the maximum end-to-end latency with and\nwithout CUDA MPS. The experiment demonstrates up to 36% improvement with CUDA MPS.\n\n![Alt text](image.png)\n\nSuch experiments can easily be conducted with [Holoscan Flow Benchmarking](../../benchmarks/holoscan_flow_benchmarking) to retrieve\nvarious end-to-end latency performance metrics.",
        "application_name": "cuda_mps",
        "source_folder": "tutorials"
    },
    {
        "metadata": {
            "name": "Creating Multi-Node Holoscan Applications",
            "description": "In this tutorial, we will walk through the process in two scenarios of creating a multi node application from existing applications.",
            "authors": [
                {
                    "name": "Jin Li",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "0.1.0",
            "changelog": {
                "0.1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Distributed",
                "Fragment"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "# Creating Multi Node Applications\n\nIn this tutorial, we will walk through the process in two scenarios of creating a multi node application from existing applications. We will demonstrate the process with Python applications but it's similar for C++ applications as well.\n\n1. When we would want to divide an application that was previously running on a single node into two fragments running on two nodes. This corresponds to use cases where we want to separate the compute and visualization workloads onto two different nodes, for example in the case of surgical robotics, the visualization node should be closest to the surgeon. For this purpose we choose the example of the [`multiai_endoscopy`](../../applications/multiai_endoscopy/) application.\n\n2. When we would want to connect and combine two previously independent applications into one application with two fragments. This corresponds to the use cases where we want to run time-critical task(s) on a node closest to the data stream, and non time-critical task(s) on another node that can have a bit more latency, for example saving the inbody video recording of the surgery to cloud can have a higher latency than the real-time visualization. For this purpose we choose the example of the [`endoscopy_tool_tracking`](../../applications/endoscopy_tool_tracking/) application and the [`endoscopy_out_of_body_detection`](../../applications/endoscopy_out_of_body_detection/) application.\n\n\nThe SDK documentation on [Creating a Distributed Application](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_create_distributed_app.html) contains the necessary core concepts and description for distributed applications, please familiarize yourself with the documentation before proceeding to this tutorial.\n\nPlease also see two SDK examples [ping_distributed](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/ping_distributed) and [video_replayer_distributed](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/video_replayer_distributed) on simple examples of creating distributed applications.\n\nIn this tutorial, we will focus on modifying existing applications you have created into distributed applications.\n## Scenario 1 - Divide an application into two fragments\nThe [`multiai_endoscopy`](../../applications/multiai_endoscopy/) application has its app graph like below:\n\n![multiai_endoscopy_app_graph](./images/multiai_endoscopy_app_graph.png)\n\nWe will divide it into two fragments. The first fragment will include all operators excluding the visualizer and the second fragment will include the visualizer, as illustrated below:\n\n![multiai_endoscopy_distributed_app_graph](./images/multiai_endoscopy_distributed_app_graph.png)\n\n### Changes in scenario 1 - Extra Imports\nTo created a distributed application, we will need to import the Fragment object.  \n\n```python\nfrom holoscan.core import Fragment\n```\n\n\n### Changes in scenario 1 - Changing the way command-line arguments are parsed\nAs seen in the [documentation](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_create_distributed_app.html#adding-user-defined-command-line-arguments), it is recommended to parse user-defined arguments from the `argv ((C++/Python))` method/property of the application. To parse in user-defined command line arguments (such as `--data`, `--source`, `--labelfile` in this app), let's make sure to avoid arguments that are unique to the multi-fragment applications, such as  `--driver`, `--worker`, `--address`, `--worker-address`, `--fragments` (see the [documentation](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_create_distributed_app.html#adding-user-defined-command-line-arguments) for more details on using those arguments).\n<br>\nIn the [non-distributed application](../../applications/multiai_endoscopy/), we would have \n```python\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Multi-AI Detection Segmentation application.\")\n    parser.add_argument(...) # for the app config yaml file via --config \n    parser.add_argument(...) # for args needed in app init --source --data --labelfile\n    args = parser.parse_args()\n    # logic to define args (config_file, labelfile) needed to pass into application init and config\n    app = MultiAIDetectionSegmentation(source=args.source, data=args.data, labelfile=labelfile)\n    app.config(config_file)\n    app.run()\n```\n\nIn the distributed application, we need to make the following changes mainly to `parser.parse_args`:\n```python !5,6\nif __name__ == \"__main__\":\n    parser = ArgumentParser(description=\"Multi-AI Detection Segmentation application.\")\n    parser.add_argument(...) # for the app config yaml file via --config \n    parser.add_argument(...) # for args needed in app init --source --data --labelfile\n    apps_argv = Application().argv # difference\n    args = parser.parse_args(apps_argv[1:]) # difference\n    # logic to define args (config_file, labelfile) needed to pass into application init and config\n    app = MultiAIDetectionSegmentation(source=args.source, data=args.data, labelfile=labelfile)\n    app.config(config_file)\n    app.run()\n```\n### Changes in scenario 1 - Changing the application structure\nPreviously, we defined our non distributed applications with the `__init__()` and `compose()` methods. \n```python\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        ...\n\n    def compose(self):\n        # define operators and add flow\n        ...\n```\nNow we will define two fragments, and add and connect them in the application's `compose()` method:\n```python\nclass Fragment1(Fragment):\n    # operators in fragment1 need the objects: sample_data_path, source, label_dict \n    def __init__(self, app, name, sample_data_path, source, label_dict):\n        super().__init__(app, name)\n        ...\n    def compose(self):\n        # define fragment1 operators\n        # add flow\n        ... \n\nclass Fragment2(Fragment):\n    # operators in fragment2 need the object: label_dict \n    def __init__(self, app, name, label_dict):\n        super().__init__(app, name)\n        ...\n    def compose(self):\n        # define the one operator in fragment2 (Holoviz)\n        # add operator\n        # no need to add_flow because there's only one operator in this fragment\n        ... \n\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        super().__init__()\n        # set self.name\n        # get self.label_dict from labelfile,self.source, self.sample_data_path\n        ...\n        \n    def compose(self):\n        # define the two fragments in this app: fragment1, fragment2\n        # pass in the objects needed to each fragment's operators when defining each fragment\n        # operators in fragment1 need the objects: sample_data_path, source, label_dict \n        fragment1 = Fragment1(self, name=\"fragment1\", source=self.source, sample_data_path=self.sample_data_path, label_dict=self.label_dict)\n        # operators in fragment2 need the object: label_dict \n        fragment2 = Fragment2(self, name=\"fragment2\", label_dict=self.label_dict)\n\n        # Connect the two fragments \n        # There are three connections between fragment1 and fragment2:\n        # (1) from the data source to Holoviz\n        source_output = self.source + \".video_buffer_output\" if self.source.lower() == \"aja\" else self.source + \".output\"\n        self.add_flow(fragment1, fragment2, {(source_output, \"holoviz.receivers\")})\n        # (2) from the detection postprocessor to Holoviz\n        self.add_flow(fragment1, fragment2, {(\"detection_postprocessor.out\" , \"holoviz.receivers\")})\n        # (3) from the segmentation postprocessor to Holoviz\n        self.add_flow(fragment1, fragment2, {(\"segmentation_postprocessor.out_tensor\" , \"holoviz.receivers\")})\n```\n### Changes in scenario 1 - Defining objects shared among fragments\nWhen creating your fragments, first make a list of all the objects each fragment's operators will need. If there are objects that are needed across multiple fragments (such as `self.label_dict` in this case), before passing them into fragments in the application's `compose()` method, create such objects in the app's `__init()__` method ideally. In the non-distributed application, in the app's `compose()` method we define `label_dict` from `self.labelfile`, and continue using `label_dict` while composing the application. In the distributed application, we move the definition of `label_dict` from `self.labelfile` into the application's `__init()__` method, and refer to `self.label_dict` in the application's `compose()` method and each fragment's `__init__()`/`compose()` methods.\n\nNon distributed application:\n```python\nclass MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        # construct the labels dictionary if the commandline arg for labelfile isn't empty\n        label_dict = self.get_label_dict(self.labelfile)\n```\n\nDistributed application:\n```python\nclass Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, label_dict):\n        super().__init__(app, name)\n\n        self.source = source\n        self.label_dict = label_dict\n        self.sample_data_path = sample_data_path\n\n    def compose(self):\n        ...\n        # Use self.label_dict\n\nclass Fragment2(Fragment):\n    def __init__(self, app, name, label_dict):\n        super().__init__(app, name)\n\n        self.label_dict = label_dict\n        \n    def compose(self):\n        ...\n        # Use self.label_dict\n\nclass MultiAIDetectionSegmentation(Application):\n    def __init__(self, data, source=\"replayer\", labelfile=\"\"):\n        ...\n        self.label_dict = self.get_label_dict(labelfile)\n    def compose(self):\n        fragment1 = Fragment1(self, name=\"fragment1\", source=self.source, sample_data_path=self.sample_data_path, label_dict=self.label_dict)\n        fragment2 = Fragment2(self, name=\"fragment2\", label_dict=self.label_dict)\n        ...\n```\n\n### Changes in scenario 1 - Adding Operators to App Graph\nWhen composing a non-distributed application, operators are created in the `compose()` method, then added to the app graph one of two ways:\n   1. For applications with a single operator (rare), `add_operator()` should be called.\n   1. for applications with multiple operators, using `add_flow()` will take care of adding each operator to the app graph on top of connecting them.\nThis applies to distributed applications as well: when composing multiple fragments, each of them are responsible for adding all their operators to the app graph in their `compose()` method. Calling `add_flow()` in the `compose()` method of the distributed application when connecting fragments together does not add the operators to the app graph. This is often relevant when breaking down a single fragment application in a multi fragments application for distributed use cases, as some fragments might end up owning a single operator, and the absence of `add_flow()` in that fragment should come with the addition of `add_operator()` instead.\n\nIn the non distributed application:\n```python\nclass MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        # define operators\n        source = SourceClass(...)\n        detection_preprocessor = FormatConverterOp(...)\n        segmentation_preprocessor = FormatConverterOp(...)\n        multi_ai_inference = InferenceOp(...)\n        detection_postprocessor = DetectionPostprocessorOp(...)\n        segmentation_postprocessor = SegmentationPostprocessorOp(...)\n        holoviz = HolovizOp(...)\n\n        # add flow between operators\n        ...\n\n```\n\nIn a distributed application:\n```python #11-17\nclass Fragment1(Fragment):\n    def compose(self):\n        # define operators\n        source = SourceClass(...)\n        detection_preprocessor = FormatConverterOp(...)\n        segmentation_preprocessor = FormatConverterOp(...)\n        multi_ai_inference = InferenceOp(...)\n        detection_postprocessor = DetectionPostprocessorOp(...)\n        segmentation_postprocessor = SegmentationPostprocessorOp(...)\n\n        # add flow between operators\n        ...\n        \n\nclass Fragment2(Fragment):\n    def compose(self):\n        holoviz = HolovizOp(...)\n        self.add_operator(holoviz)\n```\n\n### Changes in scenario 1 - Shared resources\nIn a non distributed application, there may be some shared resources defined in the application's `compose()` method, such as a `UnboundedAllocator` for various operators. When splitting the application into multiple fragments, remember to create those resources once for each fragment.\n\nNon distributed application:\n```python\nclass MultiAIDetectionSegmentation(Application):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n```\n\nDistributed application:\n```python\nclass Fragment1(Fragment):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n        # ... operator definitions in fragment1 with parameter pool=pool,\n\nclass Fragment2(Fragment):\n    def compose(self):\n        pool = UnboundedAllocator(self, name=\"pool\")\n        # ... operator definitions in fragment2 with parameter pool=pool,\n```\n\n### Changes in scenario 1 - Running the application\nPreviously for a non distributed application, the command to launch is `python3 multi_ai.py --data <DATA_DIR>`, now in the distributed application we will have the option to specify a few more things:\n```sh\n# To run fragment 1 on current node as driver and worker:\npython3 multi_ai.py --data /workspace/holohub/data/ --driver --worker --address <node 1 IP address>:<port> --fragments fragment1\n# To run fragment 2 on current node as worker:\npython3 multi_ai.py --data /workspace/holohub/data/  --worker --address <node 1 IP address>:<port> --fragments fragment2\n\n```\n\nFor more details on the commandline arguments for multi fragment apps, see the [documentation](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_create_distributed_app.html).\n\nTo run the app we create in scenario 1, please see [Running the Applications](#running-the-applications).\n\n## Scenario 2 - Connect two applications into a multi-node application with two fragments\nIn this scenario, we will combine the existing application [endoscopy_tool_tracking](../../applications/endoscopy_tool_tracking/python/) and [endoscopy_out_of_body_detection](../../applications/endoscopy_out_of_body_detection/) into a distributed application with 2 fragments.\n\nSince [endoscopy_out_of_body_detection](../../applications/endoscopy_out_of_body_detection/) is implemented in C++, we will quickly implement the Python version of the app for our Fragment2.\n\nThe app graph for `endoscopy_tool_tracking`:\n![](./images/endoscopy_tool_tracking_app_graph.png)\nThe app graph for `endoscopy_out_of_body_detection`:\n![](./images/endoscopy_out_of_body_app_graph.png)\nThe distributed app graph we want to create:\n![](./images/scenario2_distributed_app_graph.png)\n\n\n### Changes in scenario 2  - Extra Imports\nSimilar to scenario 1, to created a distributed application, we will need to import the Fragment object. When combining two non distributed apps into a multi-fragment application, remember to import all prebuilt Holoscan operators needed in both fragments.\n\n```python\nfrom holoscan.core import Fragment\nfrom holoscan.operators import (\n    AJASourceOp,\n    FormatConverterOp,\n    HolovizOp,\n    VideoStreamRecorderOp,\n    VideoStreamReplayerOp,\n    InferenceOp,\n    InferenceProcessorOp\n)\n```\n\n### Changes in scenario 2 - Changing the way command-line arguments are parsed\n\nSimilar to [Changing the way command-line arguments are parse in scenario 1](#changes-in-scenario-1---changing-the-way-command-line-arguments-are-parsed), instead of the following for the non distributed app:\n```python\nif __name__ == \"__main__\":\n    ...\n    args = parser.parse_args()\n    ...\n```\nFor the distributed app we will have:\n```python\nif __name__ == \"__main__\":\n    ...\n    apps_argv = Application().argv\n    args = parser.parse_args(apps_argv[1:])\n    ...\n```\n\n### Changes in scenario 2 - Modifying non-distributed application(s) into a distributed application\n\nIn the new distributed app, we will define the app graph in `endoscopy_tool_tracking` as the new app's fragment 1. \nThe non distributed app `endoscopy_tool_tracking` had its `__init__()` and `compose()` methods structured like following:\n```python\nclass EndoscopyApp(Application):\n    def __init__(self, data, record_type=None, source=\"replayer\"):\n        super().__init__()\n        # set self.name\n        ...\n        # get parameters for app graph composition: \n        # self.record_type, self.source, self.sample_data_path \n        ...\n    def compose(self):\n        # Create operators including:\n        # source operator (video replayer, AJA, Yuan), \n        # format converters, recorder\n        # LSTMTensorRTInferenceOp, ToolTrackingPostprocessorOp\n        # HolovizOp\n        ...\n        \n        # Add flow between operators\n        ...\n```\n\n\nWe can define our fragment1 modified from `endoscopy_tool_tracking` app with the following structure.\n\n```python\nclass Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.sample_data_path = sample_data_path\n        self.record_type = record_type\n    def compose(self):\n        # Create operators including:\n        # source operator (video replayer, AJA, Yuan), \n        # format converters, recorder\n        # LSTMTensorRTInferenceOp, ToolTrackingPostprocessorOp\n        # HolovizOp\n        ...\n        \n        # Add flow between operators\n        ...\n```\n\nWe will define the app graph in `endoscopy_out_of_body_detection` as the new app's fragment 2.\n```python\nclass Fragment2(Fragment):\n    def __init__(self, app, name, source, model_path, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.record_type = record_type\n        self.model_path = model_path\n    def compose(self):\n        \n        is_aja = self.source.lower() == \"aja\"\n        \n        pool = UnboundedAllocator(self, name=\"fragment2_pool\")\n        in_dtype = \"rgba8888\" if is_aja else \"rgb888\"\n       \n        out_of_body_preprocessor = FormatConverterOp(\n            self,\n            name=\"out_of_body_preprocessor\",\n            pool=pool,\n            in_dtype=in_dtype,\n            **self.kwargs(\"out_of_body_preprocessor\"),\n        )\n\n        model_path_map = {\"out_of_body\": os.path.join(self.model_path, \"out_of_body_detection.onnx\")}\n        for k, v in model_path_map.items():\n            if not os.path.exists(v):\n                raise RuntimeError(f\"Could not find model file: {v}\")\n        inference_kwargs = self.kwargs(\"out_of_body_inference\")\n        inference_kwargs[\"model_path_map\"] = model_path_map\n        out_of_body_inference = InferenceOp(\n            self,\n            name=\"out_of_body_inference\",\n            allocator=pool,\n            **inference_kwargs,\n        )\n        out_of_body_postprocessor = InferenceProcessorOp(\n            self,\n            name=\"out_of_body_postprocessor\",\n            allocator=pool,\n            disable_transmitter=True,\n            **self.kwargs(\"out_of_body_postprocessor\")\n        )\n        \n        # add flow between operators\n        self.add_flow(out_of_body_preprocessor, out_of_body_inference, {(\"\", \"receivers\")})\n        self.add_flow(out_of_body_inference, out_of_body_postprocessor, {(\"transmitter\", \"receivers\")})\n```\nWe can then define our distributed application with the following structure. Notice how the objects `self.record_type`, `self.source`, and `self.sample_data_path` are passed into each fragment.\n\n```python\nclass EndoscopyDistributedApp(Application):\n    def __init__(self, data, record_type=None, source=\"replayer\"):\n        super().__init__()\n        # set self.name\n        ...\n        # get parameters for app graph composition: \n        # self.record_type, self.source, self.sample_data_path \n        ...\n    def compose(self):\n        is_aja = self.source.lower() == \"aja\"\n        \n        fragment1 = Fragment1(self, \n                            name=\"fragment1\", \n                            source = self.source, \n                            sample_data_path=os.path.join(self.sample_data_path, \"endoscopy\"), \n                            record_type=self.record_type)\n        fragment2 = Fragment2(self, \n                            name=\"fragment2\", \n                            source = self.source, \n                            model_path=os.path.join(self.sample_data_path, \"endoscopy_out_of_body_detection\"),\n                            record_type = self.record_type)\n        \n        self.add_flow(fragment1, fragment2, {(\"aja.video_buffer_output\" if is_aja else \"replayer.output\", \"out_of_body_preprocessor\")})\n```\n### Changes in scenario 2 - Combining objects needed by each fragments in distributed app init time\nNote how the `__init__()` method in the distributed app structured above is now the place to get parameters for the app graph composition for both fragment1 and fragment 2. In this case, Fragment1 needs the following objects at `__init__()` time: \n```python\nclass Fragment1(Fragment):\n    def __init__(self, app, name, sample_data_path, source, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.sample_data_path = sample_data_path\n        self.record_type = record_type\n```\nand Fragment 2 needs the following objects at `__init__()` time:\n```python\nclass Fragment2(Fragment):\n    def __init__(self, app, name, source, model_path, record_type):\n        super().__init__(app, name)\n\n        self.source = source\n        self.record_type = record_type\n        self.model_path = model_path\n```\nWe need to make sure in the distributed app's `__init__()` method, we are creating the corresponding objects to pass in to each fragment's `__init__()` time.\n\n\n### Changes in scenario 2 - Configuration File\nIf you had two `yaml` files for configuring each of the non distributed applications, now in the combined distributed application you will need to combine the content of both in a `yaml` file.\n\n### Changes in scenario 2 - Running the application\nIn the newly created distributed application we will launch the application like below:\n```sh\n# To run fragment 1 on current node as driver and worker:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1 --address <node 1 IP address>:<port>\n\n# To run fragment 2 on current node as worker:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --data /workspace/holohub/data --source replayer --worker --fragments fragment2 --address <node 1 IP address>:<port>\n\n# To run on a single node:\npython3 /workspace/holohub/applications/distributed_app/python/endoscopy_tool_tracking.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1,fragment2 \n```\n\nFor more details on the commandline arguments for multi fragment apps, see the [documentation](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_create_distributed_app.html).\n\n\nTo run the app we create in scenario 2, please see [Running the Applications](#running-the-applications).\n\n## Configuration\nYou can run a distributed application across any combination of hardware that are compatible with the Holoscan SDK. Please see [SDK Installation Prerequisites](https://docs.nvidia.com/holoscan/sdk-user-guide/sdk_installation.html#prerequisites) for a list of compatible hardware. \n\nIf you would like the distributed application fragments to communicate through high speed ConnectX NICs, enable the ConnectX NIC for GPU Direct RDMA, for example the ConnectX-7 NIC on the IGX Orin Developer Kit, follow [these instructions](https://docs.nvidia.com/holoscan/sdk-user-guide/set_up_gpudirect_rdma.html). Enabling ConnectX NICs could bring significant speedup to your distributed app connection, for example, the RJ45 ports on IGX Orin Developer Kit supports 1GbE while the QSFP28 ports connected to ConnectX-7 support up to 100 GbE.\n\nIf connecting together two IGX Orin Developer Kits with dGPU, follow the above instructions on each devkit to enable GPU Direct RDMA through ConnectX-7, and make the hardware connection through the QSFP28 ports on the back panel. A QSFP-QSFP cable should be included in your devkit box alongside power cables etc in the inner small box. Use either one of the two port on the devkit, and make sure to find out the logical name of the port as detailed in the instructions.\n\n![](./images/Two%20IGX%20Connection.png)\n\nOn each machine, make sure to specify an address for the network logical name that has the QSFP cable connected, and when running your distributed application, make sure to specify that address for the `--driver` machine.\n\n## Running the Applications\nFollow [Container Build](https://github.com/nvidia-holoscan/holohub?tab=readme-ov-file#container-build-recommended) instructions to build and launch the HoloHub dev container.\n\n\n\n### Scenario 1 Application\nBefore we start to launch the application, let's first run the original application. In the dev container, make sure to build and launch the [`multiai_endoscopy`](../../applications/multiai_endoscopy/) app, this will convert the downloaded ONNX model file into a TensorRT engine at first run. \n\n> Build and run instructions may change in the future, please refer to the original application.\n```sh\n# on the node that runs fragment 1, or a node that runs the entire app\n./run build multiai_endoscopy\n./run launch multiai_endoscopy python\n```\nNow we're ready to launch the distributed application in scenario 1.\n```sh\ncd scenario1/\n```\nTo launch the two fragments on separate nodes, launch fragment 1 on node 1 as the driver node with:\n```sh\n# with replayer as source:\npython3 multi_ai.py --source replayer --data /workspace/holohub/data/ --driver --worker --address <node 1 IP address>:<port number> --fragments fragment1\n# with AJA video capture card as source:\npython3 multi_ai.py --source aja  --data /workspace/holohub/data/ --driver --worker --address <node 1 IP address>:<port number> --fragments fragment1\n```\nand launch fragment 2 with:\n```sh\npython3 multi_ai.py --worker --address <node 1 IP address>:<port number> --fragments fragment2\n```\nTo launch the two fragments together on a single node, simply launch the application without specifying the additional parameters:\n```sh\n# with replayer as source:\npython3 multi_ai.py --source replayer --data /workspace/holohub/data/ \n# with AJA video capture card as source:\npython3 multi_ai.py --source aja  --data /workspace/holohub/data/ \n```\n\n\n### Scenario 2 Application\nLet's first run the original applications. In the dev container, make sure to build and launch the [`endoscopy_tool_tracking`](../../applications/endoscopy_tool_tracking/) and [`endoscopy_out_of_body_detection`](../../applications/endoscopy_out_of_body_detection/) apps, this will convert the downloaded ONNX models into TensorRT engines at first run. \n\n> Build and run instructions may change in the future, please refer to the original applications.\n```sh\n# On the node that runs fragment 1 or a node that runs the entire app\n./run build endoscopy_tool_tracking\n./run launch endoscopy_tool_tracking python\n\n# On the node that runs fragment 2 or a node that runs the entire app\n./run build endoscopy_out_of_body_detection\ncd build && applications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection --data ../data/endoscopy_out_of_body_detection\n```\nNow we're ready to launch the distributed application in scenario 2. \n```sh\n# don't forget to do this on both machines\n# configure your PYTHONPATH environment variable \nexport PYTHONPATH=/opt/nvidia/holoscan/lib/cmake/holoscan/../../../python/lib:/workspace/holohub/build/python/lib\n# run in the build directory of Holohub in order to load extensions\ncd /workspace/holohub/build\n```\n\nTo launch the two fragments on separate nodes, launch fragment 1 on node 1 as the driver node with:\n```sh\n# with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1 --address <node 1 IP address>:<port number>\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source aja --driver --worker --fragments fragment1 --address <node 1 IP address>:<port number>\n```\nand launch fragment 2 with:\n```sh\n# with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py  --worker --fragments fragment2 --address <node 1 IP address>:<port number>\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py  --source aja --worker --fragments fragment2 --address <node 1 IP address>:<port number>\n```\nTo launch the two fragments together on a single node, simply launch the application without specifying the additional parameters:\n```sh\n# with replayer as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source replayer --data /workspace/holohub/data --driver --worker --fragments fragment1,fragment2\n# with AJA video capture card as source:\npython3 /workspace/holohub/tutorials/creating-multi-node-applications/scenario2/endoscopy_distributed_app.py --source aja --driver --worker --fragments fragment1,fragment2 \n```",
        "application_name": "creating-multi-node-applications",
        "source_folder": "tutorials"
    },
    {
        "metadata": {
            "name": "Self-Supervised Contrastive Learning for Surgical videos",
            "description": "The focus of this repo is to walkthrough the process of doing Self-Supervised Learning using Contrastive Pre-training on Surgical Video data.",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "0.1.0",
            "changelog": {
                "0.1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Computer Vision",
                "Learning",
                "Medical",
                "Self-Supervised",
                "Surgical",
                "Video"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Cholec80",
                        "url": "http://camma.u-strasbg.fr/datasets"
                    }
                ],
                "libraries": [
                    {
                        "name": "monai",
                        "version": "1.0.1"
                    },
                    {
                        "name": "ipython"
                    },
                    {
                        "name": "pytorch-lightning",
                        "version": "1.4"
                    },
                    {
                        "name": "torchmetrics",
                        "version": "0.6"
                    }
                ]
            }
        },
        "readme": "# Self-Supervised Contrastive Learning for Surgical videos\nThe focus of this repo is to walkthrough the process of doing Self-Supervised Learning using Contrastive Pre-training on Surgical Video data. \nAs part of the walk-through we will guide through the steps needed to pre-process and extract the frames from the public *Cholec80 Dataset*. This will be required to run the tutorial.\n\n\nThe repo is organized as follows - \n* `Contrastive_learning_Notebook.ipynb` walks through the process of SSL in a tutorial style\n* `train_simclr_multiGPU.py` enables running of \"pre-training\" on surgical data across multiple GPUs through the CLI\n* `downstream_task_tool_segmentation.py` shows the process of \"fine-tuning\" for a downstream task starting from a pretrained checkpoint using [MONAI](https://github.com/Project-MONAI/MONAI)\n\n\n## Dataset\nTo run through the full tutorial, it is required that the user downloads [Cholec80](http://camma.u-strasbg.fr/datasets) dataset. Additional preprocessing of the videos to extract individual frames can be performed using the python helper file as follows:\n\n`python extract_frames.py --datadir <path_to_cholec80_dataset>` \n\n### Adapt to your own dataset\nTo run this with your own dataset, you will need to extract the frames and modify the `Pytorch Dataset/Dataloader` accordingly. For SSL pre-training, a really simple CSV file formatted as follows can be used. \n```\n<path_to_frame>,<label>\n```\nwhere `<label>` can be a class/score for a downstream task, and is NOT used during pre-training.\n\n```\n# Snippet of csv file\n/workspace/data/cholec80/frames/train/video01/1.jpg,0\n/workspace/data/cholec80/frames/train/video01/2.jpg,0\n/workspace/data/cholec80/frames/train/video01/3.jpg,0\n/workspace/data/cholec80/frames/train/video01/4.jpg,0\n/workspace/data/cholec80/frames/train/video01/5.jpg,0\n....\n```\n\n## Environment\nAll environment/dependencies are captured in the [Dockerfile](Docker/Dockerfile). The exact software within the base container are [described here](https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.html).\n\n### Create Docker Image/Container\n\n```bash\nDATA_DIR=\"/mnt/sdb/data\"  # location of Cholec80 dataset\ndocker build -t surg_video_ssl_2202:latest Docker/\n\n# sample Docker command (may need to update based on local setup)\ndocker run -it --gpus=\"device=1\" \\\n    --name=SURGSSL_EXPS \\\n    -v $DATA_DIR:/workspace/data \\\n    -v `pwd`:/workspace/codes -w=/workspace/codes/ \\\n    -p 8888:8888 \\\n    --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \\\n    surg_video_ssl_2202 jupyter lab\n```\n\nFor environment dependencies refer to the [Dockerfile](Docker/Dockerfile)\n\n## Launch Training\n\nPRE-TRAINING\n```bash\n# Training on single GPU with `efficientnet_b0` backbone\npython3 train_simclr_multigpu.py --gpus 1 --backbone efficientnet_b0 --batch_size 64\n\n# Training on single GPU with `resnet50` backbone\npython3 train_simclr_multigpu.py --gpus 4 --backbone resnet50 --batch_size 128\n```\n\nDOWNSTREAM TASK - Segmentation\nThis script shows an example of taking the checkpoint above and integrating it into [MONAI](https://monai.io/). \n\n```bash\n# Fine-Tuning on \"GPU 1\" with 10% of the dataset, while freezing the encoder\npython3 downstream_task_tool_segmentation.py --gpu 1 --perc 10 --exp simclr --freeze\n```\n\n## Model/Checkpoints information\n\nAs part of this tutorial, we are also releasing a few different checkpoints for users. These are detailed below. \n\n> **NOTE** : These checkpoints were trained using an internal dataset of Chelecystectomy videos provided by [Activ Surgical](https://www.activsurgical.com/) and NOT the Cholec80 dataset. \n\n### [Pre-Trained Backbones](https://drive.google.com/drive/folders/1NIfjydQ-o6Jl-DSAvTy5obw5XQ89Xje0?usp=share_link)\n* ResNet18        - [link](https://drive.google.com/file/d/17w_LEI36JrHcUf5fGEufpyZaIk3Fp1Co/view?usp=sharing)\n* ResNet50        - [link](https://drive.google.com/file/d/1fK87Nxit5bokYuMCbMG1cEHmEZUGBDlA/view?usp=share_link)\n* efficientnet_b0 - [link](https://drive.google.com/file/d/1rgolweQ5HU6Kvf93jqLkaLVKE8DD_Aco/view?usp=sharing)\n\n### Tool Segmentation Model\n* MONAI - FlexibleUNet (efficientnet_b0) - [link](https://drive.google.com/file/d/1HLyccYY0AtZy8Sr1ty-gid-Fee4DVjWM/view?usp=share_link)\n\n## Holoscan SDK\nThis tool Segmentation Model can be used to build a Holoscan App, using the process under section \"Bring your own Model\" within the [Holoscan SDK User guide](https://developer.download.nvidia.com/assets/Clara/ClaraHoloscan-1.pdf?t=eyJscyI6InJlZiIsImxzZCI6IlJFRi1jb3Vyc2VzLm52aWRpYS5jb20vIiwibmNpZCI6InNvLW52c2gtODA1ODY2LXZ0MTIifQ==).\n\n\n## Resources\n\n[1] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020).\nA simple framework for contrastive learning of visual representations.\nIn International conference on machine learning (pp.\n1597-1607).\nPMLR.\n([link](https://arxiv.org/abs/2002.05709))\n\n[2] Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. (2020).\nBig self-supervised models are strong semi-supervised learners.\nNeurIPS 2021 ([link](https://arxiv.org/abs/2006.10029)).\n\n[3] [Pytorch Lightning SSL Tutorial](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/13-contrastive-learning.html) | [Github](https://www.github.com/PytorchLightning/pytorch-lightning/)\n\n[4] Ramesh, S., Srivastav, V., Alapatt, D., Yu, T., Muarli, A., et. al. (2023).   \nDissecting Self-Supervised Learning Methods for Surgical Computer Vision.\narXiv preprint arXiv:2207.00449.\n([link](https://arxiv.org/abs/2207.00449))\n",
        "application_name": "self_supervised_training",
        "source_folder": "tutorials"
    },
    {
        "metadata": {
            "name": "DICOM to OpenUSD mesh segmentation with MONAI Deploy and Holoscan",
            "authors": [
                {
                    "name": "Rahul Choudhury",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Andreas Heumann",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Cristiana Dinea",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Gregory Lee",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Jeroen Stinstra",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Ming Melvin Qi",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Tom Birdsong",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Wendell Hom",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DICOM",
                "mesh",
                "MONAI",
                "MONAI Deploy",
                "Omniverse",
                "OpenUSD",
                "STL"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "MONAI Deploy",
                        "version": "0.6.0"
                    },
                    {
                        "name": "highdicom",
                        "version": "0.22.0"
                    }
                ]
            },
            "run": {
                "command": "python tutorial.py",
                "workdir": "tutorials/dicom_to_usd_with_monai_and_holoscan"
            }
        },
        "readme": "# Processing DICOM to USD with MONAI Deploy and Holoscan\n\n![USD Composer Liver Segmentation Mesh](./doc/spleen-seg.png)\n\nIn this tutorial we demonstrate a method leveraging a combined MONAI Deploy and Holoscan pipeline to process DICOM input data and write a resulting mesh to disk in the OpenUSD file format.\n\n## Demonstrated Technologies\n\n### DICOM\n\nThe [Digital Imaging and Communications in Medicine (DICOM) standard](https://www.dicomstandard.org/) is a comprehensive standard for medical imaging. DICOM covers a wide variety of medical imaging modalities and defines standards for both image storage and communications in medicine. DICOM data often represent 2D or 3D volumes or series of volumes.\n\n### OpenUSD\n\n[Universal Scene Description (OpenUSD)](https://openusd.org/release/index.html) is an extensible ecosystem of file formats, compositors, renderers, and other plugins for comprehensive 3D scene description.\n\nOpenUSD serves as the backbone of the [NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/) cloud computing platform. Omniverse includes a variety of applications such as [USD Composer](https://docs.omniverse.nvidia.com/composer/latest/index.html) for viewing and manipulating OpenUSD scenes, with features such as:\n- State-of-the-art cloud rendering\n- Live collaborative scene editing\n- Multi-user mixed reality\n\nDownload the [NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/foundation-apps/) launcher to get started with Omniverse. See [NVIDIA OpenUSD Tutorials](https://developer.nvidia.com/usd/tutorials) for getting started with the OpenUSD Python libraries we use in this tutorial.\n\n### [MONAI Deploy](https://monai.io/deploy.html) + [Holoscan](https://developer.nvidia.com/holoscan-sdk) Pipeline\n\nThe MONAI Deploy App SDK provides a series of operators to load and select DICOM instances and then decode the pixel data into in-memory NumPy data objects. MONAI Deploy integrates seamlessly with Holoscan pipelines.\n\n1. The DICOM Loader operator parses a set of DICOM instance files, and loads them into a set of objects representing the logical hierarchical structure of DICOM Study, Series, and Instance. Key DICOM metadata is extracted, while the image pixel data is not loaded in memory or decoded.\n2. The DICOM Series Selector selects relevant DICOM Series, e.g, a MR T2 series, using simple configurable selection rules.\n3. The Series to Volume converter decodes and combines the pixel data of the instances to a 3D NumPy array with a set of metadata for spacing, orientation, etc.\n4. The MONAI Deploy AI Inference Operator accepts the 3D volume and runs AI inference to segment the region of interest, which in this case is pixels representing the spleen.\n5. The MONAI STL Conversion Operator converts the output label volume to a mesh in STL format.\n6. The Holoscan \"Send Mesh to USD\" operator writes the mesh to disk in the OpenUSD format.\n\n## Requirements\n\nPlease review the [HoloHub README](../../README.md) to get started with HoloHub general requirements before continuing.\n\n### Hardware\n\nThis tutorial may run on an `amd64` or `arm64` workstation.\n- For `amd64` we rely on `usd-core` Python wheels from PyPI for OpenUSD support.\n- For `arm64` we rely on NVIDIA Omniverse Python libraries for OpenUSD support.\n\n### Host Software\n\nThis tutorial should run in a `docker` container:\n\n```sh\nsudo apt-get update && sudo apt-get install docker\n```\n\n## Building the tutorial container\n\nRun the command below from the top-level HoloHub directory to build the tutorial container on the host workstation:\n\n```sh\nexport NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v1.0.3-dgpu\"\n./dev_container build --docker_file tutorials/dicom_to_usd_with_monai_and_holoscan/Dockerfile --base_img ${NGC_CONTAINER_IMAGE_PATH} --img holohub:dicom-to-usd\n```\n\n## Running the application\n\nRun the commands below on the host workstation to launch the container and run the tutorial. The application will run the MONAI Deploy + Holoscan pipeline for AI segmentation and write results to the `.usd` output file. The mesh will also be available as a `mesh.stl` file on disk.\n\n```sh\n./dev_container launch --img holohub:dicom-to-usd # start the container\ncd ./tutorials/dicom_to_usd_with_monai_and_holoscan\npython tutorial.py --output ./output/spleen-segmentation.usd # run the tutorial\n```\n\nDownload the [NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/) launcher to explore applications such as [USD Composer](https://docs.omniverse.nvidia.com/composer/latest/index.html) for viewing and manipulating the OpenUSD output file.",
        "application_name": "dicom_to_usd_with_monai_and_holoscan",
        "source_folder": "tutorials"
    },
    {
        "metadata": {
            "name": "Holoscan Playground on AWS",
            "description": "The Holoscan on AWS EC2 experience is an easy way for having a first try at the Holoscan SDK.",
            "authors": [
                {
                    "name": "Jin Li",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "0.1.0",
            "changelog": {
                "0.1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Amazon Web Services",
                "AWS",
                "Cloud",
                "EC2"
            ],
            "ranking": 1,
            "dependencies": {
                "services": [
                    {
                        "name": "Amazon Web Services EC2",
                        "url": "https://aws.amazon.com/ec2/"
                    }
                ]
            }
        },
        "readme": "# Holoscan Playground on AWS\n## Overview\nThe Holoscan on AWS EC2 experience is an easy way for having a first try at the Holoscan SDK. The [Holoscan SDK documentation](https://docs.nvidia.com/holoscan/sdk-user-guide/sdk_installation.html#prerequisites) lists out the hardware prerequisites. If you have a compatible hardware at hand, please get started with the SDK on your hardware. Otherwise, you could utilize an AWS EC2 instance to have a first look at the Holoscan SDK by following this guide. \n\nWe estimate the time needed to follow this guide is around 1 hour, after which you could feel free to explore more of the SDK examples and applications. Please note that for the g5.xlarge instance type utilized, [the cost](https://aws.amazon.com/ec2/pricing/on-demand/) is $1.006/hour.\n\n1. The AWS experience is intended as a trial environment of the Holoscan SDK, not as a full time development environment. Some limitations of running the SDK on an EC2 instance are: \n\n2. An EC2 instance does not have the capability of live input sources, including video capture cards like AJA and Deltacast, or the onboard HDMI input port on devkits.\nAn EC2 instance does not have ConnectX networking capabilities available on devkits.\n\n3. Display forwarding from EC2 to your local machine depends on internet connectivity and results in heavy latency, so if you would like to develop applications with display, it is not ideal.\n\n## Launch EC2 Instance\nType in the name that you want to give to the instance.\n\n<img src=\"./images/Launch Instance 1.png\" alt=\"drawing\" width=\"600\"/>\n\nIn the `Application and OS Images (Amazon Machine Image)` window, search for `NVIDIA`.\n\n<img src=\"./images/Launch Instance 2.png\" alt=\"drawing\" width=\"600\"/>\n\nFrom the results, switch to `AWS Marketplace AMIs` and choose `NVIDIA GPU-Optimized AMI`.\n\n<img src=\"./images/Launch Instance 3.png\" alt=\"drawing\" width=\"900\"/>\n\nSelect `Continue` after viewing the details of this AMI.\n\n<img src=\"./images/Launch Instance 4.png\" alt=\"drawing\" width=\"900\"/>\n\nThe selected AMI should look like this in the view to create an instance:\n\n<img src=\"./images/Launch Instance 5.png\" alt=\"drawing\" width=\"600\"/>\n\nFor `Instance type`, select `g5.xlarge`. \nNote: If you see an error similar to `The selected instance type is not supported in the zone (us-west-2d). Please select a different instance type or subnet.`, go down to `Network settings`, click on `Edit`, and try changing the `Subnet` selection.\nNote: If `g5.xlarge` is not available in any region/subnet accessible to you, `p3.2xlarge` should also work.\n\n<img src=\"./images/Launch Instance 7.png\" alt=\"drawing\" width=\"600\"/>\n\nFor `Key pair`, create a new key pair, enter the key pair name as you like, and store the file as prompted. After clicking on `Create key pair`, the file `your-name.pem` will be automatically downloaded by the browser to the Downloads folder. Then select the key pair in the view to create an instance.\n\n<img src=\"./images/Launch Instance 6.png\" alt=\"drawing\" width=\"600\"/>\n\nConfigure the `Network settings`. Click on `Edit` to start. \n* If you got an error in the `Instance type` selection about `g5.xlarge` being unavailable, try changing your `Subnet` selection in `Network settings`. Otherwise, there is no need to change the `Subnet` selection.\n* Make sure your `Auto-assign public IP` is enabled, otherwise you would have issues ssh\u2019ing into the instance. \n* Select a security group with a public IP address where you plan to ssh from, if one doesn\u2019t exist yet, select `create security group`. \n    * If you\u2019re already on the local machine you plan to ssh from, select `My IP` under `Source Type`. \n    * To add other machines that you plan to ssh from, select `Custom` under `Source Type` and enter your public IP address under `Source`. You can find the public IP address of the machine by going to https://www.whatsmyip.org/ from the machine. \n\n<img src=\"./images/Launch Instance 8.png\" alt=\"drawing\" width=\"600\"/>\n\nKeep the default 128 GB specification in `Configure storage`. \n\n<img src=\"./images/Launch Instance 9.png\" alt=\"drawing\" width=\"600\"/>\n\nYour Summary on the right side should look like this:\n\n<img src=\"./images/Launch Instance 10.png\" alt=\"drawing\" width=\"300\"/>\n\nPlease note that with a different instance type, Storage (volumes) may look different too.\n\nClick `Launch instance`, and you should see a `Success` notification.\n\n<img src=\"./images/Launch Instance 11.png\" alt=\"drawing\" width=\"900\"/>\n\nNow go back to the `Instances` window to view the `Status check` of the instance we had just launched. It should show `Initializing` for a few minutes:\n\n<img src=\"./images/Launch Instance 12.png\" alt=\"drawing\" width=\"900\"/>\n\nAnd later it should show `2/2 checks passed`:\n\n<img src=\"./images/Launch Instance 13.png\" alt=\"drawing\" width=\"900\"/>\n\nNow we\u2019re ready to ssh into the instance.\n\n## SSH into EC2 Instance\nClick on the instance ID, and you should see this layout for instance details. Click on the `Connect` button on the top right.\n\n<img src=\"./images/SSH into Instance 1.png\" alt=\"drawing\" width=\"900\"/>\n\nUnder the `SSH client` tab there are the SSH instructions. Note that the username `root` is guessed, and for the AMI we chose, it should be `ubuntu`. The private key file that you saved from when you were configuring the instance should be on the machine that you are ssh\u2019ing from. \n\nAdd `-X` to the ssh command to enable display forwarding.\n\n\n<img src=\"./images/SSH into Instance 2.png\" alt=\"drawing\" width=\"600\"/>\n\n## Setting up Display Forwarding from EC2 Instance\nHoloscan SDK has examples and applications that depend on seeing a display. For this experience, we will do X11 forwarding.\n\n### On EC2 Instance\nFirst,install the package needed for a simple forwarding test, xeyes.\n\n```sh\nsudo apt install -y x11-apps\n```\n\nNext, run \u201cxeyes\u201d in the terminal, and you should get a display window popping up on the machine you\u2019re ssh\u2019ing from:\n```sh\nxeyes\n```\n\nhttps://github.com/jin-nvidia/holohub/assets/60405124/57c76bed-ca16-458b-8740-1e4351ca63f7\n\n\nIf you run into display issues, ensure the machine you\u2019re ssh\u2019ing from has X11 forwarding enabled. Please see the [Troubleshooting](#troubleshooting) section.\n\n### In a Docker Container On EC2 Instance\n\n\nNow you have enabled display forwarding on the EC2 instance bare metal, let\u2019s take it one step further to enable display forwarding from a Docker container on the EC2 instance.\n\n```sh\nXSOCK=/tmp/.X11-unix\nXAUTH=/tmp/.docker.xauth\n# the error \u201cfile does not exist\u201d is expected at the next command\nxauth nlist $DISPLAY | sed -e 's/^..../ffff/' | sudo xauth -f $XAUTH nmerge -\nsudo chmod 777 $XAUTH\ndocker run -ti -e DISPLAY=$DISPLAY -v $XSOCK:$XSOCK -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH --net host ubuntu:latest\n```\n\nWithin the container:\n\n```sh\napt update && apt install -y x11-apps\nxeyes\n```\nPress  `ctrl + D` to exit the Docker container.\nNow we have enabled display forwarding from both EC2 bare metal and containerized environments!\n\n## Run Holoscan\n### Install Holoscan\n\nThere are [several ways](https://docs.nvidia.com/holoscan/sdk-user-guide/sdk_installation.html#development-software-stack) to install the Holoscan SDK. For the quickest way to get started, we will choose the Holoscan Docker container that already has all dependencies set up. \nWe run nvidia-smi in the EC2 instance to check that there are drivers installed:\n\n<img src=\"./images/nvidia-smi.png\" alt=\"drawing\" width=\"600\"/>\n\nFollow the overview of https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/containers/holoscan. Some modifications are made to the original commands due to the EC2 environment, `--gpus all` and `-v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH`.\n```sh \n# install xhost util\nsudo apt install -y x11-xserver-utils\n\nxhost +local:docker\n\nnvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f,l -print -quit 2>/dev/null | grep .) || (echo \"nvidia_icd.json not found\" >&2 && false)\n\nexport NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v0.6.0-dgpu\"\n\ndocker run -it --rm --net host \\\n  --gpus all \\\n   -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH \\\n  -v /tmp/.X11-unix:/tmp/.X11-unix \\\n  -v $nvidia_icd_json:$nvidia_icd_json:ro \\\n  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \\\n  -e DISPLAY=$DISPLAY \\\n  --ipc=host \\\n  --cap-add=CAP_SYS_PTRACE \\\n  --ulimit memlock=-1 \\\n  ${NGC_CONTAINER_IMAGE_PATH}\n```\n#### Sanity Check with Holoscan Hello World\n\n```sh\n/opt/nvidia/holoscan/examples/hello_world/cpp/hello_world\n```\n\n### Examples\nRefer to each one of the Holoscan SDK examples on [GitHub](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples). You will find these examples installed under `/opt/nvidia/holoscan/examples/`.\n\n#### Video Replayer Example\n\nLet\u2019s take a look at [the video replayer example](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/video_replayer) which is a basic video player app. Since we are in the Docker container, there\u2019s no need to manually download data as it already exists in the container.\n\nRun the video_replayer example\n\n```sh\ncd /opt/nvidia/holoscan \n./examples/video_replayer/cpp/video_replayer\n```\nYou should see a window like below\n\nhttps://github.com/jin-nvidia/holohub/assets/60405124/7ae99409-ca42-4c38-b495-84a59648b671\n\n> Please note that it is normal for the video stream to be lagging behind since it is forwarded from a docker container on a EC2 instance to your local machine. How much the forwarded video will lag heavily depends on the internet connection. When running Holoscan applications on the edge, we should have significantly less latency lag.\n\nYou can close the sample application by pressing ctrl +C.\n\nNow that we have run a simple video replayer, let\u2019s explore the examples a little more. \n\n#### Tensor Interoperability Example\n\n##### The C++ Tensor Interop example \n\nSince we used the Debian package install, run the C++ tensor interopability example by \n\n```sh\n/opt/nvidia/holoscan/examples/tensor_interop/cpp/tensor_interop\n```\n\nPlease refer to the [README](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/examples/tensor_interop/README.md) and the [source file](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/examples/tensor_interop/cpp/tensor_interop.cpp) to see how we can have interoperability between a native operator (`ProcessTensorOp`) and two wrapped GXF Codelets (`SendTensor` and `ReceiveTensor`). For the Holoscan documentation on tensor interop in the C++ API, please see [Interoperability between GXF and native C++ operators](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_create_operator.html#interoperability-between-gxf-and-native-c-operators).\n\n\n##### The Python Tensor Interop example \n\nThe Python [Tensor Interop example](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/tensor_interop) demonstrates interoperability between a native Python operator (`ImageProcessingOp`) and two operators that wrap existing C++ based operators,  (`VideoStreamReplayerOp` and `HolovizOp`) through the Holoscan Tensor object. \n\nRun the Python example by \n\n```sh\npython3 /opt/nvidia/holoscan/examples/tensor_interop/python/tensor_interop.py\n```\nThis example applies a Gaussian filtering to each frame of an endoscopy video stream and displays the filtered (blurred) video stream. You should see a window like below\n\nhttps://github.com/jin-nvidia/holohub/assets/60405124/b043637b-5fd9-4ee1-abc5-0dae069e785f\n\nThe native Python operator is defined at [tensor_interop.py#L37](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/examples/tensor_interop/python/tensor_interop.py#L37). We can see in the initialization `__init__()` of the operator, `self.count` was initialize to 1. In the `setup()` method, the input message, output message and the parameter `sigma` is defined. The `compute()` method is what gets called every time. In the `compute()` method, first we receive the upstream tensor by \n\n```python\nin_message = op_input.receive(\"input_tensor\")\n```\n\nPlease note that `input_tensor` is the name defined in `setup()`. \n\n`cp_array` is the CuPy array that holds the output value after the Gaussian filter, and we can see that the way the CuPy array gets transmitted downstream is \n```python\nout_message = dict()\n\u2026\n# add each CuPy array to the out_message dictionary\nout_message[key] = cp_array\n\u2026\nop_output.emit(out_message, \"output_tensor\")\n```\n\nPlease note that `output_tensor` is the name defined in `setup()`. \n\nSince there is only one input and one output port, when connecting the native Python operator `ImageProcessingOp` to its upstream and downstream operators, we do not need to specify the in/out name for `ImageProcessingOp`:\n```python\nself.add_flow(source, image_processing)\nself.add_flow(image_processing, visualizer, {(\"\", \"receivers\")})\n```\n\nOtherwise, with each `add_flow()`, the input and output port names need to be specified when multiple ports are present.\n\nFor more information on tensor interop in Python API, please see the Holoscan documentation [Interoperability between wrapped and native Python operators](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_create_operator.html#interoperability-between-wrapped-and-native-python-operators). \n\nNow that we have seen an example of tensor interop for single tensors per port, let\u2019s look at the next example where there are multiple tensors in the native operator\u2019s output port.\n\n#### Holoviz Example\n\nLet\u2019s take a look at the [Holoviz example](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/holoviz). Run the example\n```sh\npython3 /opt/nvidia/holoscan/examples/holoviz/python/holoviz_geometry.py\n```\n\nYou should get something like below on the display\n\nhttps://github.com/jin-nvidia/holohub/assets/60405124/6d79845a-66bd-4448-9646-284b90c5e5f3\n\nPlease take your time to look through [holoviz_geometry.py](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/examples/holoviz/python/holoviz_geometry.py) for how each one of the shapes and text in the native Holoscan Python operator is defined. \n\nLet\u2019s also dive into how we can add to `out_message` and pass to Holoviz various tensors at the same time, including the frame itself, `box_coords`,  `triangle_coords`, `cross_coords`, `oval_coords`, the time varying `point_coords`, and `label_coords`. \n\n```python\n# define the output message dictionary where box_coords is a numpy array and \u201cboxes\u201d is the tensor name\nout_message = {\n            \"boxes\": box_coords,\n            \"triangles\": triangle_coords,\n            \"crosses\": cross_coords,\n            \"ovals\": oval_coords,\n            \"points\": point_coords,\n            \"label_coords\": label_coords,\n            \"dynamic_text\": dynamic_text,\n}\n\n# emit the output message \nop_output.emit(out_message, \"outputs\")\n```\n\nWe can also see that each tensor name is referenced by the `tensors` parameter in the instantiation of a Holoviz operator [at line 249](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/examples/holoviz/python/holoviz_geometry.py#L249). \n\nThis is a great example and reference not only for passing different shapes to Holoviz, but also creating and passing multiple tensors within one message from a native Holoscan Python operator to the downstream operators.\n\nFor more information on the Holoviz module, please see the [Holoscan documentation](https://docs.nvidia.com/holoscan/sdk-user-guide/clara_holoviz.html). \n\nExit from the Docker container by ctrl+D.\n\n### Applications\n\nTo run the reference applications on Holoscan, let\u2019s go to [HoloHub](https://github.com/nvidia-holoscan/holohub) - a central repository for users and developers to share reusable operators and sample applications.\n\nOn the EC2 instance, clone the HoloHub repo:\n```sh\ncd ~\ngit clone https://github.com/nvidia-holoscan/holohub.git\ncd holohub\n```\nTo set up and build HoloHub, we will go with the option `Building dev container`: Run the following command from the holohub directory to build the development container:\n```sh\n./dev_container build\n```\n\nCheck the tag for the container we had just build:\n\n```sh\ndocker images\n```\nThere should be an image with repository:tag similar to `holohub:ngc-vx.y.z-dgpu` where `x.y.z` is the latest SDK version. We will set this as `HOLOHUB_IMAGE`:\n```sh\n# make sure to replace 0.6.0 with the actual SDK version\nexport HOLOHUB_IMAGE=holohub:ngc-v0.6.0-dgpu\n```\nNext, launch the dev container for HoloHub. On a regular machine we can do so by `./dev_container launch`, however we need to make a few adjustments to the command again since we\u2019re running on an EC2 instance:\n```sh\ndocker run -it --rm --net host  -v /etc/group:/etc/group:ro -v /etc/passwd:/etc/passwd:ro -v $PWD:/workspace/holohub -w /workspace/holohub --gpus all -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY --group-add video -v /etc/vulkan/icd.d/nvidia_icd.json:/etc/vulkan/icd.d/nvidia_icd.json:ro  -v $XAUTH:$XAUTH -e XAUTHORITY=$XAUTH $HOLOHUB_IMAGE\n```\nPlease refer to [HoloHub](https://github.com/nvidia-holoscan/holohub#building-holohub) for instructions on building each application.\n\n#### Endoscopy Tool Tracking Application\n\nBuild the sample app and run:\n```sh\n./run build endoscopy_tool_tracking\n./run launch endoscopy_tool_tracking cpp\n```\nYou should see a window like:\n\nhttps://github.com/jin-nvidia/holohub/assets/60405124/8eb93c50-d893-4b2c-897b-57de94b91371\n\n> Note: Be prepared to wait a few minutes as we\u2019re running the app for the first time, and it will convert the ONNX model to a TensorRT engine. The conversion happens only for the first time, after that, each time we run the app the TensorRT engine is already present.\n\nPlease visit [HoloHub](https://github.com/nvidia-holoscan/holohub/tree/main/applications/endoscopy_tool_tracking) to see the application graph, different input types (although on the EC2 instance we can not use a live source such as the AJA capture card), and the construction of the same application in C++ vs in Python.\n\n#### Multi AI Ultrasound Application\n\nIn the last application we saw how to run AI inference on the video source. Next, let\u2019s see how we can run inference with multiple AI models at the same time within a Holoscan application, enabled by Holoscan Inference Module.  \nBuild the application in [applications/multiai_ultrasound](https://github.com/nvidia-holoscan/holohub/tree/main/applications/multiai_ultrasound)\n```sh\n./run build multiai_ultrasound\n```\n\nLaunch the Python application:\n```sh\n./run launch multiai_ultrasound python\n```\nYou should see a window like below:\n\nhttps://github.com/jin-nvidia/holohub/assets/60405124/9d347b44-d635-4cc6-b013-7d26e3e4e2be\n\nYou can find more information on Holoscan Inference Module [here](https://docs.nvidia.com/holoscan/sdk-user-guide/clara_holoinfer.html), including the parameters you can specify to define inference configuration, how to specify the multiple (or single) model(s) you want to run, and how the Holoscan Inference Module functions as an operator within the Holoscan SDK framework.\n\nPlease see the application graph and more on [HoloHub](https://github.com/nvidia-holoscan/holohub/tree/main/applications/multiai_ultrasound) for how the multi AI inference connects to the rest of the operators, the definition of the same application in Python vs in C++, and how the [multiai_ultrasound.yaml](https://github.com/nvidia-holoscan/holohub/blob/main/applications/multiai_ultrasound/python/multiai_ultrasound.yaml) config file defines parameters for each operator especially the [Holoscan Inference Module](https://github.com/nvidia-holoscan/holohub/blob/main/applications/multiai_ultrasound/python/multiai_ultrasound.yaml#L60-L78).\n\n## Stop EC2 Instance\n\nNow that you have run several Holoscan Examples and HoloHub Applications, please continue exploring the rest of Examples and Applications, and when you\u2019re ready, stop the instance by going back to EC2 page with the list of `Instances`, select the launched instance, and select `Stop Instance` in the dropdown `Instance state`.\n\n<img src=\"./images/stop instance.png\" alt=\"drawing\" width=\"200\"/>\n\n## Troubleshooting\n\nIf you receive a display forwarding error such as \n```\nunable to open display \"localhost:10.0\"\n```\n```\nGlfw Error 65544: X11: Failed to open display localhost:10.0\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  Failed to initialize glfw\n```\nPlease see below to find the suggestion for your OS.\n\n### From a Linux Local Machine\n* Ensure that `-X` is added to the ssh command when connecting to the EC2 instance.\n\n### From a Windows Local Machine\n* Ensure that `-X` is added to the ssh command when connecting to the EC2 instance.\n* Try using MobaXTerm to establish a SSH connection with X11 forwarding enabled.\n\n### From a Mac Local Machine\n\n* Download [Quartz](https://www.xquartz.org/), reboot, and enable the following. \n\n  <img src=\"./images/mac x11.png\" alt=\"drawing\" width=\"300\"/>\n\n  Once Quartz is downloaded it will automatically launch when running display forwarding apps like `xeyes`.\n\n* Ensure that `-X` is added to the ssh command when connecting to the EC2 instance.\n",
        "application_name": "holoscan-playground-on-aws",
        "source_folder": "tutorials"
    },
    {
        "metadata": {
            "name": "Deploying Llama-2 70b model on the edge with IGX Orin",
            "description": "This tutorial will walk you through how to run a quantized version of Meta's Llama-2 70b model as the backend LLM for a Gradio chatbot app, all running on an NVIDIA IGX Orin.",
            "authors": [
                {
                    "name": "Nigel Nelson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "0.1.0",
            "changelog": {
                "0.1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "arm64"
            ],
            "tags": [
                "Chatbot",
                "CUDA",
                "HuggingFace",
                "Llama",
                "LLM"
            ],
            "ranking": 1,
            "dependencies": {
                "data": [
                    {
                        "name": "Llama-2",
                        "url": "https://about.fb.com/news/2023/07/llama-2/"
                    }
                ],
                "libraries": [
                    {
                        "name": "CUDA Toolkit",
                        "url": "https://developer.nvidia.com/cuda-downloads",
                        "minimum_required_version": "11.8"
                    },
                    {
                        "name": "llama.cpp",
                        "url": "https://github.com/ggerganov/llama.cpp"
                    }
                ],
                "python-requirements": [
                    {
                        "filepath": "requirements.txt"
                    }
                ],
                "services": [
                    {
                        "name": "HuggingFace",
                        "url": "https://huggingface.co/"
                    }
                ]
            }
        },
        "readme": "<div align=\"center\">\n<h1>\n\ud83e\udd99 Tutorial: Deploying Llama-2 70b model on the edge with IGX Orin \ud83e\udd99\n</h1>\n<img src=\"./IGX_w_llama.png\" width=50% style=\"border: 2px solid black;\">\n</div>\n\n## Introduction:\n>With the recent release of the [Llama-2](https://about.fb.com/news/2023/07/llama-2/) family of models, there has been an excess of excitement in the LLM community due to these models being released freely for research and commercial use. Upon their release, the 70b version of the Llama-2 model quickly rose to the top place on HuggingFace's [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). Additionally, thanks to the publishing of the model weights, fine-tuned versions of these models are consistently being released and raising the bar for the top performing open-LLM. This most recent release of Llama-2 provides some of the first legitimate open-source alternatives to the previously unparalleled performance of closed-source LLMs. This enables developers to deploy these Llama-2 models locally, and benefit from being able to use some of the most advanced LLMs ever created, while also keeping all of their data on their own host machines.\n>\n> The only edge device that is capable of running the Llama-2 70b locally is the [NVIDIA IGX Orin](https://www.nvidia.com/en-us/edge-computing/products/igx/). In order to get the Llama-2 70b model running inference, all you need is an IGX Orin, a mouse, a keyboard, and to follow the tutorial below.\n\n## Overview:\n>This tutorial will walk you through how to run a quantized version of Meta's Llama-2 70b model as the backend LLM for a Gradio chatbot app, all running on an NVIDIA IGX Orin. Specifically, we will use [Llama.cpp](https://github.com/ggerganov/llama.cpp), a project that ports Llama models into C and C++ with CUDA acceleration, to load and run the quantized Llama-2 models. We will setup Llama.cpp's `api_like_OAI.py` Flask app that emulates the OpenAI API. This will then enable us to create a Gradio chatbot app that utilizes the popular OpenAI API Python library to interact with our local Llama-2 model. Thus, at the conclusion of this tutorial you will have a chatbot app that rivals the performance of closed-source models, while keeping all of your data local and running everything self-contained on an NVIDIA IGX Orin.\n\n---\n\n## Hardware Requirements: \ud83d\udc49\ud83d\udcbb\n- [NVIDIA IGX Orin](https://www.nvidia.com/en-us/edge-computing/products/igx/) with:\n  - RTX A6000 dGPU\n  - 500 GB SSD\n\n## Dependencies: \ud83d\udce6\n- [NVIDIA Drivers](https://www.nvidia.com/download/index.aspx)\n- [CUDA Toolkit](https://developer.nvidia.com/cuda-downloads) >= 11.8\n- Python >= 3.8\n- [`build-essential`](https://packages.ubuntu.com/focal/build-essential) apt package (gcc, g++, etc.)\n- [Cmake](https://apt.kitware.com/) >= 3.17\n\n## Cloning and building Llama.cpp \u2692\ufe0f:\n1. Clone Llama.cpp:\n```bash\ngit clone https://github.com/ggerganov/llama.cpp.git\n```\n\n2. Checkout a stable commit of llama.cpp:\n```bash\ncd llama.cpp\ngit checkout e519621010cac02c6fec0f8f3b16cda0591042c0 # Commit date: 9/27/23\n```\n\n3. Follow [cuBLAS build instructions](https://github.com/ggerganov/llama.cpp/tree/master#cublas) for Llama.cpp to provide BLAS acceleration using the CUDA cores of your NVIDIA GPU.\nNavigate to the `/Llama.cpp` directory:\n```bash\ncd llama.cpp\n```\nUsing `make`:\n```bash\nmake LLAMA_CUBLAS=1\n```\n\nBy successfully executing these commands you will now be able to run Llama models on your local machine with BLAS acceleration!\n\n## Downloading Llama-2 70B \u2b07\ufe0f\ud83d\udcbe:\n>In order to use Llama-2 70b as it is provided by Meta, you\u2019d need 140 GB of VRAM (70b params x 2 bytes = 140 GB in FP16). However, by utilizing model quantization, we can reduce the computational and memory costs of running inference by representing the weights and activations as low-precision data types, like int8 and int4, instead of higher-precision data types like FP16 and FP32. To learn more about quantization, check out: The [Ultimate Guide to Deep Learning Model Quantization](https://deci.ai/quantization-and-quantization-aware-training/).\n>\n>Llama.cpp uses quantized models that are stored in the GGUF format. Browse to [TheBloke](https://huggingface.co/TheBloke) on [Huggingface.co](https://huggingface.co/), who provides hundred of the latest quantized models. Feel free to choose a GGUF model that suits your needs. However, for this tutorial, we will use [*TheBloke's* 4-bit medium GGUF quantization](https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF) of Meta\u2019s LLama-2-70B-Chat model.\n1. Download the GGUF model from Huggingface.co.\n\n:warning: This model requires ~43 GB of VRAM.\n```bash\ncd /media/m2 # Download the model to your SSD drive\nmkdir models # Create a directory for GGUF models\ncd models\nwget https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q4_K_M.gguf\n```\n\n## Running Llama-2 70B \ud83e\udd16:\n1. Return to the home directory of Llama.cpp:\n```bash\ncd <your_parent_dir>/llama.cpp\n```\n\n2. Run Llama.cpp\u2019s example server application to set up a HTTP API server and a simple web front end to interact with our Llama model:\n```bash\n./server -m /media/m2/models/llama-2-70b-chat.Q4_K_M.gguf -ngl 1000 -c 4096 --alias llama_2\n```\n\n- `-m`: indicates the location of our model.\n- `-ngl`: the number of layers to offload to the GPU (1000 ensures all layers are).\n- `-c`: the size of the prompt context.\n- `--alias`: name given to our model for access through the API.\n\nAfter executing, you should see the below output indicating the model being loaded to VRAM and the specs of the model:\n```bash\nggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA RTX A6000, compute capability 8.6\n{\"timestamp\":1695853185,\"level\":\"INFO\",\"function\":\"main\",\"line\":1294,\"message\":\"build info\",\"build\":1279,\"commit\":\"e519621\"}\n{\"timestamp\":1695853185,\"level\":\"INFO\",\"function\":\"main\",\"line\":1296,\"message\":\"system info\",\"n_threads\":6,\"total_threads\":12,\"system_info\":\"AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \"}\nllama_model_loader: loaded meta data with 19 key-value pairs and 723 tensors from /media/m2/models/llama-2-70b-chat.Q4_K_M.gguf (version GGUF V2 (latest))\n**Verbose llama_model_loader output removed for conciseness**\nllm_load_print_meta: format         = GGUF V2 (latest)\nllm_load_print_meta: arch           = llama\nllm_load_print_meta: vocab type     = SPM\nllm_load_print_meta: n_vocab        = 32000\nllm_load_print_meta: n_merges       = 0\nllm_load_print_meta: n_ctx_train    = 4096\nllm_load_print_meta: n_ctx          = 4096\nllm_load_print_meta: n_embd         = 8192\nllm_load_print_meta: n_head         = 64\nllm_load_print_meta: n_head_kv      = 8\nllm_load_print_meta: n_layer        = 80\nllm_load_print_meta: n_rot          = 128\nllm_load_print_meta: n_gqa          = 8\nllm_load_print_meta: f_norm_eps     = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps = 1.0e-05\nllm_load_print_meta: n_ff           = 28672\nllm_load_print_meta: freq_base      = 10000.0\nllm_load_print_meta: freq_scale     = 1\nllm_load_print_meta: model type     = 70B\nllm_load_print_meta: model ftype    = mostly Q4_K - Medium\nllm_load_print_meta: model params   = 68.98 B\nllm_load_print_meta: model size     = 38.58 GiB (4.80 BPW) \nllm_load_print_meta: general.name   = LLaMA v2\nllm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token = 2 '</s>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.23 MB\nllm_load_tensors: using CUDA for GPU acceleration\nllm_load_tensors: mem required  =  140.86 MB (+ 1280.00 MB per state)\nllm_load_tensors: offloading 80 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloading v cache to GPU\nllm_load_tensors: offloading k cache to GPU\nllm_load_tensors: offloaded 83/83 layers to GPU\nllm_load_tensors: VRAM used: 40643 MB\n....................................................................................................\nllama_new_context_with_model: kv self size  = 1280.00 MB\nllama_new_context_with_model: compute buffer total size =  561.47 MB\nllama_new_context_with_model: VRAM scratch buffer: 560.00 MB\n\nllama server listening at http://127.0.0.1:8080\n\n{\"timestamp\":1695853195,\"level\":\"INFO\",\"function\":\"main\",\"line\":1602,\"message\":\"HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":8080}\n```\n\nNow, you can interact with the simple web front end by browsing to http://127.0.0.1:8080. Use the provided chat interface to query the Llama-2 model and experiment with manipulating the provided hyperparameters to tune the responses to your liking.\n\n<div align=\"center\">\n<img src=\"./llama_cpp_api.png\" width=40% style=\"border: 2px solid black;\">\n</div>\n\n## Setting up a local OpenAI server \ud83d\udda5\ufe0f:\n>Llama.cpp includes a nifty Flask app `api_like_OAI.py`. This Flask app sets up a server that emulates the OpenAI API. Its trick is that it converts the OpenAI API requests into the format expected by the Llama model, and forwards the captured requests to our local Llama-2 model. This allows you to use the popular OpenAI Python backend, and thus, countless powerful LLM libraries like [LangChain](https://python.langchain.com/docs/get_started/introduction.html), [Scikit-LLM](https://github.com/iryna-kondr/scikit-llm), [Haystack](https://haystack.deepset.ai/), and more. However, instead of your data being sent to OpenAI\u2019s servers, it is all processed locally on your machine!\n1. In order to run the OpenAI API server and our eventual Gradio chat app, we need to open a new terminal and install a few Python dependencies:\n```bash\ncd tutorials/local-llama\npip install -r requirements.txt\n```\n2. This then allows us to run the Flask server:\n```bash\ncd <your_parent_dir>/llama.cpp/examples/server/\npython api_like_OAI.py\n```\n3. The server should begin running almost immediately and give you the following output:\n<div align=\"center\">\n<img src=\"./api_like_OAI.png\" width=70% style=\"border: 2px solid black;\">\n</div>\n\n## Creating the Gradio Chat App \ud83d\udcac\ud83d\udcf1:\n1. Create a new project directory and a `chatbot.py` file that contains the following code:\n\n```python\nimport gradio as gr\nimport openai\n\n# Indicate we'd like to send the request\n# to our local model, not OpenAI's servers\nopenai.api_base = \"http://127.0.0.1:8081\"\nopenai.api_key = \"\"\n\n\ndef to_oai_chat(history):\n    \"\"\"Converts the gradio chat history format to\n    the OpenAI chat history format:\n\n    Gradio format: ['<user message>', '<bot message>']\n    OpenAI format: [{'role': 'user', 'content': '<user message>'},\n                    {'role': 'assistant', 'content': '<bot_message>'}]\n\n    Additionally, this adds the 'system' message to the chat to tell the\n    assistant how to act.\n    \"\"\"\n    chat = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful AI Assistant who ends all of your responses with </BOT>\",\n        }\n    ]\n\n    for msg_pair in history:\n        if msg_pair[0]:\n            chat.append({\"role\": \"user\", \"content\": msg_pair[0]})\n        if msg_pair[1]:\n            chat.append({\"role\": \"assistant\", \"content\": msg_pair[1]})\n    return chat\n\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(height=650)\n    msg = gr.Textbox()\n    clear = gr.Button(\"Clear\")\n\n    def user(user_message, history):\n        \"\"\"Appends a submitted question to the history\"\"\"\n        return \"\", history + [[user_message, None]]\n\n    def bot(history):\n        \"\"\"Sends the chat history to our Llama-2 model server\n        so that the model can respond appropriately\n        \"\"\"\n        # Gradio chat -> OpenAI chat\n        oai_chat = to_oai_chat(history)\n\n        # Send chat history to our Llama-2 server\n        response = openai.ChatCompletion.create(\n            messages=oai_chat,\n            stream=True,\n            model=\"llama_2\",\n            temperature=0,\n            # Used to stop runaway responses\n            stop=[\"</BOT>\"],\n        )\n\n        history[-1][1] = \"\"\n        for response_chunk in response:\n            # Filter through meta-data in the HTTP response to get response text\n            next_token = response_chunk[\"choices\"][0][\"delta\"].get(\"content\")\n            if next_token:\n                history[-1][1] += next_token\n                # Update the Gradio app with the streamed response\n                yield history\n\n    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n    clear.click(lambda: None, None, chatbot, queue=False)\n\ndemo.queue()\ndemo.launch()\n```\n\n2. Begin running the Gradio chat app:\n```bash\npython chatbot.py\n```\n\n3. The chat app should now be accessible at http://127.0.0.1:7860:\n\n<div align=\"center\">\n<img src=\"./chatbot_app.png\" width=70% style=\"border: 2px solid black;\">\n</div>\n\nYou're now set up to interact with the Llama-2 70b model, with everything running locally! If you want to take this project further, you can experiment with different system messages to suit your needs or add the ability to interact with your local documents using frameworks like LangChain. Enjoy experimenting!\n## Sources:\n- https://huggingface.co/meta-llama/Llama-2-70b-chat-hf/tree/main\n- https://huggingface.co/docs/optimum/concept_guides/quantization\n- https://deci.ai/quantization-and-quantization-aware-training/\n- https://www.gradio.app/guides/creating-a-custom-chatbot-with-blocks#add-streaming-to-your-chatbot\n",
        "application_name": "local-llama",
        "source_folder": "tutorials"
    },
    {
        "metadata": {
            "name": "Integrate External Libraries into a Holoscan Pipeline",
            "description": "General tutorial on how to integrate external libraries into Holoscan SDK based applications and pipelines",
            "authors": [
                {
                    "name": "Meiran Peng",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "0.1.0",
            "changelog": {
                "0.1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "CV-CUDA",
                "OpenCV",
                "cuCIM",
                "Holoscan"
            ],
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "RAPIDS cuCIM",
                        "version": "23.04"
                    },
                    {
                        "name": "CV-CUDA",
                        "version": "0.2.1"
                    },
                    {
                        "name": "OpenCV",
                        "version": "4.8.0"
                    }
                ]
            }
        },
        "readme": "# Best Practices to integrate external libraries into Holoscan pipelines\nThe Holoscan SDK is part of NVIDIA Holoscan, the AI sensor processing platform that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run streaming, imaging, and other applications, from embedded to edge to cloud. It can be used to build streaming AI pipelines for a variety of domains, including Medical Devices, High Performance Computing at the Edge, Industrial Inspection and more.\n\nWith Holoscan SDK, one can develop an end-to-end GPU accelerated pipeline with RDMA support. However, with increasing requirements on pre-processing/post-processing other than inference only pipeline, the integration with other powerful, GPU-accelerated libraries is needed. \n\n<div align=\"center\">\n<img src=\"./images/typical_pipeline_holoscan.png\" style=\"border: 2px solid black;\">\n</div>\n\nThe datatype in Holoscan SDK is defined as [Tensor](https://docs.nvidia.com/holoscan/sdk-user-guide/generated/classholoscan_1_1tensor.html) which is a multi-dimensional array of elements of a single data type. The Tensor class is a wrapper around the [DLManagedTensorCtx](https://docs.nvidia.com/holoscan/sdk-user-guide/generated/structholoscan_1_1dlmanagedtensorctx.html#structholoscan_1_1DLManagedTensorCtx) struct that holds the DLManagedTensor object. It also supports both DLPack and NumPy\u2019s array interface (__array_interface__ and __cuda_array_interface__) so that it can be used with other Python libraries such as [CuPy](https://docs.cupy.dev/en/stable/user_guide/interoperability.html), [PyTorch](https://github.com/pytorch/pytorch/issues/15601), [JAX](https://github.com/google/jax/issues/1100#issuecomment-580773098), [TensorFlow](https://github.com/tensorflow/community/pull/180), and [Numba](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html).\n\nIn this tutorial, we will show how to integrate the libraries below into Holoscan applications in Python:\n- [Integrate RAPIDS **cuCIM** library](#integrate-rapids-cucim-library)\n- [Integrate RAPIDS **CV-CUDA** library](#integrate-cv-cuda-library)\n- [Integrate **OpenCV with CUDA Module**](#integrate-opencv-with-cuda-module)\n\n\n## Integrate RAPIDS cuCIM library\n[RAPIDS cuCIM](https://github.com/rapidsai/cucim) (Compute Unified Device Architecture Clara IMage) is an open-source, accelerated computer vision and image processing software library for multidimensional images used in biomedical, geospatial, material and life science, and remote sensing use cases.\n\nSee the supported Operators in [cuCIM documentation](https://docs.rapids.ai/api/cucim/stable/).\n\ncuCIM offers interoperability with CuPy. We can initialize CuPy arrays directly from Holoscan Tensors and use the arrays in cuCIM operators for processing without memory transfer between host and device. \n\n### Installation\nFollow the [cuCIM documentation](https://github.com/rapidsai/cucim?tab=readme-ov-file#install-cucim) to install the RAPIDS cuCIM library.\n\n### Sample code \nSample code as below:\n\n```py\nimport cupy as cp\nimport cucim.skimage.exposure as cu_exposure\nfrom cucim.skimage.util import img_as_ubyte\nfrom cucim.skimage.util import img_as_float\n\ndef CustomizedcuCIMOperator(Operator):\n    ### Other implementation of __init__, setup()... etc. \n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n        # Directly use Holoscan tensor to initialize CuPy array\n        cp_array = cp.asarray(input_tensor)\n\n        cp_array = img_as_float(cp_array)\n        cp_res=cu_exposure.equalize_adapthist(cp_array)\n        cp_array = img_as_ubyte(cp_res)\n\n        # Emit CuPy array memory as an item in a `holoscan.TensorMap`\n        op_output.emit(dict(out_tensor=cp_array), \"out\")\n\n```\n\n## Integrate CV-CUDA library\n[CV-CUDA](https://github.com/CVCUDA/CV-CUDA) is an open-source, graphics processing unit (GPU)-accelerated library for cloud-scale image processing and computer vision developed jointly by NVIDIA and the ByteDance Applied Machine Learning teams. CV-CUDA helps developers build highly efficient pre- and post-processing pipelines that can improve throughput by more than 10x while lowering cloud computing costs.\n\nSee the supported CV-CUDA Operators in the [CV-CUDA developer guide](https://github.com/CVCUDA/CV-CUDA/blob/main/DEVELOPER_GUIDE.md)\n\n### Installation\nFollow the [CV-CUDA documentation](https://cvcuda.github.io/installation.html) to install the CV-CUDA library.\n\nRequirement: CV-CUDA >= 0.2.1 (From which version DLPack interop is supported)\n\n### Sample code \nCV-CUDA is implemented with DLPack standards. So, CV-CUDA tensor can directly access Holocan Tensor. \n\nRefer to the [Holoscan CV-CUDA sample application](https://github.com/nvidia-holoscan/holohub/tree/main/applications/cvcuda_basic) for an example of how to use CV-CUDA with Holoscan SDK.\n\n```py\nimport cvcuda\n\nclass CustomizedCVCUDAOp(Operator):\n    def __init__(self, *args, **kwargs):\n\n        # Need to call the base class constructor last\n        super().__init__(*args, **kwargs)\n\n    def setup(self, spec: OperatorSpec):\n        spec.input(\"input_tensor\")\n        spec.output(\"output_tensor\")\n\n    def compute(self, op_input, op_output, context):\n        message = op_input.receive(\"input_tensor\")\n        input_tensor = message.get()\n        \n        cvcuda_input_tensor = cvcuda.as_tensor(input_tensor,\"HWC\")\n       \n        cvcuda_resize_tensor = cvcuda.resize(\n                    cvcuda_input_tensor,\n                    (\n                        640,\n                        640,\n                        3,\n                    ),\n                    cvcuda.Interp.LINEAR,\n                )\n       \n        buffer = cvcuda_resize_tensor.cuda()\n\n        # Emits an `holoscan.TensorMap` with a single entry `out_tensor`\n        op_output.emit(dict(out_tensor=buffer), \"output_tensor\")\n\n```\n\n## Integrate OpenCV with CUDA Module\n[OpenCV](https://opencv.org/) (Open Source Computer Vision Library) is a comprehensive open-source library that contains over 2500 algorithms covering Image & Video Manipulation, Object and Face Detection, OpenCV Deep Learning Module and much more. \n\n[OpenCV also supports GPU acceleration](https://docs.opencv.org/4.8.0/d2/dbc/cuda_intro.html), includes a CUDA module which is a set of classes and functions to utilize CUDA computational capabilities. It is implemented using NVIDIA CUDA Runtime API and provides utility functions, low-level vision primitives, and high-level algorithms.\n\n### Installation\nPrerequisites:\n- OpenCV >= 4.8.0 (From which version, OpenCV GpuMat supports initialization with GPU Memory pointer)\n\nInstall OpenCV with its CUDA module following the guide in [opencv/opencv_contrib](https://github.com/opencv/opencv_contrib/tree/4.x) \n\nWe also recommend referring to the [Holoscan Endoscopy Depth Estimation application container](https://github.com/nvidia-holoscan/holohub/blob/main/applications/endoscopy_depth_estimation/Dockerfile) as an example of how to build an image with Holoscan SDK and OpenCV CUDA.  \n\n### Sample code\nThe datatype of OpenCV is GpuMat which implements neither the __cuda_array_interface__ nor the standard DLPack. To achieve the end-to-end GPU accelerated pipeline / application, we need to implement 2 functions to convert the GpuMat to CuPy array which can be accessed directly with Holoscan Tensor and vice versa. \n\nRefer to the [Holoscan Endoscopy Depth Estimation sample application](https://github.com/nvidia-holoscan/holohub/tree/main/applications/cvcuda_basic) for an example of how to use the OpenCV operator with Holoscan SDK.\n\n1. Conversion from GpuMat to CuPy Array\n\nThe GpuMat object of OpenCV Python bindings provides a cudaPtr method which can be used to access the GPU memory address of a GpuMat object. This memory pointer can be utilized to initialize a CuPy array directly, allowing for efficient data handling by avoiding unnecessary data transfers between the host and device. \n\nRefer to the function below, which is used to create a CuPy array from a GpuMat. For more details, see the source code in [holohub/applications/endoscopy_depth_estimation-gpumat_to_cupy](https://github.com/nvidia-holoscan/holohub/blob/main/applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py#L52). \n\n```py\nimport cv2\nimport cupy as cp\n\ndef gpumat_to_cupy(gpu_mat: cv2.cuda.GpuMat) -> cp.ndarray:\n    w, h = gpu_mat.size()\n    size_in_bytes = gpu_mat.step * w\n    shapes = (h, w, gpu_mat.channels())\n    assert gpu_mat.channels() <=3, \"Unsupported GpuMat channels\"\n\n    dtype = None\n    if gpu_mat.type() in [cv2.CV_8U,cv2.CV_8UC1,cv2.CV_8UC2,cv2.CV_8UC3]:\n        dtype = cp.uint8\n    elif gpu_mat.type() == cv2.CV_8S:\n        dtype = cp.int8\n    elif gpu_mat.type() == cv2.CV_16U:\n        dtype = cp.uint16\n    elif gpu_mat.type() == cv2.CV_16S:\n        dtype = cp.int16\n    elif gpu_mat.type() == cv2.CV_32S:\n        dtype = cp.int32\n    elif gpu_mat.type() == cv2.CV_32F:\n        dtype = cp.float32\n    elif gpu_mat.type() == cv2.CV_64F:\n        dtype = cp.float64 \n\n    assert dtype is not None, \"Unsupported GpuMat type\"\n    \n    mem = cp.cuda.UnownedMemory(gpu_mat.cudaPtr(), size_in_bytes, owner=gpu_mat)\n    memptr = cp.cuda.MemoryPointer(mem, offset=0)\n    cp_out = cp.ndarray(\n        shapes,\n        dtype=dtype,\n        memptr=memptr,\n        strides=(gpu_mat.step, gpu_mat.elemSize(), gpu_mat.elemSize1()),\n    )\n    return cp_out\n\n```\n\nNote: In this function we used the [UnownedMemory](https://docs.cupy.dev/en/stable/reference/generated/cupy.cuda.UnownedMemory.html#cupy.cuda.UnownedMemory) API to create the CuPy array. In some cases, the OpenCV operators will allocate new device memory which needs to be handled by CuPy and the lifetime is not limited to one operator but the whole pipeline. In this case, the CuPy array initiated from the GpuMat shall know the owner and keep the reference to the object. Check the CuPy documentation for more detail on [CuPy interoperability](https://docs.cupy.dev/en/stable/user_guide/interoperability.html#device-memory-pointers).\n\n2. Conversion from Holoscan Tensor to GpuMat via CuPy array\n\nWith the release of OpenCV 4.8, the Python bindings for OpenCV now support the initialization of GpuMat objects directly from GPU memory pointers. This capability facilitates more efficient data handling and processing by allowing direct interaction with GPU-resident data, bypassing the need for data transfer between host and device memory. \n\nWithin pipeline applications based on Holoscan SDK, the GPU Memory pointer can be obtained through the `__cuda_array_interface__` interface provided by CuPy arrays. \n\nRefer to the functions outlined below for creating GpuMat objects utilizing CuPy arrays. For a detailed implementation, see the source code provided in [holohub/applications/endoscopy_depth_estimation-gpumat_from_cp_array](https://github.com/nvidia-holoscan/holohub/blob/main/applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py#L28).\n\n```py\nimport cv2\nimport cupy as cp\nimport holoscan as hs\nfrom holoscan.gxf import Entity\n\ndef gpumat_from_cp_array(arr: cp.ndarray) -> cv2.cuda.GpuMat:\n    assert len(arr.shape) in (2, 3), \"CuPy array must have 2 or 3 dimensions to be a valid GpuMat\"\n    type_map = {\n        cp.dtype('uint8'): cv2.CV_8U,\n        cp.dtype('int8'): cv2.CV_8S,\n        cp.dtype('uint16'): cv2.CV_16U,\n        cp.dtype('int16'): cv2.CV_16S,\n        cp.dtype('int32'): cv2.CV_32S,\n        cp.dtype('float32'): cv2.CV_32F,\n        cp.dtype('float64'): cv2.CV_64F\n    }\n    depth = type_map.get(arr.dtype)\n    assert depth is not None, \"Unsupported CuPy array dtype\"\n    channels = 1 if len(arr.shape) == 2 else arr.shape[2]\n    mat_type = depth + ((channels - 1) << 3)\n    \n     mat = cv2.cuda.createGpuMatFromCudaMemory(\n      arr.__cuda_array_interface__['shape'][1::-1],\n      mat_type,\n      arr.__cuda_array_interface__['data'][0]\n  )\n    return mat\n```\n\n3. Integrate OpenCV Operators inside customized Operator\n\nThe demonstration code is provided below. For the complete source code, please refer to the [holohub/applications/endoscopy_depth_estimation-customized Operator](https://github.com/nvidia-holoscan/holohub/blob/main/applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py#L161).\n\n```py\n   def compute(self, op_input, op_output, context):\n        stream = cv2.cuda_Stream()\n        message = op_input.receive(\"in\")\n\n        cp_frame = cp.asarray(message.get(\"\"))  # CuPy array\n        cv_frame = gpumat_from_cp_array(cp_frame)  # GPU OpenCV mat\n\n         ## Call OpenCV Operator \n        cv_frame = cv2.cuda.XXX(hsv_merge, cv2.COLOR_HSV2RGB)\n\n        cp_frame = gpumat_to_cupy(cv_frame)\n        cp_frame = cp.ascontiguousarray(cp_frame)\n\n        op_output.emit(dict(out_tensor=cp_frame), \"out\")\n```\n\n",
        "application_name": "integrate_external_libs_into_pipeline",
        "source_folder": "tutorials"
    }
]