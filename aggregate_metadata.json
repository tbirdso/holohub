[
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "python3 <holohub_app_source>/orsi_in_out_body.py --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi In - Out - Body Detection sample app\n\n\n<center> <img src=\"./docs/anonymization.png\" ></center>\n<center> Fig. 1: Example of anonymized result after inference </center><br>\n\n## Introduction\n\nIn robotic surgery, anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.\n\n## Pipeline\n\n<center> <img src=\"./docs/Holoscan_oob_pipeline.png\" ></center>\n<center> Fig. 2: Schematic overview of Holoscan application </center><br>\n\nTowards realtime anonymization, a binary out-of-body classifier was trained and deployed using Holoscan platform. Figure 2 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Anonymization Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) operator the tensor pixel values of every collor channel are normalized using the corresponding means and standard deviations of the anonymization dataset. After model inference with the Multi-AI inference operator, the result frame is anonymized in the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator according to the model output. The blurring is applied using a glsl program.\n## Controls\n\n| Action    | Control |\n| -------- | ------- |\n| Enable anonymization | B |\n\n\n## Build app\n\n```bash\n./run build orsi_in_out_body\n```\n\n## Launch app\n\n**C++** \n\n```bash\n./run launch orsi/orsi_in_out_body cpp\n```\n\n**Python**\n\n```bash\n./run launch orsi/orsi_in_out_body python\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>",
        "application_name": "orsi",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "<holohub_app_bin>/orsi_in_out_body <holohub_app_bin>/orsi_in_out_body.yaml --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi In - Out - Body Detection sample app\n\n\n<center> <img src=\"./docs/anonymization.png\" ></center>\n<center> Fig. 1: Example of anonymized result after inference </center><br>\n\n## Introduction\n\nIn robotic surgery, anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.\n\n## Pipeline\n\n<center> <img src=\"./docs/Holoscan_oob_pipeline.png\" ></center>\n<center> Fig. 2: Schematic overview of Holoscan application </center><br>\n\nTowards realtime anonymization, a binary out-of-body classifier was trained and deployed using Holoscan platform. Figure 2 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Anonymization Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) operator the tensor pixel values of every collor channel are normalized using the corresponding means and standard deviations of the anonymization dataset. After model inference with the Multi-AI inference operator, the result frame is anonymized in the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator according to the model output. The blurring is applied using a glsl program.\n## Controls\n\n| Action    | Control |\n| -------- | ------- |\n| Enable anonymization | B |\n\n\n## Build app\n\n```bash\n./run build orsi_in_out_body\n```\n\n## Launch app\n\n**C++** \n\n```bash\n./run launch orsi/orsi_in_out_body cpp\n```\n\n**Python**\n\n```bash\n./run launch orsi/orsi_in_out_body python\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>",
        "application_name": "orsi",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "python3 <holohub_app_source>/orsi_segmentation_ar.py --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi Non Organic Structure Segmentation and AR sample app\n\n\n<center> <img src=\"./docs/orsi_segmentation.png\" ></center>\n<center> Fig. 1: Application screenshot  </center><br>\n\n\n## Introduction\n\n3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows an in-app screenshot where the 3D model is aligned with the anatomy and the tools are segmented on top.\n\n<center> <img src=\"./docs/3Dmodel_stent.png\" ></center>\n<center> Fig. 2: 3D model of nutcracker case </center><br>\n\nThe application was successfully used to verify stent location during a nutcracker syndrome stent removal. Nutcracker syndrome is a rare vein compression disorder where the left renal vein is squeezed between the superior mesenteric artery and abdominal aorta, obstructing blood drainage and even risking blood to flow backwards, causing pain and blood in the urine. Typically, blood flow is restored through endovascular stenting. Although effective, over time the stent had migrated beyond the compression site, causing the symptoms to return and in addition posing an incidental obstruction for blood flow. The stent ofcourse had to be removed. Figure 2 shows a 3D model depicting the current situation.\n\n## Pipeline\n\n<center> <img src=\"./docs/segmentation_pipeline.png\" ></center>\n<center> Fig. 3: Schematic overview of Holoscan application </center><br>\n\nTowards realtime segmentation and AR overlay, a binary segmentation model for non-organic items was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Segmentation Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) operator the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation dataset. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the [Segmentation Postprocessor](/operators/orsi/orsi_segmentation_postprocessor/segmentation_postprocessor.cpp) resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program.\n\n##  Controls\n\n| Action | Control |\n| -------- | ------- |\n| Enable/Disable anonymization | B |\n| Enable/Disable manipulations | T |\n| Load 3D model orientation preset | CTRL + L |\n| Save current 3D model orientation as preset **(will overwrite default preset)** | CTRL + S |\n| Rotate 3D model (3 degrees of freedom)  | Left Click + Drag |\n| Rotate 3D model (1 degree of freedom) | CTRL + Left Click + Drag |\n| Zoom 3D model | Right Click + Drag |\n| Translate 3D  model | SHIFT + Left Click + Drag |\n| Enable/Disable 3D model | E |\n| Enable/Disable segmentation overlay | O |\n| Increase opacity 3D model | + |\n| Decrease opacity 3D model | - |\n| Toggle venous tree | 0 |\n| Toggle venous stent zone | 1 |\n| Toggle stent | 2 |\n\n\n## Build app\n\n```bash\n./run build orsi_segmentation_ar\n```\n\n## Launch app\n\n```bash\n./run launch orsi/orsi_segmentation_ar cpp\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>\n",
        "application_name": "orsi",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "<holohub_app_bin>/orsi_segmentation_ar <holohub_app_bin>/orsi_segmentation_ar.yaml --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi Non Organic Structure Segmentation and AR sample app\n\n\n<center> <img src=\"./docs/orsi_segmentation.png\" ></center>\n<center> Fig. 1: Application screenshot  </center><br>\n\n\n## Introduction\n\n3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows an in-app screenshot where the 3D model is aligned with the anatomy and the tools are segmented on top.\n\n<center> <img src=\"./docs/3Dmodel_stent.png\" ></center>\n<center> Fig. 2: 3D model of nutcracker case </center><br>\n\nThe application was successfully used to verify stent location during a nutcracker syndrome stent removal. Nutcracker syndrome is a rare vein compression disorder where the left renal vein is squeezed between the superior mesenteric artery and abdominal aorta, obstructing blood drainage and even risking blood to flow backwards, causing pain and blood in the urine. Typically, blood flow is restored through endovascular stenting. Although effective, over time the stent had migrated beyond the compression site, causing the symptoms to return and in addition posing an incidental obstruction for blood flow. The stent ofcourse had to be removed. Figure 2 shows a 3D model depicting the current situation.\n\n## Pipeline\n\n<center> <img src=\"./docs/segmentation_pipeline.png\" ></center>\n<center> Fig. 3: Schematic overview of Holoscan application </center><br>\n\nTowards realtime segmentation and AR overlay, a binary segmentation model for non-organic items was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Segmentation Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) operator the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation dataset. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the [Segmentation Postprocessor](/operators/orsi/orsi_segmentation_postprocessor/segmentation_postprocessor.cpp) resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program.\n\n##  Controls\n\n| Action | Control |\n| -------- | ------- |\n| Enable/Disable anonymization | B |\n| Enable/Disable manipulations | T |\n| Load 3D model orientation preset | CTRL + L |\n| Save current 3D model orientation as preset **(will overwrite default preset)** | CTRL + S |\n| Rotate 3D model (3 degrees of freedom)  | Left Click + Drag |\n| Rotate 3D model (1 degree of freedom) | CTRL + Left Click + Drag |\n| Zoom 3D model | Right Click + Drag |\n| Translate 3D  model | SHIFT + Left Click + Drag |\n| Enable/Disable 3D model | E |\n| Enable/Disable segmentation overlay | O |\n| Increase opacity 3D model | + |\n| Decrease opacity 3D model | - |\n| Toggle venous tree | 0 |\n| Toggle venous stent zone | 1 |\n| Toggle stent | 2 |\n\n\n## Build app\n\n```bash\n./run build orsi_segmentation_ar\n```\n\n## Launch app\n\n```bash\n./run launch orsi/orsi_segmentation_ar cpp\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>\n",
        "application_name": "orsi",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "python3 <holohub_app_source>/orsi_multi_ai_ar.py --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi Multi AI and AR sample app\n\n\n\n<center> <img src=\"./docs/multi_ai_1.png\" width=\"650\" height=\"400\"> <img src=\"./docs/multi_ai_2.png\" width=\"650\" height=\"400\"></center>\n<center> Fig. 1: Application screenshots  </center><br>\n\n## Introduction\n\n3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows in-app screenshots where the 3D model is used to support navigation. The large tumor in green is visible as anatomical landmark.  In addition, the application features an out-of-body detector. Anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.\n\n\n<center> <img src=\"./docs/3D model.png\" ></center>\n<center> Fig. 2: 3D model of kidney tumor case </center><br>\n\nThe application was successfully used to remove a kidney tumor during a partial nephrectomy. Figure 2 shows the kidney 3D model where the tumor is colored in yellow.\n\n## Pipeline\n\n<center> <img src=\"./docs/multiai_pipeline.png\" ></center>\n<center> Fig. 3: Schematic overview of Holoscan application </center><br>\n\nTowards realtime segmentation, AR overlay and anonymization, a binary segmentation model for non-organic items and a binary out-of-body detector was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Segmentation Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) and Anonymization Preprocessor operators the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation and anonymization dataset respectively. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the [Segmentation Postprocessor](/operators/orsi/orsi_segmentation_postprocessor/segmentation_postprocessor.cpp) resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program. Finally, the frame is anonymized according to the out-of-body detector output.\n\n\n##  Controls\n\n| Action | Control |\n| -------- | ------- |\n| Enable/Disable anonymization | B |\n| Enable/Disable manipulations | T |\n| Load 3D model orientation preset | CTRL + L |\n| Save current 3D model orientation as preset **(will overwrite default preset)** | CTRL + S |\n| Rotate 3D model (3 degrees of freedom)  | Left Click + Drag |\n| Rotate 3D model (1 degree of freedom) | CTRL + Left Click + Drag |\n| Zoom 3D model | Right Click + Drag |\n| Translate 3D  model | SHIFT + Left Click + Drag |\n| Enable/Disable 3D model | E |\n| Enable/Disable segmentation overlay | O |\n| Increase opacity 3D model | + |\n| Decrease opacity 3D model | - |\n| Toggle arterial tree | 1 |\n| Toggle venous tree | 2 |\n| Toggle ureter | 4 |\n| Toggle parenchyma | 5 |\n| Toggle tumor | 6 |\n\n\n## Build app\n\n```bash\n./run build orsi_multi_ai_ar\n```\n\n## Launch app\n\n```bash\n./run launch orsi/orsi_multi_ai_ar cpp\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>",
        "application_name": "orsi",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "ORSI Academy Surgical Tool Segmentation, Anonymization and AR Overlay",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "DeltaCast",
                "VideoMaster",
                "Endoscopy",
                "Segmentation",
                "Augmented Reality",
                "VTK"
            ],
            "ranking": 3,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "<holohub_app_bin>/orsi_multi_ai_ar <holohub_app_bin>/orsi_multi_ai_ar.yaml --data <holohub_data_dir>/orsi",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Orsi Multi AI and AR sample app\n\n\n\n<center> <img src=\"./docs/multi_ai_1.png\" width=\"650\" height=\"400\"> <img src=\"./docs/multi_ai_2.png\" width=\"650\" height=\"400\"></center>\n<center> Fig. 1: Application screenshots  </center><br>\n\n## Introduction\n\n3D models are used in surgery to improve patient outcomes. They provide information on patient specific anatomies that are not visible in the present surgical scene. Especially in robotic surgery these 3D models give great insights because they can be projected and aligned directly onto the endoscopic video. This augmented reality supports navigation for the surgeon in the console. The downside of the 3D model projection is that it occludes the surgical instruments, creating a possible hazardous situation for the patient. This application uses a deep learning segmentation model to identify non-organic items such as tools, clips and wires and projects them on top of the 3D model. This solves the occlusion problem and adds a sense of depth to the AR application. Figure 1 shows in-app screenshots where the 3D model is used to support navigation. The large tumor in green is visible as anatomical landmark.  In addition, the application features an out-of-body detector. Anonymization of video is necessary to ensure privacy and protect patient data. During e.g. swapping of robotic tools or lens cleaning the endoscope is removed from the body, possibly capturing faces, recognizable tattoos or written patient data. Therefore, going out of body is a suiting definition for an anonymization boundary.\n\n\n<center> <img src=\"./docs/3D model.png\" ></center>\n<center> Fig. 2: 3D model of kidney tumor case </center><br>\n\nThe application was successfully used to remove a kidney tumor during a partial nephrectomy. Figure 2 shows the kidney 3D model where the tumor is colored in yellow.\n\n## Pipeline\n\n<center> <img src=\"./docs/multiai_pipeline.png\" ></center>\n<center> Fig. 3: Schematic overview of Holoscan application </center><br>\n\nTowards realtime segmentation, AR overlay and anonymization, a binary segmentation model for non-organic items and a binary out-of-body detector was trained and deployed using Holoscan platform. Figure 3 shows a schematic overview of the application. After capturing the frame, the alpha channel is dropped by the [Format Converter](/operators/orsi/orsi_format_converter/format_converter.cpp) operator. Additionally, the black padding borders added by the robotic system are removed, the tensor is resized to 512x512 pixels and the tensor type is converted from int [0, 255] to float [0, 1]. In the [Segmentation Preprocessor](/operators/orsi/orsi_segmentation_preprocessor/segmentation_preprocessor.cpp) and Anonymization Preprocessor operators the tensor pixel values of every color channel are normalized using the corresponding means and standard deviations of the segmentation and anonymization dataset respectively. After model inference with the Multi-AI inference operator, a sigmoid layer is applied to the model predictions by the [Segmentation Postprocessor](/operators/orsi/orsi_segmentation_postprocessor/segmentation_postprocessor.cpp) resulting in a binary segmentation mask. Additionally, the resizing and cropping operations are inverted by first resizing and then adding black padding borders as to not compromise resolution. In the [Orsi Visualizer](/operators/orsi/orsi_visualizer/orsi_visualizer.cpp) operator the 3D model is rendered using the VTK library and composited onto the endoscopic image using OpenGL. The pixels corresponding to non-organic items are passed through the 3D model layer using a glsl shader program. Finally, the frame is anonymized according to the out-of-body detector output.\n\n\n##  Controls\n\n| Action | Control |\n| -------- | ------- |\n| Enable/Disable anonymization | B |\n| Enable/Disable manipulations | T |\n| Load 3D model orientation preset | CTRL + L |\n| Save current 3D model orientation as preset **(will overwrite default preset)** | CTRL + S |\n| Rotate 3D model (3 degrees of freedom)  | Left Click + Drag |\n| Rotate 3D model (1 degree of freedom) | CTRL + Left Click + Drag |\n| Zoom 3D model | Right Click + Drag |\n| Translate 3D  model | SHIFT + Left Click + Drag |\n| Enable/Disable 3D model | E |\n| Enable/Disable segmentation overlay | O |\n| Increase opacity 3D model | + |\n| Decrease opacity 3D model | - |\n| Toggle arterial tree | 1 |\n| Toggle venous tree | 2 |\n| Toggle ureter | 4 |\n| Toggle parenchyma | 5 |\n| Toggle tumor | 6 |\n\n\n## Build app\n\n```bash\n./run build orsi_multi_ai_ar\n```\n\n## Launch app\n\n```bash\n./run launch orsi/orsi_multi_ai_ar cpp\n```\n\n<center> <img src=\"./docs/orsi_logo.png\" width=\"400\"></center>",
        "application_name": "orsi",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "High Speed Endoscopy",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/high_speed_endoscopy.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# High-Speed Endoscopy\n\nThe application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.\n\n### Requirements\n\nThis application requires:\n1. an Emergent Vision Technologies camera (see [setup instructions]((https://docs.nvidia.com/clara-holoscan/sdk-user-guide/emergent_setup.html)\n2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see [prerequisites](../../README.md#prerequisites))\n3. a display with high refresh rate to keep up with the camera's framerate\n4. [additional setups](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/additional_setup.html) to reduce latency\n\n### Run Instructions\n\nTODO\n",
        "application_name": "high_speed_endoscopy",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "High Speed Endoscopy",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/high_speed_endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# High-Speed Endoscopy\n\nThe application showcases how high resolution cameras can be used to capture the scene, post-processed on GPU and displayed at high frame rate.\n\n### Requirements\n\nThis application requires:\n1. an Emergent Vision Technologies camera (see [setup instructions]((https://docs.nvidia.com/clara-holoscan/sdk-user-guide/emergent_setup.html)\n2. a NVIDIA ConnectX SmartNIC with Rivermax SDK and drivers installed (see [prerequisites](../../README.md#prerequisites))\n3. a display with high refresh rate to keep up with the camera's framerate\n4. [additional setups](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/additional_setup.html) to reduce latency\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n> \u26a0\ufe0f At this time, camera controls are hardcoded within the `gxf_emergent_source` extension. To update them at the application level, the GXF extension, and the application need to be rebuilt.\nFor more information on the controls, refer to the [EVT Camera Attributes Manual](https://emergentvisiontec.com/resources/?tab=umg)\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory. Then, run the commands of your choice:\n\n* RDMA disabled\n    ```bash\n    # C++\n    sed -i -e 's#rdma:.*#rdma: false#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n        && sudo ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n    ```\n\n* RDMA enabled\n    ```bash\n    # C++\n    sed -i -e 's#rdma:.*#rdma: true#' ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy.yaml \\\n        && sudo MELLANOX_RINGBUFF_FACTOR=14 ./applications/high_speed_endoscopy/cpp/high_speed_endoscopy\n    ```\n\n> \u2139\ufe0f The `MELLANOX_RINGBUFF_FACTOR` is used by the EVT driver to decide how much BAR1 size memory would be used on the dGPU. It can be changed to different number based on different use cases.\n",
        "application_name": "high_speed_endoscopy",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "FM-ASR",
            "authors": [
                {
                    "name": "Joshua Martinez",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.4.1",
                "tested_versions": [
                    "0.4.1",
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Signal Processing",
                "NLP",
                "ASR",
                "Automatic Speech Recognition"
            ],
            "ranking": 3,
            "dependencies": {
                "libraries": [
                    {
                        "name": "cusignal",
                        "version": "23.06"
                    },
                    {
                        "name": "nvidia-riva-client",
                        "version": "2.10.0"
                    },
                    {
                        "name": "rtl-sdr",
                        "version": "0.6.0-3"
                    },
                    {
                        "name": "NVIDIA Riva",
                        "container": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/resources/riva_quickstart"
                    }
                ]
            },
            "run": {
                "command": "./fm_asr_app.py holoscan.yml",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# FM ASR\nThis project is proof-of-concept demo featuring the combination of real-time, low-level signal processing and deep learning inference. It currently supports the [RTL-SDR](https://www.rtl-sdr.com/). Specifically, this project demonstrates the demodulation, downsampling, and automatic transcription of live, civilian FM radio broadcasts. The pipeline architecture is shown in the figure below. \n\n![Pipeline Architecture](docs/images/pipeline_arch.png)\n\nThe primary pipeline segments are written in Python. Future improvements will introduce a fully C++ system.\n\nThis project leverages NVIDIA's [Holoscan SDK](https://github.com/nvidia-holoscan/holoscan-sdk) for performant GPU pipelines, cuSignal package for GPU-accelerated signal processing, and the [RIVA SDK](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/overview.html) for high accuracy automatic speech recognition (ASR).\n\n## Table of Contents\n- [Install](#install)    \n    - [Local Sensor](#local-sensor---basic-configuration)\n        - [x86](#local-sensor---basic-configuration)\n        - [Jetson - TODO](#local-jetson-container)\n    - [Remote Sensor - TODO](#remote-sensor---network-in-the-loop)\n        - [x86](#remote-sensor---network-in-the-loop)\n        - [Jetson](#remote-jetson-containers)  \n    - [Bare Metal - TODO](#bare-metal-install)  \n- [Startup](#startup)\n    - [Scripted Launch](#scripted-launch)\n    - [Manual Launch](#manual-launch)\n- [Configuration Parameters](#configuration-parameters)\n- [Known Issues](#known-issues)\n\n## Install\nTo begin installation, clone this repository using the following:\n```bash\ngit clone https://github.com/nvidia-holoscan/holohub.git\n```\nNVIDIA Riva is required to perform the automated transcriptions. You will need to install and configure the [NGC-CLI](https://ngc.nvidia.com/setup/installers/cli) tool, if you have not done so already, to obtain the Riva container and API. The Riva installation steps may be found at this link: [Riva-Install](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html). Note that Riva performs a TensorRT build during setup and requires access to the targeted GPU. \nThis project has been tested with RIVA 2.10.0.\n\nContainer-based development and deployment is supported. The supported configurations are explained in the sections that follow. \n\n### Local Sensor - Basic Configuration\nThe Local Sensor configuration assumes that the RTL-SDR is connected directly to the GPU-enabled system via USB. I/Q samples are collected from the RTL-SDR directly, using the SoapySDR library. Specialized containers are provided for Jetson devices.\n\nOnly two containers are used in this configuration: \n- The Application Container which includes all the necessary low level libraries, radio drivers, Holoscan SDK for the core application pipeline, and the Riva client API; and\n- The Riva SDK container that houses the ASR transcription service.\n\n![LocalSensor](./docs/images/Local-Sensor-Arch.png)\n\nFor convenience, container build scripts are provided to automatically build the application containers for Jetson and x86 systems. The Dockerfiles can be readily modified for ARM based systems with a discrete GPU. To build the container for this configuration, run the following:\n```bash\n# Starting from FM-ASR root directory\ncd scripts\n./build_application_container.sh # builds Application Container\n```\nNote that this script does not build the Riva container.\n\nA script for running the application container is also provided. The run scripts will start the containers and leave the user at a bash terminal for development. Separate launch scripts are provided to automatically run the application.\n```bash\n# Starting from FM-ASR root directory\n./scripts/run_application_container.sh\n```\n\n#### Local Jetson Container\nHelper scripts will be provided in a future release.\n\n\n### Remote Sensor - Network in the Loop\nThis configuration is currently in work and will be provided in a future release. Developers can modify this code base to support this configuration if desired.\n\n### Bare Metal Install\nWill be added in the future. Not currently supported.\n\n## Startup\nAfter installation, the following steps are needed to launch the application:\n1. Start the Riva ASR service\n2. Launch the Application Container\n\n### Scripted Launch\nThe above steps are automated by some helper scripts.\n```bash\n# Starting from FM-ASR root directory\n./scripts/lauch_application.sh # Starts Application Container and launches app using the config file defined in the script\n\n```\n### Manual Launch\nAs an alternative to `launch_application.sh`, the FM-ASR pipeline can be run from inside the Application Container using the following commands:\n```bash\ncd /workspace\nexport CONFIG_FILE=/workspace/params/holoscan.yml # can be edited by user\npython fm_asr_app.py $CONFIG_FILE\n```\n\n### Initialize and Start the Riva Service\nRiva can be setup following the [Quickstart guide](https://docs.nvidia.com/deeplearning/riva/user-guide/docs/quick-start-guide.html#) (version 2.10.0 currently supported). To summarize it, run the following:\n```bash\ncd <riva_quickstart_download_directory>\nbash riva_init.sh\nbash riva_start.sh\n```\nThe initialization step will take a while to complete but only needs to be done once. Riva requires a capable GPU to setup and run properly. If your system has insufficient resources, the initialization script may hang. \n\nWhen starting the service, Riva may output a few \"retrying\" messages. This is normal and not an indication that the service is frozen. You should see a message saying ```Riva server is ready...``` once successful. \n\n*Note for users with multiple GPUs:*\n\nIf you want to specify which GPU Riva uses (defaults to device 0), open and edit ```<riva_quickstart_download_directory>/config.sh```, then change line\n```bash\ngpus_to_use=\"device=0\"\n```\nto\n```bash\ngpus_to_use=\"device=<your-device-number>\"\n# or, to guarantee a specific device\ngpus_to_use=\"device=<your-GPU-UUID>\"\n```\nYou can determine your GPUs' UUIDs by running ```nvidia-smi -L```.\n\n## Configuration Parameters\nA table of the configuration parameters used in this project is shown below, organized by application operator.\n\n| Parameter | Type | Description |\n| --------- | ---- |  ----------- |\n| run_time | int | Number of seconds that pipeline will execute |\n| RtlSdrGeneratorOp|||\n| sample_rate | float | Reception sample rate used by the radio. RTL-SDR max stable sample rate without dropping is 2.56e6.|\n| tune_frequency | float | Tuning frequency for the radio in Hz. |\n| gain | float | 40.0 | Gain applied to received signal. Max for RTL-SDR is 40. |\n| PlayAudioOp |||\n| play_audio | bool | Flag used to enable simultaneous audio playback of signal. |\n| RivaAsrOp |||\n| sample_rate | int | Audio sample rate expected by the Riva ASR model. Riva default is to 16000, other values will incurr an additional resample operation within Riva. |\n| max_alternatives | int | Riva - Maximum number of alternative transcripts to return (up to limit configured on server). Setting to 1 returns only the best response. |\n| word-time-offsets | bool | Riva - Option to output word timestamps in transcript.|\n| automatic-punctuation | bool | Riva - Flag that controls if transcript should be automatically punctuated. |\n| uri | str | localhost:50051 | Riva - URI/IP address to access the Riva server. Must match IP that Riva service was configured with. Default is localhost:50051. |\n| no-verbatim-transcripts | bool | Riva - If specified, text inverse normalization will be applied |\n| boosted_lm_words | str | Riva - words to boost when decoding. Useful for handling jargon and acronyms. |\n| boosted_lm_score | float | Value by which to boost words when decoding |\n| language-code | str | Riva - Language code of the model to be used. US English is en-US. Check Riva docs for more options|\n| interim_transcriptions | bool | Riva - Flag to include interim transcriptions in the output file. |\n| ssl_cert | str | Path to SSL client certificates file. Not currently utilized |\n| use_ssl | bool | Boolean to control if SSL/TLS encryption should be used. Not currently utilized. |\n| recognize_interval| int | Specifies the amount of data RIVA processes per request, in time (s). |\n| TranscriptSinkOp |||\n| output_file | str | File path to store a transcript. Existing files will be overwritten. |\n\n\n#### Known Issues\nThis table will be populated as issues are identified.\n\n| Issue | Description | Status|\n| ----- | ----------- | ---|\n\n\n",
        "application_name": "fm_asr",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "WebRTC Holoviz Server",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Server",
                "Holoviz"
            ],
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    },
                    {
                        "name": "aiohttp",
                        "version": "3.8.5"
                    },
                    {
                        "name": "numpy",
                        "version": "1.23"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/webrtc_server.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# WebRTC Holoviz Server\n\n![](screenshot.png)<br>\n\nThis app generates video frames with user specified content using Holoviz and sends it to a browser using WebRTC. The goal is to show how to remote control operators and view the output of a Holoscan pipeline.\n\nThe app starts a web server, the pipeline starts when a browser is connected to the web server and the `Start` button is pressed. The pipeline stops when the `Stop` button is pressed.\n\nThe web page has user inputs for specifying text and for the speed the text moves across the screen.\n\n```mermaid\nflowchart LR\n    subgraph Server\n        GeometryGenerationOp --> HolovizOp\n        HolovizOp --> FormatConverterOp\n        FormatConverterOp --> WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer <--> Browser\n        WebRTCServerOp <--> Browser\n    end\n```\n\n> **_NOTE:_** When using VPN there might be a delay of several seconds between pressing the `Start` button and the first video frames are display. The reason for this is that the STUN server `stun.l.google.com:19302` used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.\n\n## Prerequisites\n\nThe app is using [AIOHTTP](https://docs.aiohttp.org/en/stable/) for the web server and [AIORTC](https://github.com/aiortc/aiortc) for WebRTC. Install both using pip.\n\n```bash\npip install aiohttp aiortc\n```\n\n## Run Instructions\n\nRun the command:\n\n```bash\n./run launch webrtc_holoviz_server\n```\n\nOn the same machine open a browser and connect to `127.0.0.1:8080`. You can also connect from a different machine by connecting to the IP address the app is running on.\n\nPress the `Start` button. Video frames are displayed. To stop, press the `Stop` button. Pressing `Start` again will continue the video.\n\nChange the text input and the speed slider to control the generated video frame content.\n\n### Command Line Arguments\n\n```\nusage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:<ip>:<port>[<username>:<password>]` or `stun:<ip>:<port>`. This option can be specified multiple times to add multiple ICE servers.\n```\n\n\n## Running With TURN server\n\nA TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.\n\nRun the TURN server in the same machine that you're running the app on.\n\n**Note: It is strongly recommended to run the TURN server with docker network=host for best performance**\n\n```\n# This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"<ip>\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n```\n\nThen you can pass in the TURN server config into the app\n\n```\npython webrtc_server.py --ice-server \"turn:<ip>:3478[admin:admin]\"\n```\n\nThis will enable you to access the webRTC browser application from different machines.",
        "application_name": "webrtc_holoviz_server",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Segmentation from MONAI Model Zoo Application",
            "authors": [
                {
                    "name": "Jin Li",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.1",
            "changelog": {
                "1.0": "Initial Release",
                "1.1": "Update TensorRTInferenceOp to InferenceOp"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "MONAI",
                "Endoscopy",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": []
            }
        },
        "readme": "# Endoscopic Tool Segmentation from MONAI Model Zoo\nThis endoscopy tool segmentation application runs the MONAI Endoscopic Tool Segmentation from [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation).\n\n\nThis HoloHub application has been verified on the GI Genius sandbox and is currently deployable to GI Genius Intelligent Endoscopy Modules. [GI Genius](https://www.cosmoimd.com/gi-genius/) is Cosmo Intelligent Medical Devices\u2019 AI-powered endoscopy system. This implementation by Cosmo Intelligent Medical Devices showcases the fast and seamless deployment of HoloHub applications on products/platforms running on NVIDIA Holoscan.\n\n## Model\nWe will be deploying the endoscopic tool segmentation model from [MONAI Model Zoo](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation). <br>\nNote that you could also use the MONAI model zoo repo for training your own semantic segmentation model with your own data, but here we are directly deploying the downloaded MONAI model checkpoint into Holoscan. \n\n\n### Model conversion to ONNX\nBefore deploying the MONAI Model Zoo's trained model checkpoint in Holoscan SDK, we convert the model checkpoint into ONNX. <br>\nYou can choose to \n- download the [MONAI Endoscopic Tool Segmentation Model on NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/monai_endoscopic_tool_segmentation_model) directly and skip the rest of this Model section, or \n- go through the following conversion steps yourself. \n\n 1. Download the PyTorch model checkpoint linked in the README of [endoscopic tool segmentation](https://github.com/Project-MONAI/model-zoo/tree/dev/models/endoscopic_tool_segmentation#model-overview). We will assume its name to be `model.pt`.\n 2. Clone the MONAI Model Zoo repo. \n```\ncd [your-workspace]\ngit clone https://github.com/Project-MONAI/model-zoo.git\n```\nand place the downloaded PyTorch model into `model-zoo/models/endoscopic_tool_segmentation/`.\n\n 3. Pull and run the docker image for [MONAI](https://hub.docker.com/r/projectmonai/monai). We will use this docker image for converting the PyTorch model to ONNX. \n```\ndocker pull projectmonai/monai\ndocker run -it --rm --gpus all -v [your-workspace]/model-zoo:/workspace/model-zoo -w /workspace/model-zoo/models/endoscopic_tool_segmentation/ projectmonai/monai\n```\n 4. Install onnxruntime within the container\n ```\npip install onnxruntime onnx-graphsurgeon\n ```\n 5. Convert model\n \nWe will first export the model.pt file to ONNX by using the [export_to_onnx.py](https://github.com/Project-MONAI/model-zoo/blob/dev/models/endoscopic_tool_segmentation/scripts/export_to_onnx.py) file. Modify the backbone in [line 122](https://github.com/Project-MONAI/model-zoo/blob/dev/models/endoscopic_tool_segmentation/scripts/export_to_onnx.py#L122) to be efficientnet-b2:\n```\nmodel = load_model_and_export(modelname, outname, out_channels, height, width, multigpu, backbone=\"efficientnet-b2\")\n```\nNote that the model in the Model Zoo here was trained to have only two output channels: label 1 = tools, label 0 = everything else, but the same Model Zoo repo can be repurposed to train a model with a different dataset that has more than two classes.\n```\npython scripts/export_to_onnx.py --model model.pt --outpath model_endoscopic_tool_seg.onnx --width 736 --height 480 --out_channels 2\n```\nFold constants in the ONNX model.\n```\npolygraphy surgeon sanitize --fold-constants model_endoscopic_tool_seg.onnx -o model_endoscopic_tool_seg_sanitized.onnx\n```\nFinally, modify the input and output channels to have shape [n, height, width, channels], [n, channels, height, width]. \n```\npython scripts/graph_surgeon_tool_seg.py --orig_model model_endoscopic_tool_seg_sanitized.onnx --new_model model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx\n```\n\n## Data\nFor this application we will use the same [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) as the Holoscan SDK reference applications.\n\n## Requirements\nThe only requirement is to make sure the model and data are accessible by the application. At runtime we will need to specify via the `--data` arg, assuming the directory specified contains two subdirectories `endoscopy/` (endoscopy video data directory) and `monai_tool_seg_model/` (model directory).\n\n## Running the application\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\nNext, run the application, where <DATA_DIR> is a directory that contains two subdirectories `endoscopy/` and `monai_tool_seg_model/`.:\n\n```\npython3 tool_segmentation.py --data <DATA_DIR>\n```\nIf you'd like the application to run at the input framerate, change the `replayer` config in the yaml file to `realtime: true`.",
        "application_name": "monai_endoscopic_tool_seg",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Yolo Detection Application",
            "authors": [
                {
                    "name": "Meiran Peng",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Yolo",
                "bounding box"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.22.3"
                    },
                    {
                        "name": "cupy",
                        "version": "11.3"
                    }
                ]
            },
            "run": {
                "command": "python3 yolo_detection --data=./ --source=replayer --video_dir=./example_video",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Holoscan-Yolo\nThis project is aiming to provide basic guidance to deploy Yolo-based model to Holoscan SDK as \"Bring Your Own Model\"\n\nThe reference application's pipeline is the same as [Ultrasound Segmentation Application in Holoscan SDK](https://github.com/nvidia-holoscan/holohub/tree/main/applications/ultrasound_segmentation).\n\n\nThe operators applied in this operation including:\n- Video Stream Replayer (replayer as source) | AJASourceOp (AJA card is as source)\n- Format Converter (float32 + resize)\n- TensorRT Inference\n- Detection PostProcessor (customized Python Operator to extract Bounding Box)\n- HoloViz\n\nThis project includes below procedures:\n1. [Prerequisition](#prerequisition)\n    1.1 Prepare Holoscan SDK env\n    1.2 Prepare dependent libraries inside container.\n2. [Deploy Procedures](#procedures)\n    2.1 [Step1: prepare the model with NMS](#step-1-prepare-the-model-with-nms-layer-and-nms-plugin) depend on pytorch env, can be done outside Holoscan SDK, refer each models' installation env section.     \n    2.2 [Step2: Deployment](#step-2-deployment)\n        - Prepare Env\n        - Update Model with NHWC as input\n        - Prepare test video with gxf support format\n        - Update application with py/yaml\n        - Prepare working folder\n        - Run the Application.\n\nThis repo takes yolo v7 as an example.\n\n## Dependencies Repos\n- Holoscan SDK: https://github.com/nvidia-holoscan/holoscan-sdk\n- Yolo v7 repo: https://github.com/WongKinYiu/yolov7 \n- Yolo v8 repo: https://github.com/ultralytics/ultralytics \n- Yolo v8 export repo: https://github.com/triple-Mu/YOLOv8-TensorRT \n\n\n## Prerequisition\n- Holoscan environment:\n    - NGC container: [Holoscan container image on NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/containers/holoscan)\n\n- Install below libraries inside container via, or skip it to [Step 2: Deployment](#step-2-deployment)\n    ```\n        pip install onnxruntime\n        pip install nvidia-pyindex\n        pip install onnx-graphsurgeon\n        apt update\n        apt install ffmpeg\n    ```\n    - onnxruntime, nvidia-pyindex,onnx-graphsurgeon are needed for [graph_surgeon.py](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#graph_surgeonpy) scripts to do onnx model conversion. Since when converting a model from PyTorch to ONNX, it is likely that the input of the model is in the form NCHW (batch, channels, height, width), and needs to be converted to NHWC (batch, height, width, channels). \n    - cupy is used in custom operators for performance improvements\n    - numpy is used in script to convert video in [Step 2](#step-2-deployment)\n\n## Procedures\n### Step 1: Prepare the Model with NMS Layer and NMS Plugin\n- [Yolo_v8](https://github.com/triple-Mu/YOLOv8-TensorRT)\n    - ```git clone https://github.com/triple-Mu/YOLOv8-TensorRT.git```\n    - Installation dependent package as described in this repo. Recommend to use latest [PyTorch docker](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) for model export. \n    - Export the model\n        ```\n            wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt\n            python3 export-det.py --weights yolov8s.pt --iou-thres 0.65 --conf-thres 0.25 --topk 100 --opset 11 --sim --input-shape 1 3 640 640 --device cuda:0\n        ```\n    - Check the model via [netron](https://netron.app/) shall include EfficientNMS_TRT layer with output ```num_dets```, ```bboxes```, ```scores```, and ```labels```\n\n- [Yolo_v7](https://github.com/WongKinYiu/yolov7)\n    - ```git clone https://github.com/WongKinYiu/yolov7.git```\n    - Prepare env, docker recommended as [yolo v7 installation](https://github.com/WongKinYiu/yolov7#installation)\n    - Export the model\n        ```\n        wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\n        python3 export.py --weights ./yolov7-tiny.pt --grid --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 --img-size 640 640\n        ```\n        NOTE: the ```end2end``` parameter will output the the end to end model with NMS Layer and Plugin. Other parameters pls refer detail information via [link](https://github.com/WongKinYiu/yolov7/blob/main/export.py#L19)\n    - Check the model via [netron](https://netron.app/) shall include EfficientNMS_TRT layer with output ```num_dets```, ```det_boxes```, ```det_scores```, and ```det_classes```\n\nNOTE: The output name maybe different for different export models, pls correct yolo_detecton.yaml section \"output_binding_names\" in detection_inference part accordingly.\n\n### Step 2: Deployment \n- ```git clone ``` this repo to your local folder, e.g. ./holohub\n\n- ```cd ./holohub/applications/yolo_model_deployment```\n\n- Copy the onnx model with NMS in Step 1 to this application folder. (The onnx model name in this example is yolov8s.onnx for yolo v8 model, and yolov7-tiny.onnx for yolo v7 model)\n\n- Run the Holoscan container via\n    ```\n    # Update the ngc container image path as needed\n    export NGC_CONTAINER_IMAGE_PATH=\"nvcr.io/nvidia/clara-holoscan/holoscan:v0.6.0\"\n\n    # DISPLAY env may be different due to different settings,\n    # try DISPLAY=:1 if failure with \"Failed to open display :0\"\n    export DISPLAY=:0\n    xhost +local:docker\n\n    # Find the nvidia_icd.json file which could reside at different paths\n    # Needed due to https://github.com/NVIDIA/nvidia-container-toolkit/issues/16\n    nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f 2>/dev/null | grep .) || (echo \"nvidia_icd.json not found\" >&2 && false)\n\n    # Run the container\n    docker run -it --rm --net host \\\n    --runtime=nvidia \\\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\\n    -v $nvidia_icd_json:$nvidia_icd_json:ro \\\n    -v ${PWD}:/holohub-yolo \\\n    -w /holohub-yolo \\\n    -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display \\\n    -e DISPLAY=$DISPLAY \\\n    ${NGC_CONTAINER_IMAGE_PATH}\n    ```      \n\n- Install necessary libraries inside the container via:\n    _This section is intended for the initial preparation of the model and video, and can be skipped once the model has been prepared._   \n    ```\n    apt update\n    apt install ffmpeg\n    pip3 install --upgrade setuptools\n\n    cat requirement.txt | xargs -n 1 -L 1 pip3 install \n    ```     \n\n- Update the model with input from NCHW format to NHWC format inside the container    \n    _This section is intended for the initial preparation of the model and video, and can be skipped once the model has been prepared._      \n\n    If you are converting your model from PyTorch to ONNX, chances are your input is NCHW and will need to be converted to NHWC. We provide an example [transformation script on Github](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#graph_surgeonpy) named ```graph_surgeon.py``` to do such conversion. You may need to modify the dimensions as needed before modifying your model. For this example, use below command to do the conversion. \n\n    ```\n    python3 ./scripts/graph_surgeon_yolo.py [yolov8s.onnx] [yolov8-nms-update.onnx]\n    ```\n    NOTE: the yolov8s.onnx refer to input onnx model name, yolov8-nms-update.onnx is the updated model which will be used in yolo application for inference. The yolov8-nms-update.onnx will be used in yolo_detection.py, if the onnx model name changed, pls update yolo_detection.py ```self.model_file_path``` accordingly.\n\n- Prepare the Video with gxf format inside the container    \n    _This section is intended for the initial preparation of the model and video, and can be skipped once the model has been prepared._ \n    \n    Video files need to be converted into a GXF replayable tensor format to be used as stream inputs. This step has already been done for the sample applications. To do so for your own video data, we provide a [utility script on GitHub](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) named ```convert_video_to_gxf_entities.py```. This script should yield two files in .gxf_index and .gxf_entities formats, which can be used as inputs with Holoscan.\n    \n    Follow below procedures to convert the video. \n\n    1. Copy your video to the working directory. In this example to /holohub-yolo/example_video   \n    2. Take cars.mp4 from [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_cars_video) as an example. Download it to  /holohub-yolo/example_video\n        ```\n        git clone https://github.com/nvidia-holoscan/holoscan-sdk.git\n        cd ./example_video\n        ffmpeg -i cars.mp4 -pix_fmt rgb24 -f rawvideo pipe:1 | python3 ../holoscan-sdk/scripts/convert_video_to_gxf_entities.py --width 1920 --height 1080 --channels 3 --directory ./ --basename cars\n        ```\n\n        NOTE:\n        - ```basename``` will be used in VideoStreamReplayerOp, you can see yolo_detection.yaml, replayer section, the basename shall be defined as \"cars\" in this example, change the yaml file if needed.\n        - ```height``` and ```width``` are the height and the width of the video. \n\n\n- Run the application inside the container\n    Please make sure that the yolov8-nms-update.onnx file exists under the specified data path.\n    \n    ```\n    python3 ./yolo_detection.py  --data=<your data path> --source=replayer --video_dir=./example_video\n    ```\n\n## Results:\n![](docs/cars_yolo_v8.png)\n\n\n",
        "application_name": "yolo_model_deployment",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Simple Classical Radar Pipeline",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.4.0",
                "tested_versions": [
                    "0.4.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Aerospace, Defense, Communications"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.23.2"
                    },
                    {
                        "name": "cupy",
                        "version": "11.4"
                    },
                    {
                        "name": "cusignal",
                        "version": "22.12"
                    }
                ]
            },
            "run": {
                "command": "python3 simple_radar_pipeline.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Simple Radar Pipeline Application\n\nThis demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through:\n1. Pulse Compression\n2. Moving Target Indication (MTI) Filtering\n3. Range-Doppler Map\n4. Constant False Alarm Rate (CFAR) Analysis\n\nWhile this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the `SignalGeneratorOperator`.\n\nThe output of this demonstration is a measure of the number of pulses per second processed on GPU.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal\npip install holoscan\n```\n\nThe simple radar signal processing pipeline example can then be run via\n```\npython applications/simple_radar_pipeline/simple_radar_pipeline.py\n```\n",
        "application_name": "simple_radar_pipeline",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Simple Radar Pipeline in C++",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.4.0",
                "tested_versions": [
                    "0.4.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Signal Processing",
                "RADAR"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "MatX",
                        "version": "0.2.5",
                        "url": "https://github.com/NVIDIA/MatX.git"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/simple_radar_pipeline",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Simple Radar Pipeline Application\n\nThis demonstration walks the developer through building a simple radar signal processing pipeline, targeted towards detecting objects, with Holoscan. In this example, we generate random radar and waveform data, passing both through:\n1. Pulse Compression\n2. Moving Target Indication (MTI) Filtering\n3. Range-Doppler Map\n4. Constant False Alarm Rate (CFAR) Analysis\n\nWhile this example generates 'offline' complex-valued data, it could be extended to accept streaming data from a phased array system or simulation via modification of the `SignalGeneratorOperator`.\n\nThe output of this demonstration is a measure of the number of pulses per second processed on GPU.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators, particularly with handling multiple inputs and outputs into an Operator\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n## Building the application\nMake sure CMake (https://www.cmake.org) is installed on your system (minimum version 3.20)\n\n- [Holoscan Debian Package](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_dev_deb) - Follow the instructions in the link to install the latest version of Holoscan Debian package from NGC.\n\n- Create a build directory:\n  ```bash\n  mkdir -p <build_dir> && cd <build_dir>\n  ```\n- Configure with CMake:\n\n  Make sure CMake can find your installation of the Holoscan SDK. For example, setting `holoscan_ROOT` to its install directory during configuration:\n\n  ```bash\n  cmake -S <source_dir> -B <build_dir> -DAPP_simple_radar_pipeline=1 \n  ```\n\n  _Notes:_\n  _If the error `No CMAKE_CUDA_COMPILER could be found` is encountered, make sure that the :code:`nvcc` executable can be found by adding the CUDA runtime location to your `PATH` variable:_\n\n  ```\n  export PATH=$PATH:/usr/local/cuda/bin\n  ```\n\n- Build:\n\n  ```bash\n  cmake --build <build_dir>\n  ```\n\n## Running the application\n```bash\n<build_dir>/simple_radar_pipeline\n```\n\n",
        "application_name": "simple_radar_pipeline",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Out of Body Detection Pipeline in C++",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Classification"
            ],
            "ranking": 1,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection"
            },
            "run": {
                "command": "./endoscopy_out_of_body_detection endoscopy_out_of_body_detection.yaml --data <holohub_data_dir>/endoscopy_out_of_body_detection",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Endoscopy Out of Body Detection Application\n\nThis application performs endoscopy out of body detection. The application classifies if the input frame is inside the body or out of the body. If the input frame is inside the body, application prints `Likely in-body`, otherwise `Likely out-of-body`. Each likelihood is accompanied with a confidence score. \n\n__Note: there is no visualization component in the application.__\n\n`endoscopy_out_of_body_detection.yaml` is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the `basename` and the `directory` field in `replayer`.\n\n## Data\n\n__Note: the data is automatically downloaded and converted when building. If you need to manually convert the data follow the following steps.__\n\n\n* Endoscopy out of body detection model and the sample dataset is available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection)\n  After downloading the data in mp4 format, it must be converted into GXF tensors.\n* Script for GXF tensor conversion (`convert_video_to_gxf_entities.py`) is available with the Holoscan SDK, and can be accessed [here](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts)\n\n### Unzip and convert the sample data:\n\n```\n# unzip the downloaded sample data\nunzip [FILE].zip -d <data_dir>\n\n# convert video file into tensor\nffmpeg -i <INPUT_VIDEO_FILE> -fs 900M -pix_fmt rgb24 -f rawvideo pipe:1 | python convert_video_to_gxf_entities.py --width 256 --height 256 --channels 3 --framerate 30\n\n# where <INPUT_VIDEO_FILE> is one of the downloaded MP4 files: OP1-out-2.mp4, OP4-out-8.mp4 or OP8-out-4.mp4.\n```\n\nMove the model file and converted video tensor into a directory structure similar to the following:\n\n```bash\ndata\n\u2514\u2500\u2500 endoscopy_out_of_body_detection\n    \u251c\u2500\u2500 LICENSE.md\n    \u251c\u2500\u2500 out_of_body_detection.onnx\n    \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_entities\n    \u251c\u2500\u2500 sample_clip_out_of_body_detection.gxf_index\n    \u2514\u2500\u2500 sample_clip_out_of_body_detection.mp4\n```\n\n## Building the application\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\nAdditionally, if the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:\n\n* In main.cpp: `#include <holoscan/operators/inference/inference.hpp>` is replaced with `#include <holoscan/operators/multiai_inference/multiai_inference.hpp>`\n* In main.cpp: `#include <holoscan/operators/inference_processor/inference_processor.hpp>` is replaced with `#include <holoscan/operators/multiai_postprocessor/multiai_postprocessor.hpp>`\n* In main.cpp: `ops::InferenceOp` is replaced with `ops::MultiAIInferenceOp`\n* In main.cpp: `ops::InferenceProcessorOp` is replaced with `ops::MultiAIPostprocessorOp`\n* In CMakeLists.txt: update the holoscan SDK version from `0.6` to `0.5`\n* In CMakeLists.txt: `holoscan::ops::inference` is replaced with `holoscan::ops::multiai_inference`\n* In CMakeLists.txt: `holoscan::ops::inference_processor` is replaced with `holoscan::ops::multiai_postprocessor`\n\n## Running the application\n\nIn your `build` directory, run\n\n```bash\napplications/endoscopy_out_of_body_detection/endoscopy_out_of_body_detection --data ../data/endoscopy_out_of_body_detection\n```\n",
        "application_name": "endoscopy_out_of_body_detection",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Body Pose Estimation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Computer Vision",
                "Human Body Pose Estimation"
            ],
            "ranking": 2,
            "dependencies": {
                "model": "https://docs.ultralytics.com/tasks/pose/"
            },
            "run": {
                "command": "python3 ../applications/body_pose_estimation/body_pose_estimation.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Body Pose Estimation App\n<div align=\"center\">\n    <img src=\"./docs/1.png\" width=\"300\" height=\"375\">\n    <img src=\"./docs/2.png\" width=\"300\" height=\"375\">\n    <img src=\"./docs/3.png\" width=\"300\" height=\"375\">\n</div>\n\nBody pose estimation is a computer vision task that involves recognizing specific points on the human body in images or videos.\nA model is used to infer the locations of keypoints from the source video which is then rendered by the visualizer. \n\n## Model\n\nThis application uses YOLOv8 pose model from [Ultralytics](https://docs.ultralytics.com/tasks/pose/) for body pose estimation.\nThe model is downloaded when building the application.\n\n## Requirements\n\nThis application uses a v4l2 compatible device as input.  Please plug in your input device (e.g., webcam) and ensure that `applications/body_pose_estimation/body_pose_estimation.yaml` is set to the corresponding device.  The default input device is set to `/dev/video0`.\n\n## Run Instructions\n\nRun the following commands to start the body pose estimation application:\n```sh\n./dev_container build --docker_file applications/body_pose_estimation/Dockerfile --img holohub:bpe\n./dev_container launch --img holohub:bpe                                                         \n./run build body_pose_estimation\n./run launch body_pose_estimation\n```\n",
        "application_name": "body_pose_estimation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Object detection using frcnn based pytorch model in C++",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Object detection"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "./object_detection_torch --data <holohub_data_dir>/object_detection_torch",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Object Detection Application\n\nThis application performs object detection using frcnn resnet50 model from torchvision.\nThe inference is executed using `torch` backend in `holoinfer` module in Holoscan SDK.\n\n`object_detection_torch.yaml` is the configuration file. Input video file is converted into GXF tensors and the name and location of the GXF tensors are updated in the `basename` and the `directory` field in `replayer`.\n\nThis application need `Libtorch` for inferencing. Ensure that the Holoscan SDK is build with `build_libtorch` flag as true. If not, then rebuild the SDK with following: `./run build --build_libtorch true` before running this application.\n\n## Data\n\nTo run this application, you will need the following:\n\n- Model name: frcnn_resnet50_t.pt\n    - The model should be converted to torchscript format.  The original pytorch model can be downloaded from [pytorch model](https://pytorch.org/vision/main/models/generated/torchvision.models.detection.fasterrcnn_resnet50_fpn.html). `frcnn_resnet50_t.pt` is used\n- Model configuration file: frcnn_resnet50_t.yaml\n    - Model config documents input and output nodes, their dimensions and respective datatype.\n- Labels file: labels.txt\n    - Labels for identified objects.\n- Postprocessor configuration file: postprocessing.yaml\n    - This configuration stores the number and type of objects to be identified. By default, the application detects and generates bounding boxes for `car` (max 50), `person` (max 50), `motorcycle` (max 10) in the input frame. All remaining identified objects are tagged with label `object` (max 50).\n    - Additionally, color of the bounding box for each identified object can be set.\n    - Threshold of scores can be set in the `params`. Default value is 0.75.\n\nSample dataset can be any video file freely available for testing on the web. E.g. [Traffic video](https://www.pexels.com/video/cars-on-highway-854671/)\n\nOnce the video is downloaded, it must be [converted into GXF entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#usage). As shown in the command below, width and height is set to 1920x1080 by default. To reduce the size of generated tensors a lower resolution can be used. Generated entities must be saved at <data_dir>/object_detection_torch folder.\n\n```bash\nffmpeg -i <downloaded_video> -pix_fmt rgb24 -f rawvideo pipe:1 | python utilities/convert_video_to_gxf_entities.py --width 1920 --height 1080 --channels 3 --framerate 30\n```\n\nIf resolution is updated in entity generation, it must be updated in the following config files as well:\n<data_dir>/object_detection_torch/frcnn_resnet50_t.yaml\n<data_dir>/object_detection_torch/postprocessing.yaml\n\n## Building the application\n\nThe best way to run this application is inside the container, as it would provide all the required third-party packages:\n\n```bash\n# Create the container image for this application\n./dev_container build --docker_file applications/object_detection_torch/Dockerfile --img object_detection_torch\n# Launch the container\n./dev_container launch --img object_detection_torch\n# Build the application. Note that this downloads the video data as well\n./run build object_detection_torch\n# Generate the pytorch model\npython3 applications/object_detection_torch/generate_resnet_model.py  data/object_detection_torch/frcnn_resnet50_t.pt\n# Run the application\n./run launch object_detection_torch\n```\n\nPlease refer to the top level Holohub README.md file for more information on how to build this application.\n\n## Running the application\n\n```bash\n# ensure the current working directory contains the <data_dir>.\n<build_dir>/object_detection_torch\n```\n\nIf application is executed from within the holoscan sdk container and is not able to find `libtorch.so`, update `LD_LIBRARY_PATH` as below:\n\n```bash\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/libtorch/1.13.1/lib\n```\n\nOn aarch64, if application is executed from within the holoscan sdk container and libtorch throws linking errors, update the `LD_LIBRARY_PATH` as below:\n\n```bash\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:/opt/hpcx/ompi/lib\"\n```\n",
        "application_name": "object_detection_torch",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Power Spectral Density with cuNumeric",
            "authors": [
                {
                    "name": "Adam Thompson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Life Sciences, Aerospace, Defense, Communications"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.24.2"
                    },
                    {
                        "name": "cupy",
                        "version": "12.0"
                    },
                    {
                        "name": "cunumeric",
                        "version": "23.03"
                    }
                ]
            }
        },
        "readme": "# Calculate Power Spectral Density with Holoscan and cuNumeric\n\n[cuNumeric](https://github.com/nv-legate/cunumeric) is an drop-in replacement for NumPy that aims to provide a distributed and accelerated drop-in replacement for the NumPy API on top of the [Legion](https://legion.stanford.edu/) runtime. It works best for programs that have very large arrays of data that can't fit in the the memory of a single GPU or node.\n\nIn this example application, we are using the cuNumeric library within a Holoscan application graph to determine the Power Spectral Density (PSD) of an incoming signal waveform. Notably, this is simply achieved by taking the absolute value of the FFT of a data array.\n\n The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and cuNumeric\n- Demonstrate how to scale a given workload to multiple GPUs using cuNumeric\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies. This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-cunumeric-demo python=3.9\nconda activate holoscan-cunumeric-demo\nconda install -c nvidia -c conda-forge -c legate cunumeric cupy\npip install holoscan\n```\n\nThe cuNumeric PSD processing pipeline example can then be run via\n```\nlegate --gpus 2 applications/cunumeric_integration/cunumeric_psd.py\n```\n\nWhile running the application, you can confirm multi GPU utilization via watching `nvidia-smi` or using another GPU utilization tool\n\nTo run the same application without cuNumeric, simply change `import cunumeric as np` to `import cupy as np` in the code and run\n```\npython applications/cunumeric_integration/cunumeric_psd.py\n```\n",
        "application_name": "cunumeric_integration",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "OpenIGTLink 3D Slicer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Streaming",
                "Ethernet",
                "3DSlicer",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "openigtlink",
                        "version": "3.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/openigtlink_3dslicer.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Holoscan SDK as an Inference Backend for 3D Slicer\n\nThis application demonstrates how to interface Holoscan SDK with [3D Slicer](https://www.slicer.org/), using the [OpenIGTLink protocol](http://openigtlink.org/). The application is shown in the application graph below.\n\n![](./images/openigtlink_3dslicer_graph.png)\n\nIn summary, the `openigtlink` transmit and receive operators are used in conjunction with an AI segmentation pipeline to:\n\n1. Send Holoscan sample video data from a node running Holoscan SDK, using `OpenIGTLinkTxOp`, to 3D Slicer running on a different node (simulating a video source connected to 3D Slicer):\n    * For `cpp` application, the ultrasound sample data is sent.\n    * For `python` application, the colonoscopy sample data is sent.\n2. Transmit the video data back to Holoscan SDK using OpenIGTLinkIF Module, and receive the data with the `OpenIGTLinkRxOp` operator.\n3. Perform an AI segmentation pipeline in Holoscan:\n    * For `cpp` application, the ultrasound segmentation model is deployed.\n    * For `python` application, the colonoscopy segmentation model is deployed.\n4. Use Holoviz in `headless` mode to render image and segmentation and then send the data back to 3D Slicer using the `OpenIGTLinkTxOp` operator.\n\nThis workflow allows for sending image data from 3D Slicer over network to Holoscan SDK (running on either `x86` or `arm`), do some compute task (e.g., AI inference), and send the results back to 3D Slicer for visualization. Nodes can run distributed; for example, Holoscan SDK can run on an IGX Orin (Node A) sending the video data, 3D Slicer on a Windows laptop (Node B) and the AI inference pipeline on yet another machine (Node C). Also, note that the `openigtlink` operators can connect to any software/library that supports the OpenIGTLink protocol; here, 3D Slicer is used as it is a popular open source software package for image analysis and scientific visualization.\n\nFor the `cpp` application, which does ultrasound segmentations the results look like\n\n![](./images/cpp_ultrasound.png)\n\nand for the `python` application, which does colonoscopy segmentation, the results look like\n\n![](./images/python_colonoscopy.png)\n\nwhere the image data before Holoscan processing is shown in the left slice view, and the image data with segmentation overlay (after Holoscan processing) is shown in the right slice view.\n\n## Run Instructions\n\n### Machine running 3D Slicer\n\nOn the machine running 3D Slicer do:\n1. In 3D Slicer, open the Extensions Manager and install the `SlicerOpenIGTLink` extension.\n2. Next, load the scene `openigtlink_3dslicer/scene/openigtlink_3dslicer.mrb` into 3D Slicer.\n3. Go to the `OpenIGTLinkIF` module and make sure that the `SendToHoloscan` connector has the IP address of the machine running Holoscan SDK in the *Hostname* input box (under *Properties*).\n4. Then activate the two connectors `ReceiveFromHoloscan` and `SendToHoloscan` (click *Active* check box under *Properties*).\n\n### Machine running Holoscan SDK\n\nOn the machine running Holoscan SDK do the below steps.\n\nFirst, ensure that the `host_name` parameters of the two `OpenIGTLinkRxOp` operators (`openigtlink_tx_slicer_img` and `openigtlink_tx_slicer_holoscan`) have the IP address of the machine running 3D Slicer.\n\nNext, the application requires [OpenIGTLink](http://openigtlink.org/). For simplicity a DockerFile is available. To generate the container run:\n```sh\n./dev_container build --docker_file ./applications/openigtlink_3dslicer/Dockerfile --img holohub:openigtlink\n```\n\nThe application can then be built by launching this container and using the provided `run` script:\n```sh\n./dev_container launch --img holohub:openigtlink\n./run build openigtlink_3dslicer\n```\n\nThen, to run the `python` application do:\n```sh\n./run launch openigtlink_3dslicer python\n```\nand to run the `cpp` application do:\n```sh\n./run launch openigtlink_3dslicer cpp\n```",
        "application_name": "openigtlink_3dslicer",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "OpenIGTLink 3D Slicer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Streaming",
                "Ethernet",
                "3DSlicer",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "openigtlink",
                        "version": "3.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/openigtlink_3dslicer",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Holoscan SDK as an Inference Backend for 3D Slicer\n\nThis application demonstrates how to interface Holoscan SDK with [3D Slicer](https://www.slicer.org/), using the [OpenIGTLink protocol](http://openigtlink.org/). The application is shown in the application graph below.\n\n![](./images/openigtlink_3dslicer_graph.png)\n\nIn summary, the `openigtlink` transmit and receive operators are used in conjunction with an AI segmentation pipeline to:\n\n1. Send Holoscan sample video data from a node running Holoscan SDK, using `OpenIGTLinkTxOp`, to 3D Slicer running on a different node (simulating a video source connected to 3D Slicer):\n    * For `cpp` application, the ultrasound sample data is sent.\n    * For `python` application, the colonoscopy sample data is sent.\n2. Transmit the video data back to Holoscan SDK using OpenIGTLinkIF Module, and receive the data with the `OpenIGTLinkRxOp` operator.\n3. Perform an AI segmentation pipeline in Holoscan:\n    * For `cpp` application, the ultrasound segmentation model is deployed.\n    * For `python` application, the colonoscopy segmentation model is deployed.\n4. Use Holoviz in `headless` mode to render image and segmentation and then send the data back to 3D Slicer using the `OpenIGTLinkTxOp` operator.\n\nThis workflow allows for sending image data from 3D Slicer over network to Holoscan SDK (running on either `x86` or `arm`), do some compute task (e.g., AI inference), and send the results back to 3D Slicer for visualization. Nodes can run distributed; for example, Holoscan SDK can run on an IGX Orin (Node A) sending the video data, 3D Slicer on a Windows laptop (Node B) and the AI inference pipeline on yet another machine (Node C). Also, note that the `openigtlink` operators can connect to any software/library that supports the OpenIGTLink protocol; here, 3D Slicer is used as it is a popular open source software package for image analysis and scientific visualization.\n\nFor the `cpp` application, which does ultrasound segmentations the results look like\n\n![](./images/cpp_ultrasound.png)\n\nand for the `python` application, which does colonoscopy segmentation, the results look like\n\n![](./images/python_colonoscopy.png)\n\nwhere the image data before Holoscan processing is shown in the left slice view, and the image data with segmentation overlay (after Holoscan processing) is shown in the right slice view.\n\n## Run Instructions\n\n### Machine running 3D Slicer\n\nOn the machine running 3D Slicer do:\n1. In 3D Slicer, open the Extensions Manager and install the `SlicerOpenIGTLink` extension.\n2. Next, load the scene `openigtlink_3dslicer/scene/openigtlink_3dslicer.mrb` into 3D Slicer.\n3. Go to the `OpenIGTLinkIF` module and make sure that the `SendToHoloscan` connector has the IP address of the machine running Holoscan SDK in the *Hostname* input box (under *Properties*).\n4. Then activate the two connectors `ReceiveFromHoloscan` and `SendToHoloscan` (click *Active* check box under *Properties*).\n\n### Machine running Holoscan SDK\n\nOn the machine running Holoscan SDK do the below steps.\n\nFirst, ensure that the `host_name` parameters of the two `OpenIGTLinkRxOp` operators (`openigtlink_tx_slicer_img` and `openigtlink_tx_slicer_holoscan`) have the IP address of the machine running 3D Slicer.\n\nNext, the application requires [OpenIGTLink](http://openigtlink.org/). For simplicity a DockerFile is available. To generate the container run:\n```sh\n./dev_container build --docker_file ./applications/openigtlink_3dslicer/Dockerfile --img holohub:openigtlink\n```\n\nThe application can then be built by launching this container and using the provided `run` script:\n```sh\n./dev_container launch --img holohub:openigtlink\n./run build openigtlink_3dslicer\n```\n\nThen, to run the `python` application do:\n```sh\n./run launch openigtlink_3dslicer python\n```\nand to run the `cpp` application do:\n```sh\n./run launch openigtlink_3dslicer cpp\n```",
        "application_name": "openigtlink_3dslicer",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Videomaster transmitter example",
            "authors": [
                {
                    "name": "Laurent Radoux",
                    "affiliation": "DELTACAST"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Deltacast",
                "VideoMaster"
            ],
            "ranking": 2,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/deltacast_transmitter --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Deltacast transmitter\n\nThis application demonstrates the use of videomaster_transmitter to transmit a video stream through a dedicated IO device.\n\n### Requirements\n\nThis application uses the DELTACAST.TV capture card for input stream. Contact [DELTACAST.TV](https://www.deltacast.tv/) for more details on how access the SDK and to setup your environment.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nSee instructions from the top level README on how to build this application.\nNote that this application requires to provide the VideoMaster_SDK_DIR if it is not located in a default location on the system.\nThis can be done with the following command, from the top level Holohub source directory:\n\n```bash\n./run build deltacast_transmitter --configure-args -DVideoMaster_SDK_DIR=<Path to VideoMasterSDK>\n```\n\n### Run Instructions\n\nFrom the build directory, run the command:\n\n```bash\n./applications/deltacast_transmitter/deltacast_transmitter --data <holohub_data_dir>/endoscopy\n```\n",
        "application_name": "deltacast_transmitter",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Advanced Networking Benchmark",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "UDP",
                "Ethernet",
                "IP",
                "TCP"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "adv_network_rx",
                        "version": "1.0"
                    },
                    {
                        "name": "adv_network_tx",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/main.py",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "",
        "application_name": "adv_networking_bench",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Advanced Networking Benchmark",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.3",
            "changelog": {
                "1.3": "Allow app to have buffer multiple of ANO buffer",
                "1.2": "GPUDirect TX",
                "1.1": "GPUDirect mode without header data split"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "DPDK",
                "UDP",
                "Ethernet",
                "IP",
                "GPUDirect",
                "RDMA"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "advanced_networking_benchmark",
                        "version": "1.2"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/adv_networking_bench",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Advanced Networking Benchmark\n\nThis is a simple application to measure a lower bound on performance for the advanced networking operator\nby receiving packets, optionally doing work on them, and freeing the buffers. While only freeing the packets is\nan unrealistic workload, it's useful to see at a high level whether the application is able to keep up with\nthe bare minimum amount of work to do. The application contains both a transmitter and receiver that are\ndesigned to run on different systems, and may be configured independently.\n\nThe performance of this application depends heavily on a properly-configured system and choosing the best\ntuning parameters that are acceptable for the workload. To configure the system please see the documentation\nfor the advanced network operator. With the system tuned, the application performance will be dictated\nby batching size and whether GPUDirect is enabled. \n\nAt this time both the transmitter and receiver are written to handle an Ethernet+IP+UDP packet with a\nconfigurable payload. Other modes may be added in the future. Also, for simplicity, the transmitter and\nreceiver must be configured to a single packet size.\n\n## Transmit\n\nThe transmitter sends a UDP packet with an incrementing sequence of bytes after the UDP header. The batch\nsize configured dictates how many packets the benchmark operator sends to the advanced network operator\nin each tick. Typically with the same number of CPU cores the transmitter will run faster than the receiver, \nso this parameter may be used to throttle the sender somewhat by making the batches very small.\n\n## Receiver\n\nThe receiver receives the UDP packets in either CPU-only mode or header-data split mode. CPU-only mode\nwill receive the packets in CPU memory, copy the payload contents to a host-pinned staging buffer, and\nfreed. In header-data split mode the user may configure a split point where the bytes before that point\nare sent to the CPU, and all bytes afterwards are sent to the GPU. Header-data split should achieve higher\nrates than CPU mode since the amount of data to the CPU can be orders of magnitude lower compared to running\nin CPU-only mode. \n\n### Configuration\n\nThe application is configured using a separate transmit and receive file. The transmit file is called\n`adv_networking_bench_tx.yaml` while the receive is named `adv_networking_bench_rx.yaml`. Configure the\nadvanced networking operator on both transmit and receive per the instructions for that operator.\n\n#### Receive Configuration\n\n- `header_data_split`: bool\n  Turn on GPUDirect header-data split mode\n- `batch_size`: integer\n  Size in packets for a single batch. This should be a multiple of the advanced network RX operator batch size.\n  A larger batch size consumes more memory since any work will not start unless this batch size is filled. Consider\n  reducing this value if errors are occurring.\n- `max_packet_size`: integer\n  Maximum packet size expected. This value includes all headers up to and including UDP.\n\n#### Transmit Configuration\n\n- `batch_size`: integer\n  Size in packets for a single batch. This batch size is used to send to the advanced network TX operator, and \n  will loop sending that many packets for each burst.\n- `payload_size`: integer\n  Size of the payload to send after all L2-L4 headers \n\n### Requirements\n\nThis application requires all configuration and requirements from the advanced network operator.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nFirst, go in your `build` or `install` directory, then for the transmitter run:\n\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_tx.yaml\n```\n\nOr for the receiver:\n\n```bash\n./build/applications/adv_networking_bench/cpp/adv_networking_bench adv_networking_bench_tx.yaml\n```\n",
        "application_name": "adv_networking_bench",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Orthorectification with OptiX",
            "authors": [
                {
                    "name": "Brent Bartlett",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Orthorectification",
                "Drone",
                "OptiX"
            ],
            "ranking": 4,
            "dependencies": {
                "OptiX-SDK": {
                    "version": "4.7.0"
                },
                "OptiX-Toolit": {
                    "version": "0.8.1"
                }
            }
        },
        "readme": "# HoloHub Orthorectification Application\n\nThis application is an example of utilizing the nvidia OptiX SDK via the PyOptix bindings to create per-frame orthorectified imagery. In this example, one can create a visualization of mapping frames from a drone mapping mission processed with [Open Drone Map](https://opendronemap.org/). A typical output of a mapping mission is a single merged mosaic. While this product is useful for GIS applications, it is difficult to apply algorithms on a such a large single image without incurring additional steps like image chipping. Additionally, the mosaic process introduces image artifacts which can negativley impact algorithm performance. \n\nSince this holoscan pipeline processes each frame individually, it opens the door for one to apply an algorithm to the original un-modififed imagery and then map the result. If custom image processing is desired, it is recommended to insert custom operators before the Ray Trace Ortho operator in the application flow. \n\n\n![](docs/odm_ortho_pipeline.png)<br>\nFig. 1 Orthorectification sample application workflow\n\nSteps for running the application:\n\na) Download and Prep the ODM Dataset<br>\n1. Download the [Lafayette Square Dataset](https://www.opendronemap.org/odm/datasets/) and place into ~/Data.\n\n2. Process the dataset with ODM via docker command: <br>\n```docker run -ti --rm -v ~/Data/lafayette_square:/datasets/code opendronemap/odm --project-path /datasets --camera-lens perspective --dsm```\n\nIf you run out of memory add the following argument to preserve some memory: ```--feature-quality medium```\n\nb) Clone holohub and navigate to this application directory\n\nc) Download [OptiX SDK 7.4.0](https://developer.nvidia.com/optix/downloads/7.4.0/linux64-x86_64) and extract the package in the same directory as the source code\n(i.e. applications/orthorectification_with_optix).\n\nd) Build development container <br>\n1. ```DOCKER_BUILDKIT=1 docker build -t holohub-ortho-optix:latest .```\n\nYou can now run the docker container by: <br>\n1. ```xhost +local:docker```\n2. ```nvidia_icd_json=$(find /usr/share /etc -path '*/vulkan/icd.d/nvidia_icd.json' -type f 2>/dev/null | grep .) || (echo \"nvidia_icd.json not found\" >&2 && false)```\n3. ```docker run -it --rm --net host --runtime=nvidia -v ~/Data:/root/Data  -v .:/work/ -v /tmp/.X11-unix:/tmp/.X11-unix  -v $nvidia_icd_json:$nvidia_icd_json:ro  -e NVIDIA_DRIVER_CAPABILITIES=graphics,video,compute,utility,display -e DISPLAY=$DISPLAY  holohub-ortho-optix```\n\nFinish prepping the input data: <br>\n1. ```gdal_translate -tr 0.25 0.25 -r cubic ~/Data/lafayette_square/odm_dem/dsm.tif ~/Data/lafayette_square/odm_dem/dsm_small.tif```\n2. ```gdal_fillnodata.py -md 0 ~/Data/lafayette_square/odm_dem/dsm_small.tif ~/Data/lafayette_square/odm_dem/dsm_small_filled.tif```\n\nFinally run the application: <br>\n1. ```python ./python/ortho_with_pyoptix.py```\n\nYou can modify the applications settings in the file \"ortho_with_pyoptix.py\" \n\n```\nsensor_resize = 0.25 # resizes the raw sensor pixels\nncpu = 8 # how many cores to use to load sensor simulation\ngsd = 0.25 # controls how many pixels are in the rendering\niterations = 425 # how many frames to render from the source images (in this case 425 is max)\nuse_mosaic_bbox = True # render to a static bounds on the ground as defined by the DEM\nwrite_geotiff = False \nnb=3 # how many bands to write to the GeoTiff\nrender_scale = 0.5 # scale the holoview window up or down\nfps = 8.0 # rate limit the simulated sensor feed to this many frames per second\n```\n\n![](docs/holohub_ortho_app.gif)<br>\nFig. 2 Running the orthorectification sample application",
        "application_name": "orthorectification_with_optix",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "cuda_quantum",
            "authors": [
                {
                    "name": "Sean Huver",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0.0": "Initial release of cuda_quantum application"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Quantum Computing",
                "CUDA",
                "VQE"
            ],
            "ranking": 4,
            "dependencies": {
                "cuda_quantum": "^0.4.0"
            },
            "run": {
                "command": "python3 cuda_quantum.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Hybrid-Computing Sample App - CUDA Quantum Variational Quantum Eigensolver (VQE) Application\n\n## Variational Quantum Eigensolver (VQE)\nThe Variational Quantum Eigensolver (VQE) is a quantum algorithm designed to approximate the ground state energy of quantum systems. This energy, represented by what is called the Hamiltonian of the system, is central to multiple disciplines, including drug discovery, material science, and condensed matter physics. The goal of VQE is to find the state that minimizes the expectation value of this Hamiltonian, which corresponds to the ground state energy.\n\nAt its core, VQE is a lighthouse example of the synergy between classical and quantum computing, requiring them both to tackle problems traditionally deemed computationally intractable. Even in the current landscape where fault-tolerant quantum computing\u2014a stage where quantum computers are resistant to errors\u2014is not yet realized, VQE is seen as a practical tool. This is due to its design as a 'near-term' algorithm, built to operate on existing noisy quantum hardware. \n\n## Key Components of VQE\n1. **Hamiltonian**: This represents the total energy of the quantum system, which is known ahead of time. In VQE, we aim to find the lowest eigenvalue (ground state energy) of this Hamiltonian.\n  \n2. **Ansatz (or trial wavefunction)**: The ansatz is the initial guess for the state of the quantum system, represented by a parameterized quantum circuit. It's crucial for this state to be a good representation, as the quality of the ansatz can heavily influence the final results. VQE iteratively refines the parameters of this ansatz to approximate the true ground state of the Hamiltonian.\n\n## VQE Mechanism\nThe VQE operates by employing a hybrid quantum-classical approach:\n\n1. **Quantum Circuit Parameterization**: VQE begins with a parameterized quantum circuit, effectively serving as an initial guess or representation of the system's state.\n2. **Evaluation and Refinement**: The quantum system's energy is evaluated using the current quantum circuit parameters. Classical optimization algorithms then adjust these parameters in a quest to minimize the energy.\n3. **Iterative Process**: The combination of quantum evaluation and classical refinement is iterative. Over multiple cycles, the parameters are tuned to get increasingly closer to the true ground state energy.\n\n## Integration with Holoscan and CUDA Quantum\n- **NVIDIA Holoscan SDK**: The Holoscan SDK is designed for efficient handling of high-throughput, low-latency GPU tasks. Within the context of VQE, the Holoscan SDK facilitates the rapid classical computations necessary for parameter adjustments and optimization. The `ClassicalComputeOp` in the provided code sample is an example of this SDK in action, preparing the quantum circuits efficiently.\n- **CUDA Quantum**: CUDA Quantum is a framework that manages hybrid quantum-classical workflows. For VQE, CUDA Quantum processes quantum data and executes quantum operations. The `QuantumComputeOp` operator in the code uses the cuQuantum simulator backend, but the user may optionally switch out the simulator for a real quantum cloud backend provided by either IonQ or Quantinuum ([see CUDA Quantum backend documentation](https://nvidia.github.io/cuda-quantum/latest/using/hardware.html#)).\n\nHoloscan ensures swift and efficient classical computations, while CUDA Quantum manages the quantum components with precision.\n\n## Usage\n\nTo run the application, you need to have CUDA Quantum, Qiskit, and Holoscan installed. You also need an IBM Quantum account to use their quantum backends.\n\n1. Clone the repository and navigate to the `cuda_quantum` directory containing.\n\n2. Install the requirements `pip install -r requirements.txt`\n\n3. Either use or replace the `'hamiltonian'` in `cuda_quantum.yaml` dependent on the physical system you wish to model.\n\n4. Run the application with the command `python cuda_quantum.py`.\n\n## Operators\n\nThe application uses three types of operators:\n\n- `ClassicalComputeOp`: This operator performs classical computations. It also creates a quantum kernel representing the initial ansatz, or guess of the state of the system, and a Hamiltonian.\n\n- `QuantumComputeOp`: This operator performs quantum computations. It uses the quantum kernel and Hamiltonian from `ClassicalComputeOp` to iterate towards the ground state energy and parameter using VQE.\n\n- `PrintOp`: This operator prints the result from `QuantumComputeOp`.\n\n## Operator Connections\n\nThe operators are connected as follows:\n\n```mermaid\nflowchart LR\n    ClassicalComputeOp --> QuantumComputeOp\n    QuantumComputeOp --> PrintOp\n```\n\n`ClassicalComputeOp` sends the quantum kernel and Hamiltonian to `QuantumComputeOp`, which computes the energy and parameter and sends the result to `PrintOp`.",
        "application_name": "cuda_quantum",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Qt Video Replayer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Qt",
                "QML",
                "QtQuick",
                "Video",
                "UI",
                "Userinterface",
                "Interactive"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/qt_video_replayer",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Qt Video Replayer\n\n![](screenshot.png)<br>\n\nThis application demonstrates how to integrate Holoscan with a [Qt](https://www.qt.io/) application. It support displaying the video frames output by a Holoscan operator and changing operator properties using Qt UI elements.\n\n```mermaid\nflowchart LR\n    subgraph Holoscan application\n        A[(VideoFile)] --> VideostreamReplayerOp\n        VideostreamReplayerOp --> FormatConverterOp\n        FormatConverterOp --> NppFilterOp\n        NppFilterOp --> QtVideoOp\n    end\n    subgraph Qt Window\n        QtVideoOp <-.-> QtHoloscanVideo\n    end\n```\n\nThe application uses the VideostreamReplayerOp to read from a file on disk, the FormatConverterOp to convert the frames from RGB to RGBA, the [NppFilterOp](../../operators/npp_filter/README.md) to apply a filter to the frame and the [QtVideoOp](../../operators/qt_video/README.md) operator to display the video stream in a Qt window.\n\nThe [QtHoloscanApp](./qt_holoscan_app.hpp) class, which extends the `holoscan::Application` class, is used to expose parameters of Holoscan operators as Qt properties.\n\nFor example the application uses a [QML Checkbox](https://doc.qt.io/qt-6/qml-qtquick-controls-checkbox.html) is used the set the `realtime` property of the `VideostreamReplayerOp` operator.\n\n```\n    CheckBox {\n        id: realtime\n        text: \"Use Video Framerate\"\n        checked: holoscanApp.realtime\n        onCheckedChanged: {\n            holoscanApp.realtime = checked;\n        }\n    }\n```\n\nThe [QtHoloscanVideo](../../operators/qt_video/qt_video_op.hpp) is a QQuickItem which can be use in the QML file. Multiple `QtHoloscanVideo` items can be placed in a Qt window.\n\n```\nimport QtHoloscanVideo\nItem {\n    QtHoloscanVideo {\n        objectName: \"video\"\n    }\n}\n```\n\n## Run Instructions\n\nThis application requires [Qt](https://www.qt.io/).\nFor simplicity a DockerFile is available. To generate the container run:\n\n```bash\n./dev_container build --docker_file ./applications/qt_video_replayer/Dockerfile\n```\n\nThe application can then be built by launching this container and using the provided `run` script.\n\n```bash\n./dev_container launch\n./run build qt_video_replayer\n```\n\nOnce the application is build it can be launched with the `run` script.\n\n```bash\n./run launch qt_video_replayer\n```\n",
        "application_name": "qt_video_replayer",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "TAO PeopleNet Detection Model on Video Stream",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Computer Vision",
                "Detection"
            ],
            "ranking": 2,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet"
            },
            "run": {
                "command": "python3 ../applications/tao_peoplenet/tao_peoplenet.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# TAO PeopleNet Detection Model on V4L2 Video Stream\n\nUse the TAO PeopleNet available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/peoplenet) to detect faces and people in a V4L2 supported video stream. HoloViz is used to draw bounding boxes around the detections.\n\n**Prerequisite**: Download the PeopleNet ONNX model from the NGC website:\n```sh\nwget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/tao/peoplenet/pruned_quantized_decrypted_v2.3.3/files?redirect=true&path=resnet34_peoplenet_int8.onnx' -O applications/tao_peoplenet/resnet34_peoplenet_int8.onnx\n```\n\n## Requirements\n\n### Containerized Development\n\nIf using a container outside the Holoscan SDK `run` script, add `--group-add video` and `--device /dev/video0:/dev/video0` (or the ID of whatever device you'd like to use) to the `docker run` command to make your camera device available in the container.\n\n### Local Development\n\nInstall the following dependency:\n```sh\nsudo apt-get install libv4l-dev=1.18.0-2build1\n```\n\nIf you do not have permissions to open the video device, do:\n```sh\n sudo usermod -aG video $USER\n```\n\n## Run Instructions\n\nRun with:\n```sh\n./run launch tao_peoplenet\n```",
        "application_name": "tao_peoplenet",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Ultrasound Segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "Segmentation"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 <holohub_app_source>/ultrasound_segmentation.py  --data <holohub_data_dir>/ultrasound_segmentation",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Ultrasound Bone Scoliosis Segmentation\n\nFull workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. \n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/ultrasound_segmentation/python\n    python3 ultrasound_segmentation.py --source=replayer --data <DATA_DIR>/ultrasound_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/ultrasound_segmentation/python\n    python3 ultrasound_segmentation.py --source=aja\n    ```\n",
        "application_name": "ultrasound_segmentation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Ultrasound Segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "Segmentation"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/ultrasound_segmentation  --data <holohub_data_dir>/ultrasound_segmentation",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Ultrasound Bone Scoliosis Segmentation\n\nFull workflow including a generic visualization of segmentation results from a spinal scoliosis segmentation model of ultrasound videos. The model used is stateless, so this workflow could be configured to adapt to any vanilla DNN model. \n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the ultrasound data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Bone Scoliosis Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n    applications/ultrasound_segmentation/cpp/ultrasound_segmentation --data <data_dir>/ultrasound_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/ultrasound_segmentation/cpp/ultrasound_segmentation.yaml\n    applications/ultrasound_segmentation/cpp/ultrasound_segmentation\n    ```\n",
        "application_name": "ultrasound_segmentation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Networked Radar Pipeline",
            "authors": [
                {
                    "name": "Dylan Eustice",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.1",
            "changelog": {
                "1.0": "Initial Release",
                "1.1": "Update to work with ANO 1.2"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Networking",
                "Network",
                "UDP",
                "IP",
                "Signal Processing",
                "RADAR"
            ],
            "ranking": 2,
            "dependencies": {
                "operators": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    },
                    {
                        "name": "advanced_network",
                        "version": "1.2"
                    }
                ],
                "libraries": [
                    {
                        "name": "MatX",
                        "version": "0.6.0",
                        "url": "https://github.com/NVIDIA/MatX.git"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/network_radar_pipeline",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Network Radar Pipeline\nThe Network Radar Pipeline demonstrates signal processing on data streamed via packets over a network. It showcases the use of both the Advanced Network Operator and Basic Network Operator to send or receive data, combined with the signal processing operators implemented in the Simple Radar Pipeline application.\n\nUsing the GPUDirect capabilities afforded by the Advanced Network Operator, this pipeline has been tested up to 100 Gbps (Tx/Rx) using a ConnectX-7 NIC and A30 GPU.\n\nThe motivation for building this application is to demonstrate how data arrays can be assembled from packet data in real-time for low-latency, high-throughput sensor processing applications. The main components of this work are defining a message format and writing code connecting the network operators to the signal processing operators.\n\n## Prerequisites\nSee the README for the Advanced Network Operator for requirements and system tuning needed to enable high-throughput GPUDirect capabilities.\n\n## Environment\nNote: Dockerfile should be cross-compatible, but has only been tested on x86. Needs to be edited if different versions / architectures are required.\n\n## Build\nPlease refer to the top level Holohub README.md file for information on how to build this application: `./run build network_radar_pipeline`.\n\n## Run\nNote: must properly configure YAML files before running.\n- On Tx machine: `./build/applications/network_radar_pipeline/cpp/network_radar_pipeline source.yaml`\n- On Rx machine: `./build/applications/network_radar_pipeline/cpp/network_radar_pipeline process.yaml`\n\n## Network Operator Connectors\nSee each operators' README before using / for more detailed information.\n### Basic Network Operator Connector\nImplementation in `basic_network_connectors`. Only supports CPU packet receipt / transmit. Uses cudaMemcpy to move data between network operator and MatX tensors.\n### Advanced Network Operator Connector\nImplementation in `advanced_network_connectors`. RX connector is only configured to run with GPUDirect enabled, in header-data split (HDS) mode. TX connector supports both GPUDirect/HDS or CPU-only.\n#### Testing RX on generic packet data\nWhen using the Advanced network operator, the application supports testing the radar processing component in a \"spoof packets\" mode. This functionality allows for easier benchmarking of the application by ingesting generic packet data and writing in header fields such that the full radar pipeline will still be exercised. When \"SPOOF_PACKET_DATA\" (adv_networking_rx.h) is set to \"true\", the index of the packet will be used to set fields appropriately. This functionality is currently unsupported using the basic network operator connectors.\n\n## Message format\nThe message format is defined by `RFPacket`. It is a byte array, represented by `RFPacket::payload`, where the first 16 bytes are reserved for metadata and the rest are used for representing complex I/Q samples. The metadata is:\n- Sample index: The starting index for a single pulse/channel of the transmitted samples (2 bytes)\n- Waveform ID: Index of the transmitted waveform (2 bytes)\n- Channel index: Index of the channel (2 bytes)\n- Pulse index: Index of the pulse (2 bytes)\n- Number samples: Number of I/Q samples transmitted (2 bytes)\n- End of array: Boolean - true if this is the last message for the waveform (2 bytes)",
        "application_name": "network_radar_pipeline",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Depth Estimation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Depth",
                "AJA"
            ],
            "ranking": 2,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_depth_estimation_sample_data",
                "opencv": "^4.8.0"
            },
            "run": {
                "command": "python3 <holohub_app_source>/endoscopy_depth_estimation.py --data <holohub_data_dir>/endoscopy --model <holohub_data_dir>/endoscopy_depth",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopy Depth Estimation\n\nThis application demonstrates the use of custom components for depth estimation and its rendering using Holoviz with triangle interpolation.\n\n### Requirements\n\n- Python 3.8+\n- OpenCV 4.8+\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Endoscopy](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Model\n\n[\ud83d\udce6\ufe0f (NGC) App Model for AI-based Endoscopy Depth Estimation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_depth_estimation_sample_data)\n\nThe model is automatically downloaded to the same folder as the data in ONNX format.\n\n### OpenCV-GPU\n\nThis application uses OpenCV with GPU acceleration during the preprocessing stage when it runs with Histogram Equalization (flag `--clahe` or `-c`).\nHistogram equalization reduces the effect of specular reflections and improves the visual performance of the depth estimation overall. However,\nusing regular OpenCV datatypes leads to unnecessary I/O operations to transfer data from Holoscan Tensors to the CPU and back.\nWe show in this application how to blend together Holoscan Tensors and OpenCV's `GPUMat` datatype to get rid of this issue in the [`CUDACLAHEOp`](./endoscopy_depth_estimation.py#L163) operator. \nCompare it to [`CPUCLAHEOp`](./endoscopy_depth_estimation.py#L123) for reference.\n\nTo achieve an end-to-end GPU accelerated pipeline / application, the pre-processing operators shall support accessing the GPU memory (Holoscan Tensor) \ndirectly without memory copy / movement in Holoscan SDK. This means that only libraries which implement the [`__cuda_array_interface__`](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html) \nand [DLPack](https://dmlc.github.io/dlpack/latest/) standards allow conversion from/to Holoscan Tensor, such as [cuCIM](https://github.com/rapidsai/cucim).\nOpenCV, however, does not implement neither the `__cuda_array_interface__` nor the standard DLPack, and a little work is needed yet to use this library.\n\nFirst, we convert CuPy arrays to GPUMat using a fix in OpenCV only available from 4.8.0 on. More information [here](https://github.com/opencv/opencv/pull/23371).\nThis is done in the [`gpumat_from_cp_array`](./endoscopy_depth_estimation.py#L32) function. With a `GPUMat`, we can now use any [OpenCV-CUDA operations](https://docs.opencv.org/2.4/modules/gpu/doc/gpu.html).\nOnce the `GPUMat` processing has finished, we have to convert it back to a CuPy tensor with [`gpumat_to_cupy`](./endoscopy_depth_estimation.py#L53). \n\n<hr/>\n\n**Important:** In order to run this application with CUDA acceleration, one must compile [OpenCV with CUDA support](https://docs.opencv.org/4.8.0/d2/dbc/cuda_intro.html).\nWe provide a sample [Dockerfile](./Dockerfile) to build a container based on Holoscan v0.6.0 with the latest version of OpenCV and CUDA support.\nIn case you use it, note that the variable [`CUDA_ARCH_BIN` ](./Dockerfile#L25) must be modified according to your specific GPU\nconfiguration. Refer to [this site](https://developer.nvidia.com/cuda-gpus) to find out your NVIDIA GPU architecture.\n\n<hr/>\n\n### Workflows\n\nThis application can be run with or without Histogram Equalization (CLAHE) by toggling the label `--clahe`.\n\n#### With CLAHE\n![](docs/workflow_depth_estimation_clahe.png)<br>\nFig. 1 Depth Estimation Application with CLAHE enabled\n\nThe pipeline uses a recorded endoscopy video file (generated by `convert_video_to_gxf_entities` script) for input frames. Each input frame in the file is loaded by [Video Stream Replayer](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators) and passed to the following two branches:\n- In the first branch (top), the input frames are passed to the [`CUDACLAHEOp`](./endoscopy_depth_estimation.py#L163), \nthen fed to the [Format Converter](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators)\nto convert their data type from `uint8` to `float32`, and finally fed to the [`InferenceOp`](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\nThe result is then ingested by the [`DepthPostProcessingOp`](./endoscopy_depth_estimation.py#L87), which converts the depth map\nto `uint8` and reorders its dimensions for rendering with [Holoviz](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\n- In the second branch (bottom), the input frames are passed to a [Format Converter](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators)\nthat resizes them. Its output is finally fed to the [`DepthPostProcessingOp`](./endoscopy_depth_estimation.py#L87) for \nrendering with [Holoviz](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\n\n\n#### Without CLAHE\n![](docs/workflow_depth_estimation_noclahe.png)<br>\nFig. 2 Depth Estimation Application with CLAHE disabled\n\nThe pipeline uses a recorded endoscopy video file (generated by `convert_video_to_gxf_entities` script) for input frames. Each input frame in the file is loaded by [Video Stream Replayer](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators)\nand passed to a branch that firstly converts its data type to `float32` and resizes it with a [Format Converter](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\nThen, the preprocessed frames are fed to the [`InferenceOp`](https://docs.nvidia.com/holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators)\nand mixed with the original video in the custom [`DepthPostProcessingOp`](./endoscopy_depth_estimation.py#L87) for\nrendering with [Holoviz](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/holoscan_operators_extensions.html#operators).\n\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n \n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nThis application should **be run in the build directory of Holohub** in order to load the GXF extensions.\nAlternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of\nthe working directory.\n\nNext, run the command to run the application:\n\n```bash\ncd <HOLOHUB_BUILD_DIR>\npython3 <HOLOHUB_SOURCE_DIR>/applications/endoscopy_depth_estimation/endoscopy_depth_estimation.py --data=<DATA_DIR> --model=<MODEL_DIR> --clahe\n```\n",
        "application_name": "endoscopy_depth_estimation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Riva ASR to local-LLM",
            "authors": [
                {
                    "name": "Nigel Nelson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Speech-to-text",
                "Large Language Model",
                "ASR",
                "Local-LLM"
            ],
            "ranking": 4,
            "dependencies": {
                "python-packages": {
                    "requests": {
                        "version": "2.31.0",
                        "license-url": "https://github.com/opentracing-contrib/python-requests/blob/master/LICENSE"
                    },
                    "nvidia-riva-client": {
                        "version": "2.14.0",
                        "license-url": "https://github.com/nvidia-riva/python-clients/blob/main/LICENSE"
                    },
                    "pyaudio": {
                        "version": "0.2.13",
                        "license-url": "https://github.com/CristiFati/pyaudio/blob/master/LICENSE.txt"
                    },
                    "pynput": {
                        "version": "0.2.13",
                        "license-url": "https://github.com/moses-palmer/pynput/blob/master/COPYING.LGPL"
                    }
                },
                "OSS": {
                    "llama.cpp": {
                        "commit": "cf9b08485c4c2d4d945c6e74fe20f273a38b6104",
                        "license-url": "https://github.com/ggerganov/llama.cpp/blob/master/LICENSE"
                    }
                }
            },
            "run": {
                "command": "python3 asr_to_llm.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# HoloHub Applications\n\nThis directory contains applications based on the Holoscan Platform.\nSome applications might require specific hardware and software packages which are described in the \nmetadata.json and/or README.md for each application.\n\n# Contributing to HoloHub Applications\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute applications.\n",
        "application_name": "asr_to_llm",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "MultiAI Ultrasound",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "MultiAI"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 <holohub_app_source>/multiai_ultrasound.py  --data <holohub_data_dir>/multiai_ultrasound",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Multi-AI Ultrasound\n\nThis application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.\n\nThe Multi AI operators (inference and postprocessor) use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.\n\nThe applications uses models and echocardiogram data from iCardio.ai. The models include:\n- a Plax chamber model, that identifies four critical linear measurements of the heart\n- a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography\n- an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis\n\nThe default configuration (`multiai_ultrasound.yaml`) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (`mgpu_multiai_ultrasound.yaml`) is present in both `cpp` and `python` applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_multi_ai_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --source=replayer --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using a pre-recorded video on multi-GPU system\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --config mgpu_multiai_ultrasound.yaml --source=replayer --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/multiai_ultrasound/python\n    python3 multiai_ultrasound.py --source=aja\n    ```\n",
        "application_name": "multiai_ultrasound",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "MultiAI Ultrasound",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Ultrasound",
                "MultiAI"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/multiai_ultrasound --data <holohub_data_dir>/multiai_ultrasound",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Multi-AI Ultrasound\n\nThis application demonstrates how to run multiple inference pipelines in a single application by leveraging the Holoscan Inference module, a framework that facilitates designing and executing inference applications in the Holoscan SDK.\n\nThe Inference and the Processing operators use APIs from the Holoscan Inference module to extract data, initialize and execute the inference workflow, process, and transmit data for visualization.\n\nThe applications uses models and echocardiogram data from iCardio.ai. The models include:\n- a Plax chamber model, that identifies four critical linear measurements of the heart\n- a Viewpoint Classifier model, that determines confidence of each frame to known 28 cardiac anatomical view as defined by the guidelines of the American Society of Echocardiography\n- an Aortic Stenosis Classification model, that provides a score which determines likeability for the presence of aortic stenosis\n\nThe default configuration (`multiai_ultrasound.yaml`) runs on default GPU (GPU-0). Multi-AI Ultrasound application can be executed on multiple GPUs with the Holoscan SDK version 0.6 onwards. A sample configuration file for multi GPU configuration for multi-AI ultrasound application (`mgpu_multiai_ultrasound.yaml`) is present in both `cpp` and `python` applications. The multi-GPU configuration file is designed for a system with at least 2 GPUs connected to the same PCIE network.\n\n### Requirements\n\nThe provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the echocardiogram (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for Multi-AI Ultrasound Pipeline](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_multi_ai_ultrasound_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using a pre-recorded video on multi-GPU system\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound applications/multiai_ultrasound/cpp/mgpu_multiai_ultrasound.yaml --data <DATA_DIR>/multiai_ultrasound\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/multiai_ultrasound/cpp/multiai_ultrasound.yaml\n    applications/multiai_ultrasound/cpp/multiai_ultrasound\n    ```\n",
        "application_name": "multiai_ultrasound",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "WebRTC Video Client",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Client",
                "Video"
            ],
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    },
                    {
                        "name": "aiohttp",
                        "version": "3.8.5"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/webrtc_client.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# WebRTC Video Client\n\n![](screenshot.png)<br>\n\nThis app receives video frames from a web cam connected to a browser and display them on the screen.\n\nThe app starts a web server, the pipeline starts when a browser is connected to the web server and the `Start` button is pressed. The pipeline stops when the `Stop` button is pressed.\n\nThe video resolution and video codec can be selected in browser.\n\n```mermaid\nflowchart LR\n    subgraph Server\n        WebRTCClientOp --> HolovizOp\n        WebServer\n    end\n    subgraph Client\n        Webcam --> Browser\n        Browser <--> WebRTCClientOp\n        Browser <--> WebServer\n    end\n```\n\n> **_NOTE:_** When using VPN there might be a delay of several seconds between pressing the `Start` button and the first video frames are display. The reason for this is that the STUN server `stun.l.google.com:19302` used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.\n\n## Prerequisites\n\nThe app is using [AIOHTTP](https://docs.aiohttp.org/en/stable/) for the web server and [AIORTC](https://github.com/aiortc/aiortc) for WebRTC. Install both using pip.\n\n```bash\npip install aiohttp aiortc\n```\n\n## Run Instructions\n\nRun the command:\n\n```bash\n./run launch webrtc_video_client\n```\n\nOn the same machine open a browser and connect to `127.0.0.1:8080`.\n\nSelect the video resolution and codec or keep the defaults.\n\nPress the `Start` button. Video frames are displayed. To stop, press the `Stop` button. Pressing `Start` again will continue the video.\n\nYou can also connect from a different machine by connecting to the IP address the app is running on. Chrome disables features such as getUserMedia when it comes from an unsecured origin. `http://localhost` is considered as a secure origin by default, however if you use an origin that does not have an SSL/TLS certificate then Chrome will consider the origin as unsecured and disable getUserMedia.\n\nSolutions\n\n- Create an self-signed SSL/TLS certificate with `openssl req -new -newkey rsa:4096 -x509 -sha256 -days 365 -nodes -out MyCertificate.crt -keyout MyKey.key`. Pass the generated files to the `webrtc_client` using the `--cert-file` and `--key-file` arguments. Connect the browser to `https://{YOUR HOST IP}:8080`.\n- Go to chrome://flags, search for the flag `unsafely-treat-insecure-origin-as-secure`, enter the origin you want to treat as secure such as `http://{YOUR HOST IP}:8080`, enable the feature and relaunch the browser.\n\n### Command Line Arguments\n\n```\nusage: webrtc_client.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose VERBOSE]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n```",
        "application_name": "webrtc_video_client",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "prohawk_video_replayer_py",
            "authors": [
                {
                    "name": "Nigel Nelson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Enable Python support"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "arm64"
            ],
            "tags": [
                "Video Processing",
                "Prohawk"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "prohawk",
                        "version": "1.0.0"
                    }
                ]
            },
            "run": {
                "command": "python prohawk_video_replayer.py",
                "workdir": "/workspace/holohub/applications/prohawk_video_replayer/python"
            }
        },
        "readme": "# Prohawk video replayer\n\nThis application utilizes the ProHawk restoration operator along with Holoscan's Video Replayer and Holoviz operators to enhance and restore medical imagery in real-time, offering superior image quality. The user-friendly interface of the application provides a range of filter options, enabling users to dynamically select the most suitable filter for optimal results.\n\n![](screenshot.png)\n\n## ProHawk Vision Restoration Operator \n\nThe ProHawk Vision Operator is a groundbreaking solution that is transforming both healthcare and manufacturing industries by revolutionizing computer vision technology with its patented restoration capabilities. It seamlessly integrates into the NVIDIA Holoscan full-stack infrastructure, fundamentally altering the way healthcare professionals diagnose and treat patients, while also optimizing manufacturing processes. In healthcare, the ProHawk Vision Operator Plugin automatically interprets medical imaging frames, identifies real-world conditions, and employs precise control over ProHawk Vision Restoration algorithms, all driven by an objective mathematical model using quantitative measurements to enhance accuracy in diagnoses and treatments by restoring degraded frames. In manufacturing, the ProHawk Vision Operator Plugin algorithms reveal manufacturing line defects ensuring product quality.\n\n## Application Controls\n\nThe operator can be controlled with keyboard shortcuts:\n\n- **AFS (0)** - Automatic Filter Selection (AFS) \u2013 Automatically and continuously adjust the image pixel values to maximize image detail and visibility.\n- **LowLight (1)** - Lowlight preset filter that corrects lighting compromised imagery.\n- **Vascular Detail (2)** - Vasculature Detail Preset Filter that reveals the fine details of vasculature structures without the need for dye or contrast agents.\n- **Vapor (3)** - Vapor Preset Filter that removes vapor, smoke, and stream from the video.\n- **Disable Restoration (d)** - Disable ProHawk Vision computer vision restoration.\n- **Side-by-Side View (v)** - Display Side-by-Side (restored/non-restores) Video.\n- **Display Menu Items (m)** - Display menus control items.\n- **Quit (q)** - Exit the application\n\n## Data\n\nThe following dataset is used by this application:\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data).\n\n##  Building the application\n\nThe easiest way to build this application is to use the provided Docker file.\n\nFrom the Holohub main directory run the following command:\n\n  ```bash\n  ./dev_container build --docker_file applications/prohawk_video_replayer/Dockerfile --img holohub:prohawk\n  ```\n\nThen launch the container to build the application:\n\n  ```bash\n  ./dev_container launch --img holohub:prohawk\n  ```\n\nInside the container build the application:\n\n  ```bash\n  ./run build prohawk_video_replayer\n  ```\n  \nInside the container run the application:\n\n- C++:\n    ```bash\n    ./run launch prohawk_video_replayer cpp\n    ```\n- Python:\n    ```bash\n    export PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/python/lib/\n    python <prohawk_app_dir>/python/prohawk_video_replayer.py\n    ```\n\nFor more information about this application and operator please visit [https://prohawk.ai/prohawk-vision-operator/#learn](https://prohawk.ai/prohawk-vision-operator/#learn)\nFor technical support or other assistance, please don't hesitate to visit us at [https://prohawk.ai/contact](https://prohawk.ai/contact)\n",
        "application_name": "prohawk_video_replayer",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "prohawk_video_replayer",
            "authors": [
                {
                    "name": "Tim Wooldridge",
                    "affiliation": "Prohawk Technology Group"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Added watermark to the Prohawk restoration engine"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.1",
                "tested_versions": [
                    "0.5.1",
                    "0.6.0"
                ]
            },
            "platforms": [
                "arm64"
            ],
            "tags": [
                "Video Processing",
                "Prohawk"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "prohawk",
                        "version": "1.0.0"
                    }
                ]
            },
            "run": {
                "command": "./prohawk_video_replayer --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Prohawk video replayer\n\nThis application utilizes the ProHawk restoration operator along with Holoscan's Video Replayer and Holoviz operators to enhance and restore medical imagery in real-time, offering superior image quality. The user-friendly interface of the application provides a range of filter options, enabling users to dynamically select the most suitable filter for optimal results.\n\n![](screenshot.png)\n\n## ProHawk Vision Restoration Operator \n\nThe ProHawk Vision Operator is a groundbreaking solution that is transforming both healthcare and manufacturing industries by revolutionizing computer vision technology with its patented restoration capabilities. It seamlessly integrates into the NVIDIA Holoscan full-stack infrastructure, fundamentally altering the way healthcare professionals diagnose and treat patients, while also optimizing manufacturing processes. In healthcare, the ProHawk Vision Operator Plugin automatically interprets medical imaging frames, identifies real-world conditions, and employs precise control over ProHawk Vision Restoration algorithms, all driven by an objective mathematical model using quantitative measurements to enhance accuracy in diagnoses and treatments by restoring degraded frames. In manufacturing, the ProHawk Vision Operator Plugin algorithms reveal manufacturing line defects ensuring product quality.\n\n## Application Controls\n\nThe operator can be controlled with keyboard shortcuts:\n\n- **AFS (0)** - Automatic Filter Selection (AFS) \u2013 Automatically and continuously adjust the image pixel values to maximize image detail and visibility.\n- **LowLight (1)** - Lowlight preset filter that corrects lighting compromised imagery.\n- **Vascular Detail (2)** - Vasculature Detail Preset Filter that reveals the fine details of vasculature structures without the need for dye or contrast agents.\n- **Vapor (3)** - Vapor Preset Filter that removes vapor, smoke, and stream from the video.\n- **Disable Restoration (d)** - Disable ProHawk Vision computer vision restoration.\n- **Side-by-Side View (v)** - Display Side-by-Side (restored/non-restores) Video.\n- **Display Menu Items (m)** - Display menus control items.\n- **Quit (q)** - Exit the application\n\n## Data\n\nThe following dataset is used by this application:\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data).\n\n##  Building the application\n\nThe easiest way to build this application is to use the provided Docker file.\n\nFrom the Holohub main directory run the following command:\n\n  ```bash\n  ./dev_container build --docker_file applications/prohawk_video_replayer/Dockerfile --img holohub:prohawk\n  ```\n\nThen launch the container to build the application:\n\n  ```bash\n  ./dev_container launch --img holohub:prohawk\n  ```\n\nInside the container build the application:\n\n  ```bash\n  ./run build prohawk_video_replayer\n  ```\n  \nInside the container run the application:\n\n- C++:\n    ```bash\n    ./run launch prohawk_video_replayer cpp\n    ```\n- Python:\n    ```bash\n    export PYTHONPATH=$PYTHONPATH:/workspace/holohub/build/python/lib/\n    python <prohawk_app_dir>/python/prohawk_video_replayer.py\n    ```\n\nFor more information about this application and operator please visit [https://prohawk.ai/prohawk-vision-operator/#learn](https://prohawk.ai/prohawk-vision-operator/#learn)\nFor technical support or other assistance, please don't hesitate to visit us at [https://prohawk.ai/contact](https://prohawk.ai/contact)\n",
        "application_name": "prohawk_video_replayer",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Tracking",
                "AJA"
            ],
            "ranking": 1,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "python3 <holohub_app_source>/endoscopy_tool_tracking.py --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopy Tool Tracking\n\nBased on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA or Yuan capture cards for input stream, or a pre-recorded endoscopy video (replayer). \nFollow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n \n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n \nNext, run the commands of your choice:\n\nThis application should **be run in the build directory of Holohub** in order to load the GXF extensions.\nAlternatively, the relative path of the extensions in the corresponding yaml file can be modified to match path of\nthe working directory.\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3 <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=replayer --data=<DATA_DIR>/endoscopy\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3  <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=aja\n    ```\n\n* Using a YUAN card\n    ```bash\n    cd <HOLOHUB_BUILD_DIR>\n    python3  <HOLOHUB_SOURCE_DIR>/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.py --source=yuan\n    ```\n",
        "application_name": "endoscopy_tool_tracking",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Tracking",
                "AJA"
            ],
            "ranking": 0,
            "dependencies": {
                "model": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data"
            },
            "run": {
                "command": "<holohub_app_bin>/endoscopy_tool_tracking --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Endoscopy Tool Tracking\n\nBased on a LSTM (long-short term memory) stateful model, these applications demonstrate the use of custom components for tool tracking, including composition and rendering of text, tool position, and mask (as heatmap) combined with the original video stream.\n\n### Requirements\n\nThe provided applications are configured to either use capture cards for input stream, or a pre-recorded endoscopy video (replayer). \n\nFollow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\nRefer to the Deltacast documentation to use the Deltacast VideoMaster capture card.\n\nRefer to the Yuan documentation to use the Yuan QCap capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\nIn order to build with the Deltacast VideoMaster operator use ```./run build --with deltacast_videomaster```\n\n### Run Instructions\n\nIn your `build` directory, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    sed -i -e 's#^source:.*#source: replayer#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking --data <data_dir>/endoscopy\n    ```\n\n* Using an AJA card\n    ```bash\n    sed -i -e 's#^source:.*#source: aja#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n    ```\n\n* Using a Deltacast card\n    ```bash\n    sed -i -e '/^#.*deltacast_videomaster/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    sed -i -e 's#^source:.*#source: deltacast#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n    ```\n* Using a Yuan card\n    ```bash\n    sed -i -e '/^#.*yuan_qcap/s/^#//' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    sed -i -e 's#^source:.*#source: yuan#' applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking.yaml\n    applications/endoscopy_tool_tracking/cpp/endoscopy_tool_tracking\n    ```\n",
        "application_name": "endoscopy_tool_tracking",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Speech-to-text + Large Language Model",
            "authors": [
                {
                    "name": "Sean Huver",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Speech-to-text",
                "Large Language Model"
            ],
            "ranking": 2,
            "dependencies": {
                "openai-whisper": "^20230314",
                "openai": "^0.27.2"
            },
            "run": {
                "command": "python3 stt_to_nlp.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# HoloHub Applications\n\nThis directory contains applications based on the Holoscan Platform.\nSome applications might require specific hardware and software packages which are described in the \nmetadata.json and/or README.md for each application.\n\n# Contributing to HoloHub Applications\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute applications.\n",
        "application_name": "speech_to_text_llm",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Medical Image viewer in XR",
            "authors": [
                {
                    "name": "Andreas Heumann",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Connor Smith",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Cristiana Dinea",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Jiwen Cai",
                    "affiliation": "Magic Leap"
                },
                {
                    "name": "Jochen Stier",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Rendering",
                "OpenXR",
                "Mixed",
                "Reality"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "windrunner",
                        "version": "1.2.88"
                    },
                    {
                        "name": "monado-service",
                        "version": "21.0.0"
                    },
                    {
                        "name": "remote viewer apk",
                        "version": "1.2.85"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/volume_rendering_xr --config <holohub_data_dir>/volume_rendering/config.json --density <holohub_data_dir>/volume_rendering/highResCT.mhd --mask <holohub_data_dir>/volume_rendering/smoothmasks.seg.mhd",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Medical Image Viewer in XR\n\n\n## Name\n\nMedical Image viewer in XR\n\n\n## Description\n\nMedical imagery is one of the fastest-growing sources of data in any industry.\n\nWe collaborated with Magic Leap on a proof of concept AR viewer for medical imagery built on our Holoscan platform. So when we think about typical diagnostic imaging, x-ray, CT scans, and MRIs come to mind. X-rays are 2D images, so viewing them on a lightbox or, if they\u2019re digital, a computer, is fine. But CT scans and MRIs are 3D. They\u2019re incredibly important technologies, but our way of interacting with them is flawed.This technology helps physicians in so many ways, from training and education to making more accurate diagnoses and ultimately to planning and even delivering more effective treatments.\n\n## Prerequisites \n\n### OpenXR runtime\nOpenXR runtimes are implementations of the OpenXR API that will allow Holoscan XR operators to create XR sessions and render content. \nThe runtimes used by Holoscan XR are run as services.You will be able to download the windrunner binaries by running `bash magicleap.sh` inside the `thirdparty/` folder .\n```sh\ncd applications/volume_rendering_xr\nbash magicleap.sh`\n```\n\n### Android Tools\nAndroid Debug Bridge (ADB) is a command-line tool that allows you to communicate with Android Open Source Project (AOSP) devices such as the Magic Leap 2 .\n\n* First you need to install Android Tools\n\n`sudo apt install android-tools-adb`\n\n* You will need to connect to the IP of your Head Mounted Display (HMD).  While wearing the HMD, go to \"Settings->About\" menu page to find the IP address of the HMD.\n\n`adb connect <IP_headset>`\n\nUpon successful adb connect, you should be able to see the devices listed with\n\n`adb devices`\n\nYou will need to pair the IGX devkit and the headset by running the following from `applications/volume_rendering_xr` folder\n\n`./thirdparty/magicleap/windrunner-aarch64/bin/SetupRemoteViewerIP -i <IP_devkit>`\n\n## Development\n\n### Setup VSCode Dev Container Environment\n\nInstall VSCode for [Arm64](https://code.visualstudio.com/download#) and [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) with Docker.\n\nThe `volume_rendering_xr` app is built on top of vscode. In order to build the app, please copy the `.devcontainer` and `.vscode` folders to the root directory of your cloned holohub repository.\n\nWhen using the Dev Container the first time:\n* Start VSCode and go to `View -> Command Palette -> Dev Containers: Rebuild Container Without Cache`. This will build the container and it could take a few minutes.\n\n### Start an OpenXR runtime service\n\n\nThe Magic Leap Remote OpenXR runtime (windrunner) is the default configured by\nthe dev container.\n\nFor rapid iteration without a Magic Leap device, The Monado OpenXR runtime\nprovides a \"Simulated HMD\" mode that does not require specialized XR hardware to\nuse (though it does not support any user inputs).\n\n#### Option A: Start the Magic Leap Remote OpenXR runtime service (Windrunner)\n\nFrom a terminal _inside_ the dev container, ensure `openxr_windrunner.json` is\nset as the active runtime (default):\n\n`sudo update-alternatives --config openxr1-active-runtime`\n\nThen start the service\n\n```\nwindrunner-service\n```\n\n#### Option B: Start the Monado OpenXR runtime service\n\nFrom a terminal _inside_ the dev container, ensure that `openxr_monado.json` is\nset as the active runtime:\n\n```\nsudo update-alternatives --config openxr1-active-runtime\n```\n\nThen start the service:\n\n```\nmonado-service\n```\n\nNOTE: If you switch back to the Magic Leap runtime, don't forget to update the\nactive runtime alternative with `update-alternatives` (above).\n\n### Build and Run the XR Volume Renderer Application\n\nEnsure an OpenXR runtime service is running, and the correct runtime is set as\nactive (above).\n\nTo build the app, you need to run\n\n`./run build volume_rendering_xr`\n\nTo launch the app\n\n`./run launch volume_rendering_xr`\n\nWith the command above, the path of the volume configuration file, volume density file and volume mask file will be passed to the application. You can also see how to manually run the application by going to the build folder and running\n\n`./applications/volume_rendering_xr/volume_rendering_xr -h`\n\n\n\n\n\n",
        "application_name": "volume_rendering_xr",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Streaming Synthetic Aperture Radar",
            "authors": [
                {
                    "name": "Dan Campbell",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "Amanda Butler",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "0.1.0",
            "changelog": {
                "0.1": "Initial Implementation - backprojection only, python only"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Radar",
                "SAR",
                "Synthetic Aperture"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "holoviz",
                        "version": "x.x.x"
                    }
                ]
            },
            "run": {
                "command": "python3 ./holosar.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Holoscan SAR\n\n## Description\nThis application is a demonstration of using Holoscan to construct Synthetic Aperture Radar (SAR) imagery from a data collection.  In current form, the data is assumed to be precollected and contained in a particular binary format.  It has been tested with 2 versions of the publicly available GOTCHA volumetric SAR data collection.  Python-based converters are included to manipulate the public datasets into the binary format expected by the application.  The application implements Backprojection for image formation.\n<!-- , in both Python and C++.  The Python implementation is accelerated via CuPy, and is backwardly compatible with Numpy.  The C++ implementation is accelerated with MatX.\n-->\n\n\n## Requirements\n* Holoscan (>=0.5)\n* Python implementation:\n    * Python3\n    * CuPy or Numpy\n    * Pillow\n* Scripts in ``deploy/`` will build and execute a docker environment that meets the requirements for systems using nvidia-docker\n<!-- \n    * C++:\n        * CUDA Toolkit\n* Requirements are conveniently met by building and deploying the Holoscan development container [add link]\n--> \n\n## Obtain and Format GOTCHA Dataset\n* Navigate to https://www.sdms.afrl.af.mil/index.php?collection=gotcha \n* Click the DOWNLOAD link below the images\n* Log in.  You may need to create an account to do so\n* Under \"GOTCHA Volumetric SAR Data Set Challenge Problem\" download \"Disc 1 of 2\".\n    * The data in \"Disc 2 of 2\" is compatible with this demo but not used\n* Unpack the contents of \"Disc 1 of 2\" into the ``data/`` directory.  This should create a subdirectry named ``GOTCHA-CP_Disc1/``\n* ``cd data``\n* ``python3 cp-large_convert.py``\n* This should create a data file named ``gotcha-cp-td-os.dat`` that has a file size 2766987672 bytes, and a md5sum of 554b509c2d5c2c3de8e5643983a9748d\n\n## Build and Use Docker Container (Optional)\n* This demonstration is distributed with tools to build a docker container that meets the demonstration's system requirements.  This approach will only work properly with nvidia-docker\n* From the demonstration root directory:\n* ```cd deploy```\n* ```bash build_application_container.sh``` - this will build the container\n* ```bash run_application_container.sh``` - this will launch a container that meets the demonstration system requirements\n\n## Build and Execute\n<!-- * Place ```.dat``` files in ```data/``` -->\n* Python: \n    * ```python3 holosar.py```\n\nThe application will create a window with the resolved SAR image, and update after each group of 100 pulses received.  The image represents the strength of reflectivity at points on the ground within the imaging window.  The text at the top of the window indicates the (X,Y) position of the collecting radar at the most recent pulse, along with the total count of pulses received.  The red line points in the direction of the collection vehicle's location at the most recent pulse.  \n\nA screen grab is included below for reference:\n\n![image](sar-grab.png)\n\n<!-- \n* C++:\n    * ```cd cpp```\n    ```mkdir build && cd build```\n    ```cmake ..```\n    ```make```\n    ```./holosar```\n* The application will generate one or more image files (depending on configuration) containing the SAR imagery.  For the backprojection algorithm, there will be image files for various number of ingested pulses, showing the evolution of the formed SAR imagery.  For Polar Format Algorithm, only complete images will be emitted.\n--> \n",
        "application_name": "synthetic_aperture_radar",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Basic PDW Pipeline",
            "authors": [
                {
                    "name": "Joshua Anderson",
                    "affiliation": "GTRI"
                },
                {
                    "name": "Christopher Jones",
                    "affiliation": "GTRI"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "UDP",
                "Radar",
                "Electronic Support"
            ],
            "ranking": 4,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/simple_pdw_pipeline",
                "workdir": "holohub_bin"
            }
        },
        "readme": "Simple PDW Pipeline\n==================================\n\nThis is a Holoscan pipeline that shows the possibility of using Holoscan as a\nPulse Description Word (PDW) generator. This is a process that takes in IQ\nsamples (signals represented using time-series complex numbers) and picks out\npeaks in the signal that may be transmissions from another source. These PDW\nprocessors are used to see what is transmitting in your area, be they radio\ntowers or radars.\n\nsiggen.c a signal generator written in C that will transmit\nthe input to this pipeline. \n\nBasicNetworkOpRx\n--------------------------\n\nThis uses the Basic Network Operator to read udp packets this operator is\ndocumented elsewhere. \n\nPacketToTensorOp\n-------------------------\n\nThis converts the bytes from the Basic Network Operator into the packets used\nin the rest of the pipeline. The format of the incoming packets is a 16-bit id\nfollowed by 8192 IQ samples each sample has the following format:\n16 bits (I)\n16 bits (Q)\n\nFFTOp\n------------------------\nDoes what it says on the tin. Takes an FFT of the input data. Also shifts data\nso that 0 Hz is centered.\n\n\nThresholdingOp:\n------------------------\nDetects samples over a threshold and then packetizes the runs of samples that\nare above the threshold as a \u201cpulse\u201d.\n\n\nPulseDescriptiorOp\n------------------------\nTakes simple statistics of input pulses. This is where I am most excited for\nfuture work, but that is not the point of this particular project.\n\n\nPulsePrinterOp\n----------------------\nPrints the pulse to screen. Also optionally sends packets to a BasicNetworkOpTx. \nThe transmitted network packets have the following format:\nEach of the following fields are 16bit unsigned integers\n  id\n  low bin\n  high bin\n  zero bin\n  sum power\n  max amplitude\n  average amplitude\n",
        "application_name": "simple_pdw_pipeline",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "H264 Endoscopy Tool Tracking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.0",
                "tested_versions": [
                    "1.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Endoscopy",
                "Video Decoding",
                "Video Encoding"
            ],
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "videodecoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videoencoder",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "./h264_endoscopy_tool_tracking h264_endoscopy_tool_tracking.yaml --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# H.264 Endoscopy Tool Tracking Application\n\nThe application showcases how to use H.264 video source as input to and output\nfrom the Holoscan pipeline. This application is a modified version of Endoscopy\nTool Tracking reference application in Holoscan SDK that supports H.264\nelementary streams as the input and output.\n\n_The H.264 video decode operators do not adjust framerate as it reads the elementary\nstream input. As a result the video stream will be displayed as quickly as the decoding can be\nperformed. This feature will be coming soon to a new version of the operator._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. The recording of the output can be enabled by setting\n`record_output` flag in the config file to `true`. If the `record_output` flag\nin the config file is set to `true`, the output of the pipeline is again\nrecorded to a H.264 elementary stream on the disk, file name / path for this\ncan be specified in the 'h264_endoscopy_tool_tracking.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded when building the application.\n\n## Building And Running H.264 Endoscopy Tool Tracking Application\n\nFollow steps in README.md from parents directory to build and run the Holohub\ndev container. Once inside the Holohub dev container, follow steps mentioned\nbelow to build and run H.264 Endoscopy Tool Tracking application.\n\n## Building the application\n\nOnce inside Holohub dev container, run below command from a top level Holohub\ndirectory.\n\n```bash\n./run build h264_endoscopy_tool_tracking\n```\n\n## Running the application\n\n* Running the application from the top level Holohub directory\n\n```bash\n./run launch h264_endoscopy_tool_tracking\n```\n\n* Running the application `h264_endoscopy_tool_tracking` from the build directory.\n\n```bash\ncd <build_dir>/applications/h264/h264_endoscopy_tool_tracking/ \\\n  && ./h264_endoscopy_tool_tracking --data <HOLOHUB_DATA_DIR>/endoscopy\n```\n\n## Enable recording of the output\n\nThe recording of the output can be enabled by setting `record_output` flag in\nthe config file\n`<build_dir>/applications/h264/endoscopy_tool_tracking/h264_endoscopy_tool_tracking.yaml`\nto `true`.\n\n",
        "application_name": "h264",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "H.264 Video Decode Reference Application",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.0",
                "tested_versions": [
                    "1.0.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "H.264",
                "Video Decoding"
            ],
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "videodecoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "./h264_video_decode h264_video_decode.yaml --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# H.264 Video Decode Reference Application\n\nThis is a minimal reference application demonstrating usage of H.264 video\ndecode operators. This application makes use of H.264 elementary stream reader\noperator for reading H.264 elementary stream input and uses Holoviz operator\nfor rendering decoded data to the native window.\n\n_The H.264 video decode operators do not adjust framerate as it reads the elementary\nstream input. As a result the video stream will be displayed as quickly as the decoding can be\nperformed. This feature will be coming soon to a new version of the operator._\n\n## Requirements\n\nThis application is configured to use H.264 elementary stream from endoscopy\nsample data as input. To use any other stream, the filename / path for the\ninput file can be specified in the 'h264_video_decode.yaml' file.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\nThe data is automatically downloaded when building the application.\n\n## Building And Running H.264 Endoscopy Tool Tracking Application\n\nFollow steps in README.md from parents directory to build and run the Holohub\ndev container. Once inside the Holohub dev container, follow steps mentioned\nbelow to build and run H.264 Endoscopy Tool Tracking application.\n\n## Building the application\n\nOnce inside Holohub dev container, run below command from a top level Holohub\ndirectory.\n\n```bash\n./run build h264_video_decode\n```\n\n## Running the application\n\n* Running the application from the top level Holohub directory\n\n```bash\n./run launch h264_video_decode\n```\n\n* Running the application `h264_video_decode` from the build directory.\n\n```bash\ncd <build_dir>/applications/h264/h264_video_decode/ \\\n  && ./h264_video_decode --data <HOLOHUB_DATA_DIR>/endoscopy\n```\n",
        "application_name": "h264",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Software Defined Radio FM Demodulation",
            "authors": [
                {
                    "name": "Adam Thompson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.4.0",
                "tested_versions": [
                    "0.4.0"
                ]
            },
            "platforms": [
                "amd64"
            ],
            "tags": [
                "Communications",
                "Aerospace",
                "Defence",
                "Lifesciences"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.23.2"
                    },
                    {
                        "name": "cupy",
                        "version": "11.4"
                    },
                    {
                        "name": "cusignal",
                        "version": "22.12"
                    },
                    {
                        "name": "SoapySDR",
                        "version": "0.8.1"
                    },
                    {
                        "name": "soapysdr-module-rtlsdr",
                        "version": "0.3"
                    },
                    {
                        "name": "pyaudio",
                        "version": "0.2.13"
                    }
                ]
            },
            "run": {
                "command": "python3 sdr_fm_demodulation.py",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# SDR FM Demodulation Application\n\nAs the \"Hello World\" application of software defined radio developers, this demonstration highlights real-time FM demodulation, resampling, and playback on GPU with NVIDIA's Holoscan SDK. In this example, we are using an inexpensive USB-based [RTL-SDR](https://www.rtl-sdr.com/) dongle to feed complex valued Radio Frequency (RF) samples into GPU memory and use [cuSignal](https://github.com/rapidsai/cusignal) functions to perform the relevant signal processing. The main objectives of this demonstration are to:\n- Highlight developer productivity in building an end-to-end streaming application with Holoscan and existing GPU-Accelerated Python libraries\n- Demonstrate how to construct and connect isolated units of work via Holoscan operators\n- Emphasize that operators created for this application can be re-used in other ones doing similar tasks\n\n# Running the Application\n\nPrior to running the application, the user needs to install the necessary dependencies (and, of course, plug in a SDR into your computer). This is most easily done in an Anaconda environment.\n\n```\nconda create --name holoscan-sdr-demo python=3.8\nconda activate holoscan-sdr-demo\nconda install -c conda-forge -c rapidsai -c nvidia cusignal soapysdr soapysdr-module-rtlsdr pyaudio\npip install holoscan\n```\n\nThe FM demodulation example can then be run via\n```\npython applications/sdr_fm_demodulation/sdr_fm_demodulation.py\n```\n",
        "application_name": "sdr_fm_demodulation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Model Benchmarking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.2",
                "tested_versions": [
                    "1.0.2"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Benchmarking"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "python3 <holohub_app_source>/model_benchmarking.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Benchmark Model\n\nThis application demonstrates an easy, quick and straightforward way to benchmark the scalaibility of\ninferencing with a model against a single video data stream in a typical Holoscan application. The\nvideo stream is played via a V4L2 loopback device. Then, the stream is preprocessed and fed to the\nmodel for inferencing. Then, the results are visualized after postprocessing.\n\n## Usage\n\nAs this application is, by default, set to use the \n[ultrasound segmentation](../ultrasound_segmentation/) example, you can build and run the ultrasound\nsegmentation example first, and then try running this benchmarking application.\n\nBuild and run the ultrasound segmentation application:\n```\n./run build ultrasound_segmentation && ./run launch ultrasound_segmentation cpp\n```\n\nNow, this benchmarking application can be built and run. However, before doing so, the v4l2loopback\nmust be run first. Check out the [notes and prerequisites\nhere](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/v4l2_camera#use-with-v4l2-loopback-devices) to play a\nvideo via a V4L2 loopback device. Assuming, everything is set up correctly, the ultrasound\nsegmentation example video could be run with the following command:\n\n> Note: we are playing the video to `/dev/video6` after running `sudo modprobe v4l2loopback video_nr=6 max_buffers=4`\n```\n$ ffmpeg -stream_loop -1 -re -i ./data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video6\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/\n  ...\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  ...\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560a570b0740] st: 0 edit list: 1 Missing key frame while searching for timestamp: 0\n...\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from './data/ultrasound_segmentation/ultrasound_256x256.avi':\n...\n```\n\nNow, the benchmarking application can be built and run:\n```\n./run build model_benchmarking\n./run launch model_benchmarking <cpp/python>\n```\n\nTo use a different video, the video can be played via the above `ffmpeg` command.\n\nTo use a different model, you can specify the data path in the \n`./run launch model_benchmarking <cpp/python>` command with the `-d` option, and the model name,\nresiding in the data path directory, with the `-m` option.\n\n```\n./run launch model_benchmarking <cpp/python> --extra_args \"-d <data_path> -m <model_name>\"\n```\n\nTo check the full list of options, run:\n```\n./run launch model_benchmarking <cpp/python> --extra_args \"-h\"\n```\n\n## Capabilities\nThis benchmarking application can be used to measure performance of parallel inferences for the same\nmodel on a single video stream. The `-l` option can be used to specify the number of parallel\ninferences to run. Then, the same model will be loaded to the GPU multiple times defined by the `-l`\nparameter. \n\nThe schematic diagram of this benchmarking application is in Figure 1. The visualization and\n(visualization + postprocessing) steps are marked as grey, as they can optionally be turned off\nwith, respectively, `-p` and `-i` options. The figure shows a single video data stream is used in\nthe application. Multiple ML/AI models are ingested by the Holoscan Inference operator to perform\ninference on a single data stream. The same ML model is replicated to be loaded multiple times to\nthe GPU in this application.\n\n![Benchmark Model](./model_benchmarking.png)\n\nFigure 1. The schematic diagram of the benchmarking application\n",
        "application_name": "model_benchmarking",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Model Benchmarking",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "cpp",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.2",
                "tested_versions": [
                    "1.0.2"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Benchmarking"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/model_benchmarking",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Benchmark Model\n\nThis application demonstrates an easy, quick and straightforward way to benchmark the scalaibility of\ninferencing with a model against a single video data stream in a typical Holoscan application. The\nvideo stream is played via a V4L2 loopback device. Then, the stream is preprocessed and fed to the\nmodel for inferencing. Then, the results are visualized after postprocessing.\n\n## Usage\n\nAs this application is, by default, set to use the \n[ultrasound segmentation](../ultrasound_segmentation/) example, you can build and run the ultrasound\nsegmentation example first, and then try running this benchmarking application.\n\nBuild and run the ultrasound segmentation application:\n```\n./run build ultrasound_segmentation && ./run launch ultrasound_segmentation cpp\n```\n\nNow, this benchmarking application can be built and run. However, before doing so, the v4l2loopback\nmust be run first. Check out the [notes and prerequisites\nhere](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/examples/v4l2_camera#use-with-v4l2-loopback-devices) to play a\nvideo via a V4L2 loopback device. Assuming, everything is set up correctly, the ultrasound\nsegmentation example video could be run with the following command:\n\n> Note: we are playing the video to `/dev/video6` after running `sudo modprobe v4l2loopback video_nr=6 max_buffers=4`\n```\n$ ffmpeg -stream_loop -1 -re -i ./data/ultrasound_segmentation/ultrasound_256x256.avi -pix_fmt yuyv422 -f v4l2 /dev/video6\nffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/\n  ...\n  libavutil      56. 31.100 / 56. 31.100\n  libavcodec     58. 54.100 / 58. 54.100\n  ...\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x560a570b0740] st: 0 edit list: 1 Missing key frame while searching for timestamp: 0\n...\nInput #0, mov,mp4,m4a,3gp,3g2,mj2, from './data/ultrasound_segmentation/ultrasound_256x256.avi':\n...\n```\n\nNow, the benchmarking application can be built and run:\n```\n./run build model_benchmarking\n./run launch model_benchmarking <cpp/python>\n```\n\nTo use a different video, the video can be played via the above `ffmpeg` command.\n\nTo use a different model, you can specify the data path in the \n`./run launch model_benchmarking <cpp/python>` command with the `-d` option, and the model name,\nresiding in the data path directory, with the `-m` option.\n\n```\n./run launch model_benchmarking <cpp/python> --extra_args \"-d <data_path> -m <model_name>\"\n```\n\nTo check the full list of options, run:\n```\n./run launch model_benchmarking <cpp/python> --extra_args \"-h\"\n```\n\n## Capabilities\nThis benchmarking application can be used to measure performance of parallel inferences for the same\nmodel on a single video stream. The `-l` option can be used to specify the number of parallel\ninferences to run. Then, the same model will be loaded to the GPU multiple times defined by the `-l`\nparameter. \n\nThe schematic diagram of this benchmarking application is in Figure 1. The visualization and\n(visualization + postprocessing) steps are marked as grey, as they can optionally be turned off\nwith, respectively, `-p` and `-i` options. The figure shows a single video data stream is used in\nthe application. Multiple ML/AI models are ingested by the Holoscan Inference operator to perform\ninference on a single data stream. The same ML model is replicated to be loaded multiple times to\nthe GPU in this application.\n\n![Benchmark Model](./model_benchmarking.png)\n\nFigure 1. The schematic diagram of the benchmarking application\n",
        "application_name": "model_benchmarking",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "SSD Detection Application",
            "authors": [
                {
                    "name": "Jin Li",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.1",
            "changelog": {
                "1.0": "Initial Release",
                "1.1": "Update from TensorRTInferenceOp to InferenceOp"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "SSD",
                "bounding box",
                "Detection"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.22.3"
                    },
                    {
                        "name": "cupy",
                        "version": "11.6.0"
                    }
                ]
            }
        },
        "readme": "# SSD Detection Application\n## Model\nWe can train the [SSD model from NVIDIA DeepLearningExamples repo]((https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)) with any data of our choosing. Here for the purpose of demonstrating the deployment process, we will use a SSD model checkpoint that is only trained for the [demo video clip](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data). \n\nPlease download the models [at this NGC Resource](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/ssd_surgical_tool_detection_model) for `epoch_24.pt`, `epoch24_nms.onnx` and `epoch24.onnx`. You can go through the next steps of Model Conversion to ONNX to convert `epoch_24.pt` into `epoch24_nms.onnx` and `epoch24.onnx`, or use the downloaded ONNX models directly.\n\n\n### Model Conversion to ONNX\nThe scripts we need to export the model from .pt checkpoint to the ONNX format are all within this dir `./scripts`. It is a two step process.\n\n\n Step 1: Export the trained checkpoint to ONNX. <br> We use [`export_to_onnx_ssd.py`](./scripts/export_to_onnx_ssd.py) if we want to use the model as is without NMS, or [`export_to_onnx_ssd_nms.py`](./scripts/export_to_onnx_ssd_nms.py) to prepare the model with NMS. \n Let's assume the re-trained SSD model checkpoint from the [repo](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD) is saved as `epoch_24.pt`.\n The export process is \n```\n# For exporting the original ONNX model\n python export_to_onnx_ssd.py --model epoch_24.pt  --outpath epoch24_temp.onnx\n```\n```\n# For preparing to add the NMS step to ONNX model\npython export_to_onnx_ssd_nms.py --model epoch_24.pt  --outpath epoch24_nms_temp.onnx\n```\nStep 2: modify input shape. <br> Step 1 produces a onnx model with input shape `[1, 3, 300, 300]`, but we will want to modify the input node to have shape `[1, 300, 300, 3]` or in general `[batch_size, height, width, channels]` for compatibility and easy of deployment in the Holoscan SDK. If we want to incorporate the NMS operation in the the ONNX model, we could add a `EfficientNMS_TRT` op, which is documented in [`graph_surgeon_ssd.py`](./scripts/graph_surgeon_ssd.py)'s nms related block.\n```\n# For exporting the original ONNX model\npython graph_surgeon_ssd.py --orig_model epoch24_temp.onnx --new_model epoch24.onnx\n```\n```\n# For adding the NMS step to ONNX model, use --nms\npython graph_surgeon_ssd.py --orig_model epoch24_nms_temp.onnx --new_model epoch24_nms.onnx --nms\n```\n\nNote that\n - `epoch24.onnx` is used in `ssd_step1.py` and `ssd_step2_route1.py`\n - `epoch24_nms.onnx` is used in `ssd_step2_route2.py` and `ssd_step2_route2_render_labels.py` \n\n## Data\nFor this application we will use the same [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) as the Holoscan SDK reference applications.\n\n## Requirements\nThere are two requirements \n1. To run `ssd_step1.py` and `ssd_step2_route1.py` with the original exported model, we need the installation of PyTorch and CuPy. \n<br> To run `ssd_step2_route2.py` and `ssd_step2_route2_render_labels.py` with the exported model with additional NMS layer in ONNX, we need the installation of CuPy. \n<br> If you're using the dGPU on the devkit, since there are no prebuilt PyTorch wheels for aarch64 dGPU, the simplest way is to modify the Dockerfile and build from source; if you're on x86 or using the iGPU on the devkit, there should be existing prebuilt PyTorch wheels.\n<br> If you choose to build the SDK from source, you can find the [modified Dockerfile here](./docker/Dockerfile) to replace the SDK repo [Dockerfile](https://github.com/nvidia-holoscan/holoscan-sdk/blob/main/Dockerfile) to satisfy the installation requirements. \n<br> The main changes in Dockerfile for dGPU: the base image changed to `nvcr.io/nvidia/pytorch:22.03-py3` instead of the `nvcr.io/nvidia/tensorrt:22.03-py3` as dGPU's base image; adding the installation of NVTX for optional profiling.\n<br>Build the SDK container following the [README instructions](https://github.com/nvidia-holoscan/holoscan-sdk#recommended-using-the-run-script). \n<br>\n Make sure the directory containing this application and the directory containing the NGC data and models are mounted in the container. Add the `-v` mount options to the `docker run` command launched by `./run launch` in the SDK repo. \n\n2. Make sure the model and data are accessible by the application. \n<br> Make sure the yaml files `ssd_endo_model.yaml` and `ssd_endo_model_with_NMS.yaml` are pointing to the right locations for the ONNX model and data. The assumption in the yaml file is that the `epoch24_nms.onnx` and `epoch24.onnx` are located at:\n```\nmodel_file_path: /byom/models/endo_ssd/epoch24_nms.onnx \nengine_cache_dir: /byom/models/endo_ssd/epoch24_nms_engines\n```\nand / or\n```\nmodel_file_path: /byom/models/endo_ssd/epoch24.onnx\nengine_cache_dir: /byom/models/endo_ssd/epoch24_engines\n```\nThe [Endoscopy Sample Data](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data) is assumed to be at \n```\n/workspace/holoscan-sdk/data/endoscopy\n```\nPlease check and modify the paths to model and data in the yaml file if needed.\n\n## Building the application\nPlease refer to the README under [./app_dev_process](./app_dev_process/README.md) to see the process of building the applications.\n\n## Running the application\nRun the incrementally improved Python applications by:\n```\npython ssd_step1.py\n\npython ssd_step2_route1.py\n\npython ssd_step2_route2.py\n\npython ssd_step2_route2_render_labels.py --labelfile endo_ref_data_labels.csv\n```",
        "application_name": "ssd_detection_endoscopy_tools",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "CV-CUDA: Basic Interoperability",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "CV-CUDA",
                "Computer Vision",
                "CV"
            ],
            "ranking": 1,
            "dependencies": {
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "libraries": [
                    {
                        "name": "cvcuda",
                        "version": "0.3.1-beta"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/cvcuda_basic.py --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Simple CV-CUDA application\n\nThis application demonstrates seamless interoperability between Holoscan tensors and CV-CUDA tensors. The image processing pipeline is just a simple flip of the video orientation.\n\nNote that the C++ version of this application currently requires extra code to handle conversion\nback and forth between CV-CUDA and Holoscan tensor types. On the Python side, the conversion is\ntrivial due to the support for the [DLPack Python\nspecification](https://dmlc.github.io/dlpack/latest/python_spec.html) in both CV-CUDA and Holoscan.\nWe provide two [operators](../../operators/cvcuda_holoscan_interop/README.md) to handle the\ninteroperability between CVCUDA and Holoscan tensors.\n\n# Using the docker file\n\nThis application requires a compiled version of [CV-CUDA](https://github.com/CVCUDA/CV-CUDA).\nFor simplicity a DockerFile is available. To generate the container run:\n\n```bash\n./dev_container build --docker_file ./applications/cvcuda_basic/Dockerfile\n```\n\nThe C++ version of the application can then be built by launching this container and using the provided `run` script.\n\n```bash\n./dev_container launch\n./run build cvcuda_basic\n```\n\n# Running the Application\n\nThis application uses the endoscopy dataset as an example. The `run build` command above will automatically download it. This application is then run inside the container.\n\n```bash\n./dev_container launch\n```\n\nThe Python version of the simple CV-CUDA pipeline example can be run via\n```\npython applications/cvcuda_basic/python/cvcuda_basic.py --data=/workspace/holohub/data/endoscopy\n```\n\nor using the run script\n\n```bash\n./run launch cvcuda_basic python\n```\n\nThe C++ version of the simple CV-CUDA pipeline example can then be run via\n```\n./build/applications/cvcuda_basic/cpp/cvcuda_basic --data=/workspace/holohub/data/endoscopy\n```\n\nor using the run script\n\n```bash\n./run launch cvcuda_basic cpp\n```\n",
        "application_name": "cvcuda_basic",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "CV-CUDA: Basic Interoperability",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "CV-CUDA",
                "Computer Vision",
                "CV"
            ],
            "ranking": 1,
            "dependencies": {
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "libraries": [
                    {
                        "name": "cvcuda",
                        "version": "0.3.1-beta"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/cvcuda_basic --data <holohub_data_dir>/endoscopy",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Simple CV-CUDA application\n\nThis application demonstrates seamless interoperability between Holoscan tensors and CV-CUDA tensors. The image processing pipeline is just a simple flip of the video orientation.\n\nNote that the C++ version of this application currently requires extra code to handle conversion\nback and forth between CV-CUDA and Holoscan tensor types. On the Python side, the conversion is\ntrivial due to the support for the [DLPack Python\nspecification](https://dmlc.github.io/dlpack/latest/python_spec.html) in both CV-CUDA and Holoscan.\nWe provide two [operators](../../operators/cvcuda_holoscan_interop/README.md) to handle the\ninteroperability between CVCUDA and Holoscan tensors.\n\n# Using the docker file\n\nThis application requires a compiled version of [CV-CUDA](https://github.com/CVCUDA/CV-CUDA).\nFor simplicity a DockerFile is available. To generate the container run:\n\n```bash\n./dev_container build --docker_file ./applications/cvcuda_basic/Dockerfile\n```\n\nThe C++ version of the application can then be built by launching this container and using the provided `run` script.\n\n```bash\n./dev_container launch\n./run build cvcuda_basic\n```\n\n# Running the Application\n\nThis application uses the endoscopy dataset as an example. The `run build` command above will automatically download it. This application is then run inside the container.\n\n```bash\n./dev_container launch\n```\n\nThe Python version of the simple CV-CUDA pipeline example can be run via\n```\npython applications/cvcuda_basic/python/cvcuda_basic.py --data=/workspace/holohub/data/endoscopy\n```\n\nor using the run script\n\n```bash\n./run launch cvcuda_basic python\n```\n\nThe C++ version of the simple CV-CUDA pipeline example can then be run via\n```\n./build/applications/cvcuda_basic/cpp/cvcuda_basic --data=/workspace/holohub/data/endoscopy\n```\n\nor using the run script\n\n```bash\n./run launch cvcuda_basic cpp\n```\n",
        "application_name": "cvcuda_basic",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "WebRTC Video Server",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Server",
                "Video"
            ],
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    },
                    {
                        "name": "aiohttp",
                        "version": "3.8.5"
                    },
                    {
                        "name": "numpy",
                        "version": "1.23"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/webrtc_server.py",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# WebRTC Video Server\n\n![](screenshot.png)<br>\n\nThis app reads video frames from a file and sends it to a browser using WebRTC.\n\nThe app starts a web server, the pipeline starts when a browser is connected to the web server and the `Start` button is pressed. The pipeline stops when the `Stop` button is pressed.\n\n```mermaid\nflowchart LR\n    subgraph Server\n        A[(VideoFile)] --> VideoStreamReplayerOp\n        VideoStreamReplayerOp --> FormatConverterOp\n        FormatConverterOp --> WebRTCServerOp\n        WebServer\n    end\n    subgraph Client\n        WebServer <--> Browser\n        WebRTCServerOp <--> Browser\n    end\n```\n\n> **_NOTE:_** When using VPN there might be a delay of several seconds between pressing the `Start` button and the first video frames are display. The reason for this is that the STUN server `stun.l.google.com:19302` used by default might not be available when VPN is active and the missing support for Trickle ICE in the used aiortc library. Trickle ICE is an optimization to speed up connection establishment. Normally, possible connections paths are tested one after another. If connections time out this is blocking the whole process. Trickle ICE checks each possible connection path in parallel so the connection timing out won't block the process.\n\n## Prerequisites\n\nThe app is using [AIOHTTP](https://docs.aiohttp.org/en/stable/) for the web server and [AIORTC](https://github.com/aiortc/aiortc) for WebRTC. Install both using pip.\n\n```bash\npip install aiohttp aiortc\n```\n\n## Run Instructions\n\nRun the command:\n\n```bash\n./run launch webrtc_video_server\n```\n\nOn the same machine open a browser and connect to `127.0.0.1:8080`. You can also connect from a different machine by connecting to the IP address the app is running on.\n\nPress the `Start` button. Video frames are displayed. To stop, press the `Stop` button. Pressing `Start` again will continue the video.\n\n### Command Line Arguments\n\n```\nusage: webrtc_server.py [-h] [--cert-file CERT_FILE] [--key-file KEY_FILE] [--host HOST] [--port PORT] [--verbose] [--ice-server ICE_SERVER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --cert-file CERT_FILE\n                        SSL certificate file (for HTTPS)\n  --key-file KEY_FILE   SSL key file (for HTTPS)\n  --host HOST           Host for HTTP server (default: 0.0.0.0)\n  --port PORT           Port for HTTP server (default: 8080)\n  --verbose, -v\n  --ice-server ICE_SERVER\n                        ICE server config in the form of `turn:<ip>:<port>[<username>:<password>]` or `stun:<ip>:<port>`. This option can be specified multiple times to add multiple ICE servers.\n```\n\n## Running With TURN server\n\nA TURN server may be needed if you're running in a containerized environment without host networking (e.g. Kubernetes or Docker). Here are some basic steps to run this example with a TURN server.\n\nRun the TURN server in the same machine that you're running the app on.\n\n**Note: It is strongly recommended to run the TURN server with docker network=host for best performance**\n\n```\n# This is the external IP address of the machine running the TURN server\nexport TURN_SERVER_EXTERNAL_IP=\"<ip>\"\n\n# Command below use admin:admin as the username and password as an example\ndocker run -d --rm --network=host instrumentisto/coturn \\\n    -n --log-file=stdout \\\n    --external-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --listening-ip=$TURN_SERVER_EXTERNAL_IP \\\n    --lt-cred-mech --fingerprint \\\n    --user=admin:admin \\\n    --no-multicast-peers \\\n    --verbose \\\n    --realm=default.realm.org\n```\n\nThen you can pass in the TURN server config into the app\n\n```\npython webrtc_server.py --ice-server \"turn:<ip>:3478[admin:admin]\"\n```\n\nThis will enable you to access the webRTC browser application from different machines.",
        "application_name": "webrtc_video_server",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Colonoscopy segmentation",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Colonoscopy",
                "Classification"
            ],
            "ranking": 1,
            "dependencies": {
                "data": "https://ngc.nvidia.com/resources/colonoscopy_sample_app_data"
            },
            "run": {
                "command": "python3 colonoscopy_segmentation.py --data <holohub_data_dir>/colonoscopy_segmentation",
                "workdir": "holohub_app_source"
            }
        },
        "readme": "# Colonoscopy Polyp Segmentation\n\nFull workflow including a generic visualization of segmentation results from a polyp segmentation models.\n\n### Requirements\n\n- Python 3.8+\n- The provided applications are configured to either use the AJA capture card for input stream, or a pre-recorded video of the colonoscopy data (replayer). Follow the [setup instructions from the user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/aja_setup.html) to use the AJA capture card.\n\n### Data\n\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI Colonoscopy Segmentation of Polyps](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_colonoscopy_sample_data)\n\nThe data is automatically downloaded and converted to the correct format when building the application.\nIf you want to manually convert the video data, please refer to the instructions for using the [convert_video_to_gxf_entities](https://github.com/nvidia-holoscan/holoscan-sdk/tree/main/scripts#convert_video_to_gxf_entitiespy) script.\n\n### Run Instructions\n\nTo run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\nNext, run the commands of your choice:\n\n* Using a pre-recorded video\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/colonoscopy_segmentation\n    python3 colonoscopy_segmentation.py --source=replayer --data=<DATA_DIR>/colonoscopy_segmentation\n    ```\n\n* Using an AJA card\n    ```bash\n    cd <HOLOHUB_SOURCE_DIR>/applications/colonoscopy_segmentation\n    python3 colonoscopy_segmentation.py --source=aja\n    ```\n\n### Holoscan SDK version\n\nColonoscopy segmentation application in HoloHub requires version 0.6+ of the Holoscan SDK.\nIf the Holoscan SDK version is 0.5 or lower, following code changes must be made in the application:\n\n* In python/CMakeLists.txt: update the holoscan SDK version from `0.6` to `0.5`\n* In python/multiai_ultrasound.py: `InferenceOp` is replaced with `MultiAIInferenceOp`\n",
        "application_name": "colonoscopy_segmentation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Basic Networking Benchmark",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "UDP",
                "Ethernet",
                "IP",
                "TCP"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "operators": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/basic_networking_ping.py",
                "workdir": "holohub_app_bin"
            }
        },
        "readme": "# Basic Networking Ping\n\nThis application takes the existing ping example that runs over Holoscan ports and instead uses the basic\nnetwork operator to run over a UDP socket.\n\nThe basic network operator allows users to send and receive UDP messages over a standard Linux socket.\nSeparate transmit and receive operators are provided so they can run independently and better suit\nthe needs of the application.\n\n### Configuration\n\nThe application is configured using the file basic_networking_ping_rx.yaml or basic_networking_ping_tx.yaml,\nwhere RX will receive packets and TX will transmit. Depending on how the machine is configured, the IP and\nUDP port likely need to be configured. All other settings do not need to be changed.\n\nPlease refer to the basic network operator documentation for more configuration information.\n\n### Requirements\n\nThis application requires:\n1. Linux\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nRunning the sample uses the standard HoloHub `run` script:\n\n\n```bash\n./run launch basic_networking_ping <language> --configure-args config_file.yaml\n```\n\nLanguage can be either C++ or Python.\n",
        "application_name": "basic_networking_ping",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Basic Networking Ping",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Networking",
                "Network",
                "UDP",
                "IP"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/basic_networking_ping",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Basic Networking Ping\n\nThis application takes the existing ping example that runs over Holoscan ports and instead uses the basic\nnetwork operator to run over a UDP socket.\n\nThe basic network operator allows users to send and receive UDP messages over a standard Linux socket.\nSeparate transmit and receive operators are provided so they can run independently and better suit\nthe needs of the application.\n\n### Configuration\n\nThe application is configured using the file basic_networking_ping_rx.yaml or basic_networking_ping_tx.yaml,\nwhere RX will receive packets and TX will transmit. Depending on how the machine is configured, the IP and\nUDP port likely need to be configured. All other settings do not need to be changed.\n\nPlease refer to the basic network operator documentation for more configuration information.\n\n### Requirements\n\nThis application requires:\n1. Linux\n\n### Build Instructions\n\nPlease refer to the top level Holohub README.md file for information on how to build this application.\n\n### Run Instructions\n\nRunning the sample uses the standard HoloHub `run` script:\n\n\n```bash\n./run launch basic_networking_ping <language> --configure-args config_file.yaml\n```\n\nLanguage can be either C++ or Python.\n",
        "application_name": "basic_networking_ping",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "matlab_gpu_coder",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                },
                {
                    "name": "MathWorks Team",
                    "affiliation": "MathWorks"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "MATLAB",
                "Ultrasound",
                "Beamforming",
                "CUDA"
            ],
            "ranking": 4,
            "dependencies": {
                "operators": [
                    {
                        "name": "MATLAB",
                        "version": "R2023b"
                    }
                ]
            }
        },
        "readme": "# Ultrasound Beamforming with MATLAB GPU Coder\n\nThis application does ultrasound beamforming of simulated data. The beamforming algorithm is implemented in [MATLAB](https://uk.mathworks.com/products/matlab.html) and MATLAB [GPU Coder](https://www.mathworks.com/products/gpu-coder.html) is used to generate CUDA code. The CUDA code is compiled on the target platform, either x86 or arm64, and called by the application on each Holoscan operator `compute()` call. When the application is run, Holoviz will display the beamformed data in real time.\n\n<img src=\"resources/architecture_diagram.png\" alt=\"isolated\" width=\"800\"/>\n\n## MATLAB Requirements\n\nThis application has been tested on MATLAB R2023b, but should work on other versions of MATLAB.\n\nThe required MATLAB Toolboxes are:\n\n* [GPU Coder Toolbox](https://www.mathworks.com/products/gpu-coder.html)\n* To generate simulated data:\n    * [Phased Array System Toolbox](https://uk.mathworks.com/products/phased-array.html)\n    * [Communications Toolbox](https://uk.mathworks.com/products/communications.html)\n* To compile on Jetson devices (arm64):\n    * [Embedded Coder Toolbox](https://uk.mathworks.com/products/embedded-coder.html)\n    * [MATLAB Coder Support Package for NVIDIA Jetson and NVIDIA DRIVE Platforms](https://uk.mathworks.com/help/supportpkg/nvidia/)\n    * Look at [this documentation](https://uk.mathworks.com/help/coder/nvidia/ug/install-and-setup-prerequisites.html) for making CUDA accessible to the MATLAB host machine\n\n## Folder Structure\n\n```sh\nmatlab_gpu_coder\n\u251c\u2500\u2500 data  # Data is generated with generate_data.mlx\n\u2502   \u2514\u2500\u2500 ultrasound_beamforming.bin  # Simulated ultrasound data\n\u251c\u2500\u2500 matlab  # MATLAB files\n\u2502   \u251c\u2500\u2500 generate_beamform_jetson.m  # MATLAB script to generate CUDA DLLs on Jetson\n\u2502   \u251c\u2500\u2500 generate_beamform_x86.m  # MATLAB script to generate CUDA DLLs on x86\n\u2502   \u251c\u2500\u2500 generate_data.mlx  # MATLAB script to generate simulated data\n\u2502   \u2514\u2500\u2500 matlab_beamform.m  # MATLAB function that CUDA code is generated from\n\u251c\u2500\u2500 holoscan_matlab_utils.cu  # Utility functions/kernels\n\u251c\u2500\u2500 holoscan_matlab_utils.h  # Utility functions/kernels\n\u251c\u2500\u2500 main.cpp  # Ultrasound beamforming app\n\u2514\u2500\u2500 matlab_beamform.yaml  # Ultrasound beamforming config\n```\n\n## Generate Simulated Data\n\nSimply run the script `matlab/generate_data.mlx` from MATLAB and a binary file `ultrasound_beamforming.bin` will be written to a top-level `data` folder. The binary file contains the simulated ultrasound data, prior to beamforming.\n\n## Generate CUDA Code with MATLAB GPU Coder\n\n### x86: Ubuntu >= 20.04\n\nIn order to generate the CUDA Code, start MATLAB and `cd` to the `matlab_gpu_coder/matlab` folder and open the `generate_beamform_x86.m` script. Run the script and a folder `codegen/dll/matlab_beamform` will be generated in the `matlab_gpu_coder` folder.\n\n### arm64: Jetson\n\nOn an x86 computer with MATLAB installed, `cd` to the `matlab_gpu_coder/matlab` folder and open the `generate_beamform_jetson.m` script. Having an `ssh` connection to the Jetson device you want to build the CUDA DLLs on, specify the parameters of that connection in the `hwobj` on line 7, also replace `<ABSOLUTE_PATH>` of `cfg.Hardware.BuildDir` on line 39, as the absolute path (on the Jetson device) to `holohub` folder. Run the script and a folder `MATLAB_ws` will be created in the `matlab_gpu_coder` folder.\n\n## Configure/Build/Run Application\n\nStart by pulling the Holoscan image from NVIDIA NGC:\n```sh\ndocker pull nvcr.io/nvidia/clara-holoscan/holoscan:v1.0.3-dgpu\n```\n\n### Configure Holoscan for MATLAB\n\n#### x86: Ubuntu >= 20.04\n\nDefine the environment variable:\n```sh\nexport MATLAB_ROOT=/usr/local/MATLAB/R2023b\n```\nwhere you replace the path with the location of your MATLAB install.\n\nNext, run the HoloHub Docker container:\n```sh\n./dev_container launch --img nvcr.io/nvidia/clara-holoscan/holoscan:v1.0.3-dgpu --add-volume ${MATLAB_ROOT}\n```\n\nInside the container, define the environment variable (update the MATLAB version if needed):\n```sh\nexport MATLAB_ROOT=/workspace/volumes/R2023b\n```\n\n#### arm64: Jetson\n\nThe folder `MATLAB_ws`, created by MATLAB, mirrors the folder structure of the host machine and is therefore different from one user to another; hence, we need to specify the path to the `codegen` folder in the `matlab_gpu_coder/CMakeLists.txt`, in order for the build to find the required libraries. Set the variable `REL_PTH_MATLAB_CODEGEN` to the relative path where the `codegen` folder is located in the `MATLAB_ws` folder. For example, if GPU Coder created the following folder structure on the Jetson device:\n```sh\nmatlab_gpu_coder\n\u251c\u2500\u2500 MATLAB_ws\n    \u251c\u2500\u2500 R2023b\n        \u251c\u2500\u2500 C\n            \u251c\u2500\u2500 Users\n                \u251c\u2500\u2500 Jensen\n                    \u251c\u2500\u2500 holohub\n                        \u251c\u2500\u2500 applications\n                            \u251c\u2500\u2500 matlab_gpu_coder\n                                \u251c\u2500\u2500 matlab\n                                    \u251c\u2500\u2500 codegen\n```\nthe variable should be set as:\n```sh\nREL_PTH_MATLAB_CODEGEN=MATLAB_ws/R2023b/C/Users/Jensen/holohub/applications/matlab_gpu_coder/matlab/codegen\n```\n\nNext, run the HoloHub Docker container:\n```sh\n./dev_container launch --img nvcr.io/nvidia/clara-holoscan/holoscan:v1.0.3-dgpu\n```\n\n### Build and Run\n\nRun the below commands to build the application:\n```sh\ncd applications/matlab_gpu_coder\nmkdir -p build\ncmake -S . -B build\ncmake --build build\n```\n\nThe application can then be run with:\n```sh\ncd build\n./matlab_beamform\n```\n",
        "application_name": "matlab_gpu_coder",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "HoloChat-local",
            "authors": [
                {
                    "name": "Nigel Nelson",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "0.1.0",
            "changelog": {
                "0.1.0": "Beta release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "LLM",
                "Vector Database",
                "AI-Assistant"
            ],
            "ranking": 4,
            "dependencies": {
                "OSS": [
                    {
                        "name": "Llama.cpp",
                        "version": "cf9b08485c4c2d4d945c6e74fe20f273a38b6104"
                    },
                    {
                        "name": "LangChain",
                        "version": "0.0.277"
                    }
                ]
            },
            "run": {
                "command": "make -C ./applications/holochat_local run_holochat",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# HoloChat-local\n\nHoloChat-local is an AI-driven chatbot, built on top of a locally hosted Code Llama model which acts as developer's copilot in Holoscan development. The Code Llama model leverages a vector database comprised of the Holoscan SDK repository and user guide, enabling HoloChat to answer general questions about Holoscan, as well act as a Holoscan SDK coding assistant.\n<p align=\"center\">\n  <kbd style=\"border: 2px solid black;\">\n    <img src=\"holochat_demo.gif\" alt=\"HoloChat Demo\" />\n  </kbd>\n</p>\n\n## Hardware Requirements: \ud83d\udc49\ud83d\udcbb\n- **Processor:** x86/Arm64\n- **GPU**: NVIDIA dGPU w/ >= 28 GB VRAM\n- **Memory**: \\>= 28 GB of available disk memory\n  - Needed to download [fine-tuned Code Llama 34B](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/models/phind-codellama-34b-v2-q5_k_m) and [BGE-Large](https://huggingface.co/BAAI/bge-large-en) embedding model\n\n*Tested using [NVIDIA IGX Orin](https://www.nvidia.com/en-us/edge-computing/products/igx/) w/ RTX A6000 and [Dell Precision 5820 Workstation](https://www.dell.com/en-us/shop/desktop-computers/precision-5820-tower-workstation/spd/precision-5820-workstation/xctopt5820us) w/ RTX A6000, both running Ubuntu 20.04 LTS*\n\n## Dependencies: \ud83d\udce6\n- [NVIDIA Drivers >= 520.61.05](https://www.nvidia.com/download/index.aspx)\n- [Docker](https://docs.docker.com/desktop/install/linux-install/)\n\n## Running HoloChat-local: \ud83c\udfc3\ud83d\udca8\n\n### Build Notes:\n\n**Build Time:**\n- HoloChat uses a [PyTorch container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) from [NGC](https://catalog.ngc.nvidia.com/?filters=&orderBy=weightPopularDESC&query=) and two large models that are downloaded from [HuggingFace.co](https://huggingface.co/). As such, the first time building this application **will likely take ~45 minutes** depending on your internet speeds. However, this is a one-time set-up and subsequent runs of HoloChat should take seconds to launch.\n\n**Build Location:**\n- HoloChat downloads ~28 GB of model data to the `holochat/models` directory. As such, it is **recommended** to only run this application on a disk drive with ample storage (ex: the 500 GB SSD included with NVIDIA IGX Orin).\n\n\n### Running Instructions:\n\nIf connecting to your machine via SSH, be sure to forward the ports 7860 & 8080:\n```bash\nssh <user_name>@<IP address> -L 7860:localhost:7860 -L 8080:localhost:8080\n```\n\n**Recommended:**\n```bash\nmake run_holochat\n```\n\n**Manual Setup:**\n```bash\nmake build_llamaCpp\nmake build_db\nmake download_llama\nmake start_holochat\n```\nHoloChat is hosted at http://127.0.0.1:7860/. Open this URL in a web browser to begin interacting with your personal Holoscan SDK assistant!\n\n## Usage Notes: \ud83d\uddd2\ufe0f \n\n### Intended use: \ud83c\udfaf\n  >HoloChat is developed to accelerate and assist Holoscan developers\u2019 learning and development. HoloChat serves as an intuitive chat interface, enabling users to pose natural language queries related to the Holoscan SDK. Whether seeking general information about the SDK or specific coding insights, users can obtain immediate responses thanks to the underlying Large Language Model (LLM) and vector database, both hosted 100% locally on user\u2019s NVIDIA IGX Orin.\n  > \n  >HoloChat's local hosting approach is designed to provide developers with the same benefits as popular closed-source chatbots, all while eliminating the privacy and security concerns associated with sending data to 3rd-party remote servers for processing. However, what makes HoloChat unique from a general chatbot is that it is given access to the Holoscan SDK repository, the HoloHub repository, and the Holoscan SDK user guide. This essentially allows users to engage in natural language conversations with these documents, gaining instant access to the information they need, thus sparing them the task of sifting through vast amounts of documentation themselves.\n\n### Known Limitations: \u26a0\ufe0f\ud83d\udea7\nBefore diving into how to make the most of HoloChat, it's crucial to understand and acknowledge its known limitations. These limitations can guide you in adopting the best practices below, which will help you navigate and mitigate these issues effectively.\n* **Hallucinations:** Occasionally, HoloChat may provide responses that are not entirely accurate. It's advisable to approach answers with a healthy degree of skepticism.\n* **Memory Loss:** CodeLlama's limited attention window may lead to the loss of previous conversation history. To mitigate this, consider restarting the application to clear the chat history when necessary.\n* **Limited Support for Stack Traces**: HoloChat's knowledge is based on the Holoscan repository and the user guide, which lack large collections of stack trace data. Consequently, HoloChat may face challenges when assisting with stack traces.\n\n### Best Practices: \u2705\ud83d\udc4d\nWhile users should be aware of the above limitations, following the recommended tips will drastically minimize these possible shortcomings. In general, the more detailed and precise a question is, the better the results will be. Some best practices when asking questions are:\n* **Be Verbose**: If you want to create an application, specify which operators should be used if possible (HolovizOp, V4L2VideoCaptureOp, InferenceOp, etc.).\n* **Be Specific**: The less open-ended a question is the less likely the model will hallucinate.\n* **Specify Programming Language**: If asking for code, include the desired language (Python or C++).\n* **Provide Code Snippets:** If debugging errors include as much relevant information as possible. Copy and paste the code snippet that produces the error, the abbreviated stack trace, and describe any changes that may have introduced the error.\n\nIn order to demonstrate how to get the most out of HoloChat two example questions are posed below. These examples illustrate how a user can refine their questions and as a result, improve the responses they receive: \n\n---\n**Worst\ud83d\udc4e:**\n\u201cCreate an app that predicts the labels associated with a video\u201d\n\n**Better\ud83d\udc4c:**\n\u201cCreate a Python app that takes video input and sends it through a model for inference.\u201d\n\n**Best\ud83d\ude4c:**\n\u201cCreate a Python Holoscan application that receives streaming video input, and passes that video input into a pytorch classification model for inference. Then, collect the model\u2019s predicted class and use Holoviz to display the class label on each video frame.\u201d\n\n---\n**Worst\ud83d\udc4e:**\n\u201cWhat os can I use?\u201d\n\n**Better\ud83d\udc4c:**\n\u201cWhat operating system can I use with Holoscan?\u201d\n\n**Best\ud83d\ude4c:**\n\u201cCan I use MacOS with the Holoscan SDK?\u201d\n\n\n## Appendix:\n### Meta Terms of Use:\nBy using the Llama 2 model, you are agreeing to the terms and conditions of the [license](https://ai.meta.com/llama/license/), [acceptable use policy](https://ai.meta.com/llama/use-policy/) and Meta\u2019s [privacy policy](https://www.facebook.com/privacy/policy/).\n### Implementation Details: \n  >HoloChat operates by taking user input and comparing it to the text stored within the vector database, which is comprised of Holoscan SDK information. The most relevant text segments from SDK code and the user guide are then appended to the user's query. This approach allows [Phind\u2019s fine-tuned CodeLlama 34b](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2) to answer questions about the Holoscan SDK, without being explicitly trained on SDK data.\n  >\n  >However, there is a drawback to this method - the most relevant documentation is not always found within the vector database. Since the user's question serves as the search query, queries that are too simplistic or abbreviated may fail to extract the most relevant documents from the vector database. As a consequence, the LLM will then lack the necessary context, leading to poor and potentially inaccurate responses. This occurs because LLMs strive to provide the most probable response to a question, and without adequate context, they hallucinate to fill in these knowledge gaps.\n\n",
        "application_name": "holochat_local",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Multiai",
                "SSD",
                "bounding box",
                "Detection",
                "MONAI",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "libraries": [
                    {
                        "name": "numpy",
                        "version": "1.22.3"
                    },
                    {
                        "name": "cupy",
                        "version": "11.6.0"
                    }
                ]
            },
            "run": {
                "command": "python3 <holohub_app_source>/multi_ai.py --data <holohub_data_dir>/",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation \nIn this application, we show how to build a Multi AI application with detection and segmentation models, write postprocessing operators using CuPy and NumPy in Python tensor interop and [MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax)](https://github.com/NVIDIA/MatX) in C++ tensor interop, and pass multiple tensors from postprocessing to Holoviz.\n\nPlease refer to the README under [./app_dev_process](./app_dev_process/README.md) to see the process of developing the applications.\n\nThe application graph looks like:\n![](./images/multiai_endoscopy_app_graph.png)\n\n## Model\nWe combine two models from the single model applications [SSD Tool Detection](https://github.com/nvidia-holoscan/holohub/tree/main/applications/ssd_detection_endoscopy_tools) and [MONAI Endoscopic Tool Segmentation](https://github.com/nvidia-holoscan/holohub/tree/main/applications/monai_endoscopic_tool_seg):\n\n - [SSD model from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/ssd_surgical_tool_detection_model) with additional NMS op: `epoch24_nms.onnx`\n - [MONAI tool segmentation model from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/monai_endoscopic_tool_segmentation_model): `model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx`\n## Data\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\n## Requirements\nEnsure you have installed the Holoscan SDK via one of the methods specified in [the SDK user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/sdk_installation.html#development-software-stack).\n\nThe directory specified by `--data` at app runtime is assumed to contain three subdirectories, corresponding to the three NGC resources specified in [Model](#model) and [Data](#data): `endoscopy`, `monai_tool_seg_model` and `ssd_model`.  These resources will be automatically downloaded to the holohub data directory when building the application.\n\n## Building the application\n\nThe repo level build command \n```sh\n./run build multiai_endoscopy\n```\nwill build one of the cpp apps `post-proc-cpu`. \n\n\n## Running the application\n### Python Apps\nTo run the Python application, you can make use of the run script\n```sh\n./run launch multiai_endoscopy python\n```\nAlternatively, to run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\nNext, run the application:\n```sh\ncd <HOLOHUB_SOURCE_DIR>/applications/multiai_endoscopy/python\npython3 multi_ai.py --data <DATA_DIR>\n```\n\n### C++ Apps\n\nThere are three versions of C++ apps, with the only difference being that they implement the inference post-processing operator `DetectionPostprocessorOp` in different ways:\n\n- `post-proc-cpu`: Multi-AI app running the inference post-processing operator on the CPU using `std` features only.\n- `post-proc-matx-cpu`: Multi-AI app running the inference post-processing operator on the CPU using the [MatX library]([GitHub - NVIDIA/MatX: An efficient C++17 GPU numerical computing library with Python-like syntax](https://github.com/NVIDIA/MatX)).\n- `post-proc-matx-gpu`: Multi-AI app running  the inference post-processing operator on the GPU using MatX (CUDA).\n\nTo run `post-proc-cpu`, since it already gets built with `./run build multiai_endoscopy`:\n```sh\n./run launch multiai_endoscopy cpp\n```\n\nFor the other two C++ applications, you'll need to build these without the run script as follows.\n\nTo run `post-proc-matx-cpu` or `post-proc-matx-gpu`, first navigate to the app directory.\n\n```shell\ncd cpp/post-proc-matx-cpu\n```\n\nNext we need to configure and build the app.\n\n#### Configuring\n\nFirst, create a build folder:\n\n```shell\nmkdir -p build\n```\n\nthen run CMake configure with:\n\n```shell\ncmake -S . -B build\n```\n\nUnless you make changes to `CMakeLists.txt`, this step only needs to be done **once**.\n\n#### Building\n\nThe app can be built with:\n\n```shell\ncmake --build build\n```\n\nor equally:\n\n```shell\ncd build\nmake\n```\n\n#### Running\n\nYou can run the app with:\n\n```shell\n./build/multi_ai --data <DATA_DIR>\n```\n",
        "application_name": "multiai_endoscopy",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Multi AI SSD Detection MONAI Endoscopic Tool Segmentation Application",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Multiai",
                "SSD",
                "bounding box",
                "Detection",
                "MONAI",
                "Segmentation"
            ],
            "ranking": 2,
            "dependencies": {
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "models": [
                    "https://api.ngc.nvidia.com/v2/resources/nvidia/clara-holoscan/ssd_surgical_tool_detection_model",
                    "https://api.ngc.nvidia.com/v2/resources/nvidia/clara-holoscan/monai_endoscopic_tool_segmentation_model"
                ]
            },
            "run": {
                "command": "<holohub_app_bin>/post-proc-cpu/multiai_endoscopy --data <holohub_data_dir>/",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Multi AI Application with SSD Detection and MONAI Endoscopic Tool Segmentation \nIn this application, we show how to build a Multi AI application with detection and segmentation models, write postprocessing operators using CuPy and NumPy in Python tensor interop and [MatX library (An efficient C++17 GPU numerical computing library with Python-like syntax)](https://github.com/NVIDIA/MatX) in C++ tensor interop, and pass multiple tensors from postprocessing to Holoviz.\n\nPlease refer to the README under [./app_dev_process](./app_dev_process/README.md) to see the process of developing the applications.\n\nThe application graph looks like:\n![](./images/multiai_endoscopy_app_graph.png)\n\n## Model\nWe combine two models from the single model applications [SSD Tool Detection](https://github.com/nvidia-holoscan/holohub/tree/main/applications/ssd_detection_endoscopy_tools) and [MONAI Endoscopic Tool Segmentation](https://github.com/nvidia-holoscan/holohub/tree/main/applications/monai_endoscopic_tool_seg):\n\n - [SSD model from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/ssd_surgical_tool_detection_model) with additional NMS op: `epoch24_nms.onnx`\n - [MONAI tool segmentation model from NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/monai_endoscopic_tool_segmentation_model): `model_endoscopic_tool_seg_sanitized_nhwc_in_nchw_out.onnx`\n## Data\n[\ud83d\udce6\ufe0f (NGC) Sample App Data for AI-based Endoscopy Tool Tracking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data)\n\n## Requirements\nEnsure you have installed the Holoscan SDK via one of the methods specified in [the SDK user guide](https://docs.nvidia.com/clara-holoscan/sdk-user-guide/sdk_installation.html#development-software-stack).\n\nThe directory specified by `--data` at app runtime is assumed to contain three subdirectories, corresponding to the three NGC resources specified in [Model](#model) and [Data](#data): `endoscopy`, `monai_tool_seg_model` and `ssd_model`.  These resources will be automatically downloaded to the holohub data directory when building the application.\n\n## Building the application\n\nThe repo level build command \n```sh\n./run build multiai_endoscopy\n```\nwill build one of the cpp apps `post-proc-cpu`. \n\n\n## Running the application\n### Python Apps\nTo run the Python application, you can make use of the run script\n```sh\n./run launch multiai_endoscopy python\n```\nAlternatively, to run this application, you'll need to configure your PYTHONPATH environment variable to locate the\nnecessary python libraries based on your Holoscan SDK installation type.\n\nYou should refer to the [glossary](../../README.md#Glossary) for the terms defining specific locations within HoloHub.\n\nIf your Holoscan SDK installation type is:\n\n* python wheels:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\n\n* otherwise:\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:<HOLOSCAN_INSTALL_DIR>/python/lib:<HOLOHUB_BUILD_DIR>/python/lib\n  ```\nNext, run the application:\n```sh\ncd <HOLOHUB_SOURCE_DIR>/applications/multiai_endoscopy/python\npython3 multi_ai.py --data <DATA_DIR>\n```\n\n### C++ Apps\n\nThere are three versions of C++ apps, with the only difference being that they implement the inference post-processing operator `DetectionPostprocessorOp` in different ways:\n\n- `post-proc-cpu`: Multi-AI app running the inference post-processing operator on the CPU using `std` features only.\n- `post-proc-matx-cpu`: Multi-AI app running the inference post-processing operator on the CPU using the [MatX library]([GitHub - NVIDIA/MatX: An efficient C++17 GPU numerical computing library with Python-like syntax](https://github.com/NVIDIA/MatX)).\n- `post-proc-matx-gpu`: Multi-AI app running  the inference post-processing operator on the GPU using MatX (CUDA).\n\nTo run `post-proc-cpu`, since it already gets built with `./run build multiai_endoscopy`:\n```sh\n./run launch multiai_endoscopy cpp\n```\n\nFor the other two C++ applications, you'll need to build these without the run script as follows.\n\nTo run `post-proc-matx-cpu` or `post-proc-matx-gpu`, first navigate to the app directory.\n\n```shell\ncd cpp/post-proc-matx-cpu\n```\n\nNext we need to configure and build the app.\n\n#### Configuring\n\nFirst, create a build folder:\n\n```shell\nmkdir -p build\n```\n\nthen run CMake configure with:\n\n```shell\ncmake -S . -B build\n```\n\nUnless you make changes to `CMakeLists.txt`, this step only needs to be done **once**.\n\n#### Building\n\nThe app can be built with:\n\n```shell\ncmake --build build\n```\n\nor equally:\n\n```shell\ncd build\nmake\n```\n\n#### Running\n\nYou can run the app with:\n\n```shell\n./build/multi_ai --data <DATA_DIR>\n```\n",
        "application_name": "multiai_endoscopy",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Volume Rendering",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Render",
                "ClaraViz"
            ],
            "ranking": 1,
            "dependencies": {},
            "run": {
                "command": "<holohub_app_bin>/volume_rendering --config <holohub_data_dir>/volume_rendering/config.json --density <holohub_data_dir>/volume_rendering/highResCT.mhd --mask <holohub_data_dir>/volume_rendering/smoothmasks.seg.mhd",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Volume rendering using ClaraViz\n\n![](screenshot.png)<br>\n\nThis application loads a medical CT scan and renders it in real time at interactive frame rates using ClaraViz (https://github.com/NVIDIA/clara-viz).\n\nThe application uses the `VolumeLoaderOp` operator to load the medical volume data, the `VolumeRendererOp` operator to render the volume and the `HolovizOp` operator to display the result and handle the camera movement.\n\n### Data\n\nYou can find CT scan datasets for use with this application from [embodi3d](https://www.embodi3d.com/).\n\n\n## Build Instructions\n\nTo build this application, use the ```run``` script:\n\n```bash\n  ./run build volume_rendering\n```\n\n## Run Instructions\n\nFrom the build directory, run the command:\n\n```bash\n  ./run launch volume_rendering\n```\n\nThe path of the volume configuration file, volume density file and volume mask file can be passed to the application.\n\nYou can use the following command to get more information on command line parameters for this application:\n\n```bash\n./run launch volume_rendering --extra_args -h\n```\n",
        "application_name": "volume_rendering",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "Hyperspectral image segmentation",
            "authors": [
                {
                    "name": "Lars Doorenbos",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "Python",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Segmentation",
                "Hyperspectral"
            ],
            "ranking": 2,
            "dependencies": {
                "blosc": "^1.11.1",
                "torch": "^2.1.0",
                "onnx": "^1.15.0",
                "onnxruntime": "^1.16.1",
                "Pillow": "^10.1.0",
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/hyperspectral_segmentation"
            },
            "run": {
                "command": "python3 <holohub_app_source>/hyperspectral_segmentation.py --output_folder <holohub_app_source>/ --data <holohub_data_dir>/hyperspectral/data --model <holohub_data_dir>/hyperspectral",
                "workdir": "holohub_bin"
            }
        },
        "readme": "# Hyperspectral Image Segmentation\n\n![](screenshot.png)<br>\n\nThis application segments endoscopic hyperspectral cubes into 20 organ classes. It visualizes the result together with the RGB image corresponding to the cube.\n\n## Data and Models\n\nThe data is a subset of the [HeiPorSPECTRAL](https://www.heiporspectral.org/) dataset. The application loops over the 84 cubes selected. The model is the `2022-02-03_22-58-44_generated_default_model_comparison` checkpoint from [this repository](https://github.com/IMSY-DKFZ/htc), converted to ONNX with the script in `utils/convert_to_onnx.py`.\n\n[\ud83d\udce6\ufe0f (NGC) App Data and Model for Hyperspectral Segmentation](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/hyperspectral_segmentation).  This resource is automatically downloaded when building the application.\n\n## Run Instructions\n\nThis application requires some python modules to be installed.  For simplicity, a Dockerfile is available.  To generate the container run:\n```\n./dev_container build --docker_file ./applications/hyperspectral_segmentation/Dockerfile\n```\nThe application can then be built by launching this container and using the provided run script.\n```\n./dev_container launch\n./run build hyperspectral_segmentation\n```\nOnce the application is built it can be launched with the run script.\n```\n./run launch hyperspectral_segmentation\n```\n\n## Viewing Results\n\nWith the default settings, the results of this application are saved to `result.png` file in the hyperspectral segmentation app directory. Each time a new image is processed, it overwrites `result.png`.  By opening this image while the application is running, you can see the results as the updates are made (may depend on your image viewer).\n",
        "application_name": "hyperspectral_segmentation",
        "source_folder": "applications"
    },
    {
        "metadata": {
            "name": "orsi_format_converter",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "converter"
            ],
            "ranking": 3,
            "dependencies": {}
        },
        "readme": "",
        "application_name": "orsi",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "orsi_segmentation_postprocessor",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "postprocessor"
            ],
            "ranking": 3,
            "dependencies": {}
        },
        "readme": "",
        "application_name": "orsi",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "orsi_visualizer",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "visualizer"
            ],
            "ranking": 3,
            "dependencies": {}
        },
        "readme": "",
        "application_name": "orsi",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "orsi_segmentation_preprocessor",
            "authors": [
                {
                    "name": "Jasper Hofman",
                    "affiliation": "Orsi Academy"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "preprocessor"
            ],
            "ranking": 3,
            "dependencies": {}
        },
        "readme": "",
        "application_name": "orsi",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_write_bitstream",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Video Bit Stream Writer\n\nThe `video_write_bitstream` writes bit stream to the disk at specified output\npath.\n\n#### `holoscan::ops::VideoWriteBitstreamOp`\n\nOperator class to write video bit stream to the disk.\n\nThis implementation is based on `nvidia::gxf::VideoWriteBitstream`.\n\n##### Parameters\n\n- **`output_video_path`**: The file path of the output video\n  - type: `std::string`\n- **`frame_width`**: The width of the output video\n  - type: `int`\n- **`frame_height`**: The height of the output video\n  - type: `int`\n- **`inbuf_storage_type`**: Input Buffer storage type, 0:kHost, 1:kDevice\n  - type: `int`\n- **`data_receiver`**: Receiver to get the data\n  - type: `holoscan::IOSpec*`\n",
        "application_name": "video_write_bitstream",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "emergent_source",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Emergent"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "emergent_source",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "emergent_source",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "yuan_qcap",
            "authors": [
                {
                    "name": "David Su",
                    "affiliation": "Yuan"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Yuan"
            ],
            "ranking": 2,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "yuan_qcap",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "yuan_qcap",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "videomaster",
            "authors": [
                {
                    "name": "Laurent Radoux",
                    "affiliation": "Deltacast"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Camera",
                "Deltacast"
            ],
            "ranking": 2,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videomaster",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "# VideoMaster GXF Operator\n\nThis library contains two operators:\n- videomaster_source: get signal from capture card\n- videomaster_transmitter: generate signal\n\nThese operators wrap the GXF extension to provide support for VideoMaster SDK.\n\n## Requirements\n\nThis operator requires the VideoMaster SDK from Deltacast.\n\n## Building the operator\n\nAs part of Holohub, running CMake on Holohub and point to Holoscan SDK install tree.\n\nThe path to the VideoMaster SDK is also mandatory and can be given through the VideoMaster_SDK_DIR parameter.\n",
        "application_name": "deltacast_videomaster",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_read_bitstream",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Video Bit Stream Reader\n\nThe `video_read_bitstream` reads h264 bit stream from specified input file.\n\n#### `holoscan::ops::VideoReadBitstreamOp`\n\nOperator class to read H264 video bit stream.\n\nThis implementation is based on `nvidia::gxf::VideoReadBitStream`.\n\n##### Parameters\n\n- **`output_transmitter`**: Transmitter to send the compressed data\n  - type: `holoscan::IOSpec*`\n- **`input_file_path`**: Path to image file\n  - type: `std::string`\n- **`pool`**: Memory pool for allocating output data\n  - type: `std::shared_ptr<Allocator>`\n- **`outbuf_storage_type`**: Output Buffer storage type, 0:kHost, 1:kDevice\n  - type: `int32_t`\n",
        "application_name": "video_read_bitstream",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "tensor_to_video_buffer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Tensor",
                "Video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### GXF Tensor to VideoBuffer Converter\n\nThe `tensor_to_video_buffer` converts GXF Tensor to VideoBuffer.\n\n#### `holoscan::ops::TensorToVideoBufferOp`\n\nOperator class to convert GXF Tensor to VideoBuffer. This operator is required\nfor data transfer  between Holoscan operators that output GXF Tensor and\nthe other Holoscan Wrapper Operators that understand only VideoBuffer.\nIt receives GXF Tensor as input and outputs GXF VideoBuffer created from it.\n\n##### Parameters\n\n- **`data_in`**: Data in GXF Tensor format\n - type: `holoscan::IOSpec*`\n- **`data_out`**: Data in GXF VideoBuffer format\n - type: `holoscan::IOSpec*`\n- **`in_tensor_name`**: Name of the input tensor\n  - type: `std::string`\n- **`video_format`**: The video format, supported values: \"yuv420\", \"rgb\"\n  - type: `std::string`\n",
        "application_name": "tensor_to_video_buffer",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_encoder_request",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "encoder",
                "request"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videoencoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videoencoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Encoder Request\n\nThe `video_encoder_request` handles the input for encoding YUV frames to H264 bit stream.\n\n#### `holoscan::ops::VideoEncoderOp`\n\nOperator class to handle the input for encoding YUV frames to H264 bit stream.\n\nThis implementation is based on `nvidia::gxf::VideoEncoderRequest`.\n\n##### Parameters\n\n- **`input_frame`**: Receiver to get the input frame.\n  - type: `holoscan::IOSpec*`\n- **`videoencoder_context`**: Encoder context Handle.\n  - type: `std::shared_ptr<holoscan::ops::VideoEncoderContext>`\n- **`inbuf_storage_type`**: Input Buffer storage type, 0: kHost, 1: kDevice. Default: 1\n  - type: `uint32_t`\n- **`codec`**: Video codec to use,  0: H264, only H264 supported. Default: 0.\n  - type: `int32_t`\n- **`input_height`**: Input frame height.\n  - type: `uint32_t`\n- **`input_width`**: Input image width.\n  - type: `uint32_t`\n- **`input_format`**: Input color format, nv12,nv24,yuv420planar. Default: nv12.\n  - type: `nvidia::gxf::EncoderInputFormat`\n- **`profile`**: Encode profile, 0: Baseline Profile, 1: Main, 2: High. Default: 2.\n  - type: `int32_t`\n- **`bitrate`**: Bitrate of the encoded stream, in bits per second. Default: 20000000.\n  - type: `int32_t`\n- **`framerate`**: Frame Rate, frames per second. Default: 30.\n  - type: `int32_t`\n- **`qp`**: Encoder constant QP value. Default: 20.\n  - type: `uint32_t`\n- **`level`**: Video H264 level. Maximum data rate and resolution, select from 0 to 14. Default: 14.\n  - type: `int32_t`\n- **`iframe_interval`**: I Frame Interval, interval between two I frames. Default: 30.\n  - type: `int32_t`\n- **`rate_control_mode`**: Rate control mode, 0: CQP[RC off], 1: CBR, 2: VBR. Default: 1.\n  - type: `int32_t`\n- **`config`**: Preset of parameters, select from pframe_cqp, iframe_cqp, custom. Default: custom.\n  - type: `nvidia::gxf::EncoderConfig`\n\n",
        "application_name": "video_encoder",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_encoder_response",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "encoder",
                "response"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videoencoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videoencoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Encoder Response\n\nThe `video_encoder_response` handles the output of the encoded YUV frames.\n\n#### `holoscan::ops::VideoEncoderResponseOp`\n\nOperator class to handle the output of the encoded YUV frames.\n\nThis implementation is based on `nvidia::gxf::VideoEncoderResponse`.\n\n##### Parameters\n\n- **`output_transmitter`**: Transmitter to send the compressed data.\n  - type: `holoscan::IOSpec*`\n- **`pool`**: Memory pool for allocating output data.\n  - type: `std::shared_ptr<Allocator>`\n- **`videoencoder_context`**: Encoder context Handle.\n  - type: `std::shared_ptr<holoscan::ops::VideoEncoderContext>`\n- **`outbuf_storage_type`**: Output Buffer Storage(memory) type used by this allocator. Can be 0: kHost, 1: kDevice. Default: 1.\n  - type: `uint32_t`\n\n",
        "application_name": "video_encoder",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_encoder_context",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "encoder",
                "context"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videoencoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videoencoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Encoder Context\n\nThe `video_encoder_context` is used to hold common variables and underlying context. The encoder context handle is passed to both `holoscan::ops::VideoEncoderRequestOp` and `holoscan::ops::VideoEncoderResponseOp`.\n\n#### `holoscan::ops::VideoEncoderContext`\n\nA class used to hold common variables and underlying context required by `holoscan::ops::VideoEncoderRequestOp` and `holoscan::ops::VideoEncoderResponseOp`.\n\nThis implementation is based on the GXF Component `nvidia::gxf::VideoEncoderContext`.\n\n##### Parameters\n\n- **`async_scheduling_term`**: Asynchronous scheduling condition required to get/set event state.\n  - type: `std::shared_ptr<holoscan::AsynchronousCondition>`\n\n",
        "application_name": "video_encoder",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "npp_filter",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Filter",
                "NPP",
                "Gauss",
                "Sobel"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### NPP Filter\n\nThe `npp_filter` operator uses [NPP](https://developer.nvidia.com/npp) to apply a filters to a Tensor or VideBuffer.\n\n#### `holoscan::ops::NppFilter`\n\nOperator class to apply a filter of the [NPP library]() to a Tensor or VideBuffer.\n\n##### Parameters\n\n- **`filter`**: Name of the filter to apply (supported Gauss, SobelHoriz, SobelVert)\n  - type: `std::string`\n- **`mask_size`**: Filter mask size (supported values 3, 5, 7, 9, 11, 13)\n  - type: `uint32_t`\n- **`allocator`**: Allocator used to allocate the output data\n  - type: `std::shared_ptr<Allocator>`\n\n##### Inputs\n\n- **`input`**: Input frame data\n  - type: `nvidia::gxf::Tensor` or `nvidia::gxf::VideoBuffer`\n\n##### Outputs\n\n- **`input`**: Output frame data\n  - type: `nvidia::gxf::Tensor` or `nvidia::gxf::VideoBuffer`\n",
        "application_name": "npp_filter",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrTransformOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### User interface Control Operator\n\nThe `XrTransformControlOp` maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.\n\n#### `holoscan::openxr::XrTransformControlOp`\n\n##### Inputs\n \nController state\n- **`trigger_click`**: trigger button state\n  - type: `bool`\n- **`shoulder_click`**: shoulder button state\n  - type: `bool`\n- **`trackpad_touch`**: trackpad state\n  - type: `bool`\n- **`trackpad`**: trackpad values [x,y]\n  - type: `std::array<float, 2>`\n- **`aim_pose`**: world space pose of the controller tip\n  - type: `nvidia::gxf::Pose3D`\n\nDevice state\n- **`head_pose`**: world space head pose of the device\n  - type: `nvidia::gxf::Pose3D`\n\nVolume state\n- **`extent`**: size of bounding box containing volume\n  - type: `std::array<float, 3>`\n\n##### Outputs\n\nUser interface widget state structures\n- **`ux_box`**: bounding box state structure\n  - type: `UxBoundingBox`\n- **`ux_cursor`**: cursor state structure\n  - type: `UxCursor`\n\nVolume rendering parameters\n- **`volume_pose`**: world pose of dataset \n  - type: `nvidia::gxf::Pose3D`\n- **`crop_box`**: axis aligned cropping planes in local coordinates\n  - type: `std::array<nvidia::gxf::Vector2f, 3>`",
        "application_name": "XrTransformOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrTransformControlOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### User interface Control Operator\n\nThe `XrTransformControlOp` maintains the state of the mixed reality user interface for the volumetric rendering application. It consumes controller events and produces user interface state structures as well as parameters for the volume rendering operator. The operator is application specific and will grow over time to include additional user interface widgets.\n\n#### `holoscan::openxr::XrTransformControlOp`\n\n##### Inputs\n \nController state\n- **`trigger_click`**: trigger button state\n  - type: `bool`\n- **`shoulder_click`**: shoulder button state\n  - type: `bool`\n- **`trackpad_touch`**: trackpad state\n  - type: `bool`\n- **`trackpad`**: trackpad values [x,y]\n  - type: `std::array<float, 2>`\n- **`aim_pose`**: world space pose of the controller tip\n  - type: `nvidia::gxf::Pose3D`\n\nDevice state\n- **`head_pose`**: world space head pose of the device\n  - type: `nvidia::gxf::Pose3D`\n\nVolume state\n- **`extent`**: size of bounding box containing volume\n  - type: `std::array<float, 3>`\n\n##### Outputs\n\nUser interface widget state structures\n- **`ux_box`**: bounding box state structure\n  - type: `UxBoundingBox`\n- **`ux_cursor`**: cursor state structure\n  - type: `UxCursor`\n\nVolume rendering parameters\n- **`volume_pose`**: world pose of dataset \n  - type: `nvidia::gxf::Pose3D`\n- **`crop_box`**: axis aligned cropping planes in local coordinates\n  - type: `std::array<nvidia::gxf::Vector2f, 3>`",
        "application_name": "XrTransformOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrTransformRenderOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### User interface Render Operator\n\nThe `XrTransformRenderOp` renders the mixed reality user interface of the volumetric rendering application. It consumes interface widget state structures as well as render buffers into which to overlay the interface widgets. The operator is application specific and will grow over time to include additional user interface widgets.\n\n#### `holoscan::openxr::XrTransformRenderOp`\n\n##### Parameters \n\n- **`display_width`**: pixel height of display\n  - type: `int`\n- **`display_height`**: pixel width of display\n  - type: `int`\n \n##### Inputs\n\nCamera state for stereo view\n- **`left_camera_pose`**: world space pose of the left eye\n  - type: `nvidia::gxf::Pose3D`\n- **`right_camera_pose`**: world space pose of the right eye\n  - type: `nvidia::gxf::Pose3D`\n- **`left_camera_model`**: camera model for the left eye\n  - type: `nvidia::gxf::CameraModel`\n- **`right_camera_model`**: camera model for the right eye\n  - type: `nvidia::gxf::CameraModel`\n- **`depth_range`**: depth range\n\nUser interface widget state structures\n- **`ux_box`**: bounding box state structure\n  - type: `UxBoundingBox`\n- **`ux_cursor`**: cursor state structure\n  - type: `UxCursor`\n\nRender buffers to be populated\n- **`Collor buffer_in`**: color buffer\n  - type: `holoscan::gxf::VideoBuffer`\n- **`Depth buffer_in`**: depth buffer\n  - type: `holoscan::gxf::VideoBuffer`\n\n##### Outputs\n\nRender buffers including interface widgets\n- **`color_buffer_out`**: color buffer\n  - type: `holoscan::gxf::VideoBuffer`\n- **`depth_buffer_out`**: depth buffer\n  - type: `holoscan::gxf::VideoBuffer`\n",
        "application_name": "XrTransformOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrFrameOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### XrFrame Operator\n\nThe `XrFrameOp` directory contains the `XrBeginFrameOp` and the `XrEndFrameOp`. `XrBeginFrameOp` operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to `XrEndFrameOp` in order to deliver the frame back to the OpenXR runtime. Note that a single connection xr_frame from `XrBeginFrameOp` to `XrEndFrameOp` is required to synchronize the OpenXR calls issued by the two operators.",
        "application_name": "XrFrameOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "Convert Depth to Screen Space",
            "authors": [
                {
                    "name": "Magic Leap team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6",
                "tested_versions": [
                    "0.6"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Convert",
                "Depth",
                "Screen"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### Convert Depth To Screen Space Operator\n\nThe `ConvertDepthToScreenSpaceOp` operator remaps the depth buffer from Clara Viz to an OpenXR specific range. The depth buffer is converted in place.\n\n#### `holoscan::openxr::ConvertDepthToScreenSpaceOp`\n\nConverts a depth buffer from linear world units to screen space ([0,1])\n\n##### Inputs\n\n- **`depth_buffer_in`**: input depth buffer to be remapped\n  - type: `holoscan::gxf::VideoBuffer`\n- **`depth_range`**: Allocator used to allocate the volume data\n  - type: `nvidia::gxf::Vector2f`\n\n##### Outputs\n- **`depth_buffer_out`**: output depth buffer \n  - type: `holoscan::gxf::Entity`\n",
        "application_name": "XrFrameOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrBeginFrameOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### XRBeginFrame Operator\n\nThe `XrBeginFrameOp` operator initiates the rendering of a single OpenXR frame. It converts render buffers and events generated by the OpenXR runtime into Holoscan specific events to be consumed by downstream operators. Application specific operators are then expected to render left and right camera views into the given color and depth buffers. Once complete, the buffers must be passed to `XrEndFrameOp` in order to deliver the frame back to the OpenXR runtime. Note that a single arc xr_frame from `XrBeginFrameOp` to `XrEndFrameOp` is required to synchronize the OpenXR calls issued by the two operators.\n\n#### `holoscan::openxr::XrBeginFrameOp`\n\n##### Outputs\n\nOutput for camera state \n- **`left_camera_pose`**: camera pose for the left eye\n  - type: `nvidia::gxf::Pose3D`\n- **`right_camera_pose`**: camera pose for the right eye\n  - type: `nvidia::gxf::Pose3D`\n- **`left_camera_model`**: camera model for the left eye\n  - type: `nvidia::gxf::CameraModel`\n- **`right_camera_model`**: camera model for the right eye\n  - type: `nvidia::gxf::CameraModel`\n- **`depth_range`**: depth range\n  - type: `nvidia::gxf::Vector2f`\n\nOutput for input state \n- **`trigger_click`**: trigger click , values true/false\n  - type: `bool`\n- **`shoulder_click`**: shoulder click , values true/false\n  - type: `bool`\n- **`trackpad_touch`**: trackpad touch , values true/false\n  - type: `bool`\n- **`trackpad`**: trackpad values [x.y]\n  - type: `std::array<float, 2>`\n- **`aim_pose`**: aim pose for the controller specific for the right hand\n  - type: `nvidia::gxf::Pose3D`\n- **`head_pose`**: head pose \n  - type: `nvidia::gxf::Pose3D`\n- **`color_buffer`**: color buffer\n  - type: `holoscan::gxf::Entity`\n- **`depth_buffer`**: depth buffer\n  - type: `holoscan::gxf::Entity`\n\n\n##### Parameters \n\n- **`XrSession`**: A class that encapsulates a single OpenXR session\n  - type: `holoscan::openxr::XrSession`\n \n\nNote:\n\n- **`XrCudaInteropSwapchain`**: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR\n  \n \n",
        "application_name": "XrFrameOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "XrEndFrameOp",
            "authors": [
                {
                    "name": "Magic Leap Team",
                    "affiliation": "Magic Leap"
                }
            ],
            "language": "C++",
            "version": "0.0",
            "changelog": {
                "0.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "XR",
                "XRFrame"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "### XrEndFrame Operator\n\nThe `XrEndFrameOp` operator completes the rendering of a single OpenXR frame by passing populated color and depth buffer for the left and right eye to the OpenXR device. Note that a single connection `xr_frame` from `XrBeginFrameOp` to `XrEndFrameOp` is required to synchronize the OpenXR calls issued by the two operators.\n\n#### `holoscan::openxr::XrEndFrameOp`\n\n##### Parameters \n\n- **`XrSession`**: A class that encapsulates a single OpenXR session\n  - type: `holoscan::openxr::XrSession`\n\n##### Inputs\n \nRender buffers populated by application\n- **`color_buffer`**: color buffer\n  - type: `holoscan::gxf::VideoBuffer`\n- **`depth_buffer`**: depth buffer\n  - type: `holoscan::gxf::VideoBuffer`\n\nOpenXR synchronization\n- **`XrFrame`**: connection to synchronize `XrBeginFrameOp` and `XrEndFrameOp`\n  - type: `XrFrame`\n\n\n\nNote:\n\n- **`XrCudaInteropSwapchain`**: A class that encapsulates the Vulkan buffers of the OpenXR runtime and compatible CUDA buffer to provide interoperability between ClaraViz and OpenXR",
        "application_name": "XrFrameOp",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "advanced_network",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.2",
            "changelog": {
                "1.3": "Plugin support",
                "1.2": "TX GPUDirect",
                "1.1": "GPUDirect updates",
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "DPDK",
                "UDP",
                "Ethernet",
                "IP",
                "GPUDirect",
                "RDMA"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "advanced_network",
                        "version": "1.3"
                    }
                ]
            }
        },
        "readme": "### Advanced Network Operator\n\nThe Advanced Network Operator provides a way for users to achieve the highest throughput and lowest latency\nfor transmitting and receiving Ethernet frames out of and into their operators. Direct access to the NIC hardware\nis available in userspace using this operator, thus bypassing the kernel's networking stack entirely. With a\nproperly tuned system the advanced network operator can achieve hundreds of Gbps with latencies in the low\nmicroseconds. Performance is highly dependent on system tuning, packet sizes, batch sizes, and other factors.\nThe data may optionally be sent to the GPU using GPUDirect to prevent extra copies to and from the CPU.\n\nSince the kernel's networking stack is bypassed, the user is responsible for defining the protocols used\nover the network. In most cases Ethernet, IP, and UDP are ideal for this type of processing because of their\nsimplicity, but any type of protocol can be implemented or used. The advanced network operator\ngives the option to use several primitives to remove the need for filling out these headers for basic packet types,\nbut raw headers can also be constructed.\n\n#### Requirements\n\n- Linux\n- A DPDK-compatible network card. For GPUDirect only NVIDIA NICs are supported\n- System tuning as described below\n- DPDK 22.11\n- MOFED 5.8-1.0.1.1 or later\n\n#### Features\n\n- **High Throughput**: Hundreds of gigabits per second is possible with the proper hardware\n- **Low Latency**: With direct access to the NIC's ring buffers, most latency incurred is only PCIe latency\n- **GPUDirect**: Optionally send data directly from the NIC to GPU, or directly from the GPU to NIC. GPUDirect has two modes:\n  - Header-data split: Split the header portion of the packet to the CPU and the rest (payload) to the GPU. The split point is\n    configurable by the user. This option should be the preferred method in most cases since it's easy to use and still\n    gives near peak performance.\n  - Batched GPU: Receive batches of whole packets directly into the GPU memory. This option requires the GPU kernel to inspect\n    and determine how to handle packets. While performance may increase slightly over header-data split, this method\n    requires more effort and should only be used for advanced users.\n- **Flow Configuration**: Configure the NIC's hardware flow engine for configurable patterns. Currently only UDP source\n    and destination are supported.\n\n#### Limitations\n\nThe limitations below will be removed in a future release.\n\n- Only UDP fill mode is supported\n\n#### Implementation\n\nInternally the advanced network operator is implemented using DPDK. DPDK is an open-source userspace packet processing\nlibrary supported across platforms and vendors. While the DPDK interface is abstracted away from users of the\nadvanced network operator, the method in which DPDK integrates with Holoscan is important for understanding\nhow to achieve the highest performance and for debugging.\n\nWhen the advanced network operator is compiled/linked against a Holoscan application, an instance of the DPDK manager\nis created, waiting to accept configuration. When either an RX or TX advanced network operator is defined in a\nHoloscan application, their configuration is sent to the DPDK manager. Once all advanced network operators have initialized,\nthe DPDK manager is told to initialize DPDK. At this point the NIC is configured using all parameters given by the operators.\nThis step allocates all packet buffers, initializes the queues on the NIC, and starts the appropriate number of internal\nthreads. The job of the internal threads is to take packets off or put packets onto the NIC as fast as possible. They\nact as a proxy between the advanced network operators and DPDK by handling packets faster than the operators may be\nable to.\n\nTo achieve zero copy throughout the whole pipeline only pointers are passed between each entity above. When the user\nreceives the packets from the network operator it's using the same buffers that the NIC wrote to either CPU or GPU\nmemory. This architecture also implies that the user must explicitly decide when to free any buffers it's owning.\nFailure to free buffers will result in errors in the advanced network operators not being able to allocate buffers.\n\n#### System Tuning\n\nFrom a high level, tuning the system for a low latency workload prevents latency spikes large enough to cause anomalies\nin the application. This section details how to perform the basic tuning steps needed on both a Clara AGX and Orin IGX systems.\n\n##### Create Hugepages\n\nHugepages give the kernel access to a larger page size than the default (usually 4K) which reduces the number of memory\ntranslations that have to be actively maintained in MMUs. 1GB hugepages are ideal, but 2MB may be used as well if 1GB is not\navailable. To configure 1GB hugepages:\n\n```\nsudo mkdir /mnt/huge\nsudo mount -t hugetlbfs nodev /mnt/huge\nsudo sh -c \"echo nodev /mnt/huge hugetlbfs pagesize=1GB 0 0 >> /etc/fstab\"\n```\n\n##### Linux Boot Command Line\n\nThe Linux boot command line allows configuration to be injected into Linux before booting. Some configuration options are\nonly available at the boot command since they must be provided before the kernel has started. On the Clara AGX and Orin IGX\nediting the boot command can be done with the following configuration:\n\n```\nsudo vim /boot/extlinux/extlinux.conf\n# Find the line starting with APPEND and add the following\n\n# For Orin IGX:\nisolcpus=6-11 nohz_full=6-11 irqaffinity=0-5 rcu_nocbs=6-11 rcu_nocb_poll tsc=reliable audit=0 nosoftlockup default_hugepagesz=1G hugepagesz=1G hugepages=2\n\n# For Clara AGX:\nisolcpus=4-7 nohz_full=4=7 irqaffinity=0-3 rcu_nocbs=4-7 rcu_nocb_poll tsc=reliable audit=0 nosoftlockup default_hugepagesz=1G hugepagesz=1G hugepages=2\n```\n\nThe settings above isolate CPU cores 6-11 on the Orin and 4-7 on the Clara, and turn 1GB hugepages on.\n\nFor non-IGX or AGX systems please look at the documentation for your system to change the boot command.\n\n##### Setting the CPU governor\n\nThe CPU governor reduces power consumption by decreasing the clock frequency of the CPU when cores are idle. While this is useful\nin most environments, increasing the clocks from an idle period can cause long latency stalls. To disable frequency scaling:\n\n```\nsudo apt install cpufrequtils\nsudo sed -i 's/^GOVERNOR=.*/GOVERNOR=\"performance\"/' /etc/init.d/cpufrequtils\n```\n\nReboot the system after these changes.\n\n##### Permissions\n\nDPDK typically requires running as a root user. If you wish to run as a non-root user, you may follow the directions here:\nhttp://doc.dpdk.org/guides/linux_gsg/enable_func.html\n\nIf running in a container, you will need to run in privileged container, and mount your hugepages mount point from above into the container. This\ncan be done as part of the `docker run` command by adding the following flags:\n\n```\n-v /mnt/huge:/mnt/huge \\\n--privileged \\\n```    \n\n#### Configuration Parameters\n\nThe advanced network operator contains a separate operator for both transmit and receive. This allows applications to choose\nwhether they need to handle bidirectional traffic or only unidirectional. Transmit and receive are configured separately in\na YAML file, and a common configuration contains items used by both directions. Each configuration section is described below.\n\n##### Common Configuration\n\nThe common configuration container parameters are used by both TX and RX:\n\n- **`version`**: Version of the config. Only 1 is valid currently.\n  - type: `integer`\n- **`master_core`**: Master core used to fork and join network threads. This core is not used for packet processing and can be\nbound to a non-isolated core\n  - type: `integer`\n\n##### Receive Configuration\n\n- **`if_name`**: Name of the interface or PCIe BDF to use\n  - type: `string`\n- **`queues`**: Array of queues\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`\n- **`gpu_direct`**: GPUDirect is enabled on the queue\n  - type: `boolean`\n- **`batch_size`**: Number of packets in a batch that is passed between the advanced network operator and the user's operator. A\nlarger number increases throughput and latency by requiring fewer messages between operators, but takes longer to populate a single\nbuffer. A smaller number reduces latency and bandwidth by passing more messages.\n- **`num_concurrent_batches`**: Number of batches that can be outstanding (not freed) at any given time. This value directly affects\nthe amount of memory needed for receiving packets. A value too small and packets will be dropped, while a value too large will\nunnecessarily use excess CPU and/or GPU memory.\n  - type: `integer`\n- **`max_packet_size`**: Largest packet size expected\n  - type: `integer`\n- **`split_boundary`**: Split point in bytes where any byte before this value is sent to CPU, and anything after to GPU\n  - type: `integer`\n- **`gpu_device`**: GPU device number if using GPUDirect\n  - type: `integer`\n- **`cpu_cores`**: List of CPU cores from the isolated set used by the operator for receiving\n  - type: `string`\n- **`flows`**: Array of flows\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`\n- **`action`**: Action section of flow\n  - type: `sequence`\n- **`type`**: Type of action. Only \"queue\" is supported currently.\n  - type: `string`\n- **`id`**: ID of queue to steer to\n  - type: `integer`\n- **`match`**: Match section of flow\n  - type: `sequence`\n- **`udp_src`**: UDP source port\n  - type: `integer`\n- **`udp_dst`**: UDP destination port\n  - type: `integer`\n\n##### Transmit Configuration\n\n- **`if_name`**: Name of the interface or PCIe BDF to use\n  - type: `string`\n- **`accurate_send`**: Boolean flag to turn on accurate TX scheduling\n  - type: `boolean`\n- **`queues`**: Array of queues\n  - type: `array`\n- **`name`**: Name of queue\n  - type: `string`\n- **`id`**: ID of queue to steer to\n  - type: `integer`\n- **`gpu_direct`**: GPUDirect is enabled on the queue\n  - type: `boolean`\n- **`batch_size`**: Number of packets in a batch that is passed between the advanced network operator and the user's operator. A\nlarger number increases throughput and latency by requiring fewer messages between operators, but takes longer to populate a single\nbuffer. A smaller number reduces latency and bandwidth by passing more messages.\n  - type: `integer`\n- **`max_payload_size`**: Largest payload size expected\n  - type: `integer`\n- **`layer_fill`**: Layer(s) that the advanced network operator should populate in the packet. Anything higher than the layer\nspecified must be populated by the user. For example, if `ethernet` is specified, the user is responsible for populating values of\nany item above that layer (IP, UDP, etc...). Valid values are `raw`, `ethernet`, `ip`, and `udp`\n  - type: `string`\n- **`eth_dst_addr`**: Destination ethernet MAC address. Only used for `ethernet` layer_fill mode or above\n  - type: `string`\n- **`ip_src_addr`**: Source IP address to send packets from. Only used for `ip` layer_fill and above\n  - type: `string`\n- **`ip_dst_addr`**: Destination IP address to send packets to. Only used for `ip` layer_fill and above\n  - type: `string`\n- **`udp_dst_port`**: UDP destination port. Only used for `udp` layer_fill and above\n  - type: `integer`\n- **`udp_src_port`**: UDP source port. Only used for `udp` layer_fill and above\n  - type: `integer`\n- **`cpu_cores`**: List of CPU cores for transmitting\n  - type: `string`\n\n  #### API Structures\n\n  Both the transmit and receive operators use a common structure named `AdvNetBurstParams` to pass data to/from other operators.\n  `AdvNetBurstParams` provides pointers to all packets on the CPU and GPU, and contains metadata needed by the operator to track\n  allocations. Since the advanced network operator utilizes a generic interface that does not expose the underlying low-level network\n  card library, interacting with the `AdvNetBurstParams` is mostly done with the helper functions described below. A user should\n  never modify any members of `AdvNetBurstParams` directly as this may break in future versions. The `AdvNetBurstParams` is described\n  below:\n\n  ```\n  struct AdvNetBurstParams {\n    union {\n        AdvNetBurstParamsHdr hdr;\n        uint8_t buf[HS_NETWORK_HEADER_SIZE_BYTES];\n    };\n\n    void **cpu_pkts;\n    void **gpu_pkts;\n};\n```\n\nStarting from the top, the `hdr` field contains metadata about the batch of packets. `buf` is a placeholder for future expansion\nof fields. `cpu_pkts` contains pointers to CPU packets, while `gpu_pkts` contains pointers to the GPU packets. As mentioned above,\nthe `cpu_pkts` and `gpu_pkts` are opaque pointers and should not be access directly. See the next section for information on interacting\nwith these fields.\n\n#### Example API Usage\n\nFor an entire list of API functions, please see the `adv_network_common.h` header file.\n\n##### Receive\n\nThe section below describes a workflowusing GPUDirect to receive packets using header-data split. The job of the user's operator(s)\nis to process and free the buffers as quickly as possible. This might be copying to interim buffers or freeing before the entire\npipeline is done processing. This allows the networking piece to use relatively few buffers while still achieving very high rates.\n\nThe first step in receiving from the advanced network operator is to tie your operator's input port to the output port of the RX\nnetwork operator's `burst_out` port.\n\n```\nauto adv_net_rx    = make_operator<ops::AdvNetworkOpRx>(\"adv_network_rx\", from_config(\"adv_network_common\"), from_config(\"adv_network_rx\"), make_condition<BooleanCondition>(\"is_alive\", true));\nauto my_receiver   = make_operator<ops::MyReceiver>(\"my_receiver\", from_config(\"my_receiver\"));\nadd_flow(adv_net_rx, my_receiver, {{\"burst_out\", \"burst_in\"}});\n```\n\nOnce the ports are connected, inside the `compute()` function of your operator you will receive a `AdvNetBurstParams` structure\nwhen a batch is complete:\n\n```\nauto burst = op_input.receive<std::shared_ptr<AdvNetBurstParams>>(\"burst_in\").value();\n```\n\nThe packets arrive in scattered packet buffers. Depending on the application, you may need to iterate through the packets to\naggregate them into a single buffer. Alternatively the operator handling the packet data can operate on a list of packet\npointers rather than a contiguous buffer. Below is an example of aggregating separate GPU packet buffers into a single GPU\nbuffer:\n\n```\n  for (int p = 0; p < adv_net_get_num_pkts(burst); p++) {\n    h_dev_ptrs_[aggr_pkts_recv_ + p]   = adv_net_get_cpu_pkt_ptr(burst, p);\n    ttl_bytes_in_cur_batch_           += adv_net_get_gpu_packet_len(burst, p) + sizeof(UDPPkt);\n  }\n\n  simple_packet_reorder(buffer, h_dev_ptrs, packet_len, burst->hdr.num_pkts);\n```\n\nFor this example we are tossing the header portion (CPU), so we don't need to examine the packets. Since we launched a reorder\nkernel to aggregate the packets in GPU memory, we are also done with the GPU pointers. All buffers may be freed to the\nadvanced network operator at this point:\n\n```\nadv_net_free_all_burst_pkts_and_burst(burst_bufs_[b]);\n```\n\n##### Transmit\n\nTransmitting packets works similar to the receive side, except the user is tasked with filling out the packets as much as it\nneeds to. As mentioned above, helper functions are available to fill in most boilerplate header information if that doesn't\nchange often.\n\nSimilar to the receive, the transmit operator needs to connect to `burst_in` on the advanced network operator transmitter:\n\n```\nauto my_transmitter  = make_operator<ops::MyTransmitter>(\"my_transmitter\", from_config(\"my_transmitter\"), make_condition<BooleanCondition>(\"is_alive\", true));  \nauto adv_net_tx       = make_operator<ops::AdvNetworkOpTx>(\"adv_network_tx\", from_config(\"adv_network_common\"), from_config(\"adv_network_tx\"));\nadd_flow(my_transmitter, adv_net_tx, {{\"burst_out\", \"burst_in\"}});\n```\n\nBefore sending packets, the user's transmit operator must request a buffer from the advanced network operator pool:\n\n```\nauto msg = std::make_shared<AdvNetBurstParams>();\nmsg->hdr.num_pkts = num_pkts;\nif ((ret = adv_net_get_tx_pkt_burst(msg.get())) != AdvNetStatus::SUCCESS) {\n  HOLOSCAN_LOG_ERROR(\"Error returned from adv_net_get_tx_pkt_burst: {}\", static_cast<int>(ret));\n  return;\n}\n```\n\nThe code above creates a shared `AdvNetBurstParams` that will be passed to the advanced network operator, and uses\n`adv_net_get_tx_pkt_burst` to populate the burst buffers with valid packet buffers. On success, the buffers inside the\nburst structure will be allocate and are ready to be filled in. Each packet must be filled in by the user. In this\nexample we loop through each packet and populate a buffer:\n\n```\nfor (int num_pkt = 0; num_pkt < msg->hdr.num_pkts; num_pkt++) {\n  void *payload_src = data_buf + num_pkt * nom_pkt_size;\n  if (adv_net_set_udp_payload(msg->cpu_pkts[num_pkt], payload_src, nom_pkt_size) != AdvNetStatus::SUCCESS) {\n    HOLOSCAN_LOG_ERROR(\"Failed to create packet {}\", num_pkt);\n  }\n}\n```\n\nThe code iterates over `msg->hdr.num_pkts` (defined by the user) and passes a pointer to the payload and the packet\nsize to `adv_net_set_udp_payload`. In this example our configuration is using `fill_mode` \"udp\" on the transmitter, so\n`adv_net_set_udp_payload` will populate the Ethernet, IP, and UDP headers. The payload pointer passed by the user\nis also copied into the buffer. Alternatively a user could use the packet buffers directly as output from a previous stage\nto avoid this extra copy.\n\nWith the `AdvNetBurstParams` populated, the burst can be sent off to the advanced network operator for transmission:\n\n```\nop_output.emit(msg, \"burst_out\");\n```\n",
        "application_name": "advanced_network",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "cvcuda_holoscan_interop",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "CV-CUDA",
                "Computer Vision",
                "CV"
            ],
            "ranking": 1,
            "dependencies": {
                "data": "https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/holoscan_endoscopy_sample_data",
                "libraries": [
                    {
                        "name": "cvcuda",
                        "version": "0.3.1-beta"
                    }
                ]
            }
        },
        "readme": "### CVCUDA Holoscan Interoperability Operators\n\nThis directory contains two operators to enable interoperability between the [CVCUDA](https://github.com/CVCUDA/CV-CUDA) and Holoscan\ntensors: `holoscan::ops::CvCudaToHoloscan` and `holoscan::ops::HoloscanToCvCuda`.\n\n#### `holoscan::ops::CvCudaToHoloscan`\n\nOperator class to convert a `nvcv::Tensor` to a `holoscan::Tensor`.\n\n##### Inputs\n\n- **`input`**: a CV-CUDA tensor\n  - type: `nvcv::Tensor`\n\n##### Outputs\n\n- **`output`**: a Holoscan tensor as `holoscan::Tensor` in `holoscan::TensorMap`\n  - type: `holoscan::TensorMap`\n\n#### `holoscan::ops::HoloscanToCvCuda`\n\n##### Inputs\n\n- **`input`**: a `gxf::Entity` containing a Holoscan tensor as `holoscan::Tensor`\n  - type: `gxf::Entity`\n\n##### Outputs\n\n- **`output`**: a CV-CUDA tensor\n  - type: `nvcv::Tensor`",
        "application_name": "cvcuda_holoscan_interop",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "qt_video",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Qt",
                "QML",
                "QtQuick",
                "Video",
                "UI",
                "Userinterface",
                "Interactive"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Qt Video Operator\n\nThe `qt_video` operator is used to display a video in a [QtQuick](https://doc.qt.io/qt-6/qtquick-index.html) application.\n\nFor more information on how to use this operator in an application see [Qt video replayer example](../../applications/qt_video_replayer/README.md).\n\n#### `holoscan::ops::QtVideoOp`\n\nOperator class.\n\n##### Parameters\n\n- **`QtHoloscanVideo`**: Instance of QtHoloscanVideo to be used\n      - type: `QtHoloscanVideo\n\n##### Inputs\n\n- **`input`**: Input frame data\n  - type: `nvidia::gxf::Tensor` or `nvidia::gxf::VideoBuffer`\n",
        "application_name": "qt_video",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "lstm_tensor_rt_inference",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "LSTM",
                "TensorRT"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Custom LSTM Inference\n\nThe `lstm_tensor_rt_inference` extension provides LSTM (Long-Short Term Memory) stateful inference module using TensorRT.\n\n#### `nvidia::holoscan::lstm_tensor_rt_inference::TensorRtInference`\n\nCodelet, taking input tensors and feeding them into TensorRT for LSTM inference.\n\nThis implementation is based on `nvidia::gxf::TensorRtInference`.\n`input_state_tensor_names` and `output_state_tensor_names` parameters are added to specify tensor names for states in LSTM model.\n\n##### Parameters\n\n- **`model_file_path`**: Path to ONNX model to be loaded\n  - type: `std::string`\n- **`engine_cache_dir`**: Path to a directory containing cached generated engines to be serialized and loaded from\n  - type: `std::string`\n- **`plugins_lib_namespace`**: Namespace used to register all the plugins in this library (default: `\"\"`)\n  - type: `std::string`\n- **`force_engine_update`**: Always update engine regard less of existing engine file. Such conversion may take minutes (default: `false`)\n  - type: `bool`\n- **`input_tensor_names`**: Names of input tensors in the order to be fed into the model\n  - type: `std::vector<std::string>`\n- **`input_state_tensor_names`**: Names of input state tensors that are used internally by TensorRT\n  - type: `std::vector<std::string>`\n- **`input_binding_names`**: Names of input bindings as in the model in the same order of what is provided in input_tensor_names\n  - type: `std::vector<std::string>`\n- **`output_tensor_names`**: Names of output tensors in the order to be retrieved from the model\n  - type: `std::vector<std::string>`\n- **`input_state_tensor_names`**: Names of output state tensors that are used internally by TensorRT\n  - type: `std::vector<std::string>`\n- **`output_binding_names`**: Names of output bindings in the model in the same order of of what is provided in output_tensor_names\n  - type: `std::vector<std::string>`\n- **`pool`**: Allocator instance for output tensors\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`cuda_stream_pool`**: Instance of gxf::CudaStreamPool to allocate CUDA stream\n  - type: `gxf::Handle<gxf::CudaStreamPool>`\n- **`max_workspace_size`**: Size of working space in bytes (default: `67108864l` (64MB))\n  - type: `int64_t`\n- **`dla_core`**: DLA Core to use. Fallback to GPU is always enabled. Default to use GPU only (`optional`)\n  - type: `int64_t`\n- **`max_batch_size`**: Maximum possible batch size in case the first dimension is dynamic and used as batch size (default: `1`)\n  - type: `int32_t`\n- **`enable_fp16_`**: Enable inference with FP16 and FP32 fallback (default: `false`)\n  - type: `bool`\n- **`verbose`**: Enable verbose logging on console (default: `false`)\n  - type: `bool`\n- **`relaxed_dimension_check`**: Ignore dimensions of 1 for input tensor dimension check (default: `true`)\n  - type: `bool`\n- **`clock`**: Instance of clock for publish time (`optional`)\n  - type: `gxf::Handle<gxf::Clock>`\n- **`rx`**: List of receivers to take input tensors\n  - type: `std::vector<gxf::Handle<gxf::Receiver>>`\n- **`tx`**: Transmitter to publish output tensors\n  - type: `gxf::Handle<gxf::Transmitter>`\n",
        "application_name": "lstm_tensor_rt_inference",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "volume_loader",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Load",
                "MHD",
                "NIFTI",
                "NRRD"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Volume Loader\n\nThe `volume_loader` operator reads 3D volumes from the specified input file.\n\nThe operator supports these file formats:\n* MHD https://itk.org/Wiki/ITK/MetaIO/Documentation\n* NIFTI https://nifti.nimh.nih.gov/\n* NRRD https://teem.sourceforge.net/nrrd/format.html\n\n#### `holoscan::ops::VolumeLoaderOp`\n\nOperator class to read a volume.\n\n##### Parameters\n\n- **`file_name`**: Volume data file name\n  - type: `std::string`\n- **`allocator`**: Allocator used to allocate the volume data\n  - type: `std::shared_ptr<Allocator>`\n\n##### Outputs\n\n- **`volume`**: Output volume data\n  - type: `nvidia::gxf::Tensor`\n- **`spacing`**: Physical size of each volume element\n  - type: `std::array<float, 3>`\n- **`permute_axis`**: Volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}\n  - type: `std::array<uint32_t, 3>`\n- **`flip_axes`**: Volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}\n  - type: `std::array<bool, 3>`\n- **`extent`**: Physical size of the the volume in world space\n  - type: `std::array<float, 3>`\n",
        "application_name": "volume_loader",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "webrtc_server",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Server",
                "Browser",
                "Video"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    }
                ]
            }
        },
        "readme": "### WebRTC Server Operator\n\nThe `webrtc_server` operator sends video frames through a WebRTC connection. The application using this operator needs to call the `offer` method of the operator when a new WebRTC connection is available.\n\n### Methods\n\n- **`async def offer(self, sdp, type) -> (local_sdp, local_type)`**\n  Start a connection between the local computer and the peer.\n\n  **Parameters**\n  - **sdp** peer Session Description Protocol object\n  - **type** peer session type\n\n  **Return values**\n  - **sdp** local Session Description Protocol object\n  - **type** local session type\n\n### Inputs\n\n- **`input`**: Tensor or numpy array with 8 bit per component RGB data\n  - type: `Tensor`\n",
        "application_name": "webrtc_server",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "visualizer_icardio",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Visualizer iCardio\n\nThe `visualizer_icardio` extension generates the visualization components from the processed results of the plax chamber model.\n\n#### `nvidia::holoscan::multiai::VisualizerICardio`\n\nVisualizer iCardio extension ingests the processed results of the plax chamber model and generates the key points, the key areas and the lines that are transmitted to the HoloViz codelet.\n\n##### Parameters\n\n- **`in_tensor_names_`**: Input tensor names\n  - type: `std::vector<std::string>`\n- **`out_tensor_names_`**: Output tensor names\n  - type: `std::vector<std::string>`\n- **`allocator_`**: Memory allocator\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`receivers_`**: Vector of input receivers. Multiple receivers supported.\n  - type: `HoloInfer::GXFReceivers`\n- **`transmitter_`**: Output transmitter. Single transmitter supported.\n  - type: `HoloInfer::GXFTransmitters`\n",
        "application_name": "visualizer_icardio",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "openigtlink",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Streaming",
                "Ethernet",
                "3DSlicer"
            ],
            "ranking": 2,
            "dependencies": {
                "SDK": "OpenIGTLink"
            }
        },
        "readme": "### OpenIGTLink operator\n\nThe `openigtlink` operator provides a way to send and receive imaging data using the [OpenIGTLink](http://openigtlink.org/) library. The `openigtlink` operator contains separate operators for transmit and receive. Users may choose one or the other, or use both in applications requiring bidirectional traffic.\n\nThe `openigtlink` operators use class names: `OpenIGTLinkTxOp` and `OpenIGTLinkRxOp`\n\n#### `nvidia::holoscan::openigtlink`\n\nOperator class to send and transmit data using the OpenIGTLink protocol.\n\n##### Receiver Configuration Parameters\n\n- **`port`**: Port number of server\n  - type: `integer`\n- **`out_tensor_name`**: Name of output tensor\n  - type: `string`\n- **`flip_width_height`**: Flip width and height (necessary for receiving from 3D Slicer)\n  - type: `bool`\n\n##### Transmitter Configuration Parameters\n\n- **`device_name`**: OpenIGTLink device name\n  - type: `string`\n- **`input_names`**: Names of input messages\n  - type: `std::vector<std::string>`\n- **`host_name`**: Host name\n  - type: `string`\n- **`port`**: Port number of server\n  - type: `integer`",
        "application_name": "openigtlink",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "mesh_to_usd",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "1.0.3",
                "tested_versions": [
                    "1.0.3"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "mesh",
                "OpenUSD",
                "STL"
            ],
            "ranking": 2,
            "dependencies": {}
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "mesh_to_usd",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "volume_renderer",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Volume",
                "Render",
                "ClaraViz"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "### Volume Renderer\n\nThe `volume_renderer` operator renders a volume using ClaraViz (https://github.com/NVIDIA/clara-viz).\n\n#### `holoscan::ops::VolumeRenderer`\n\nOperator class to render a volume.\n\n##### Parameters\n\n- **`config_file`**: Config file path. The content of the file is passed to `clara::viz::JsonInterface::SetSettings()` at initialization time.\n  - type: `std::string`\n- **`allocator`**: Allocator used to allocate render buffer outputs when no pre-allocated color or depth buffer is passed to `color_buffer_in` or `depth_buffer_in`. Allocator needs to be capable to allocate device memory.\n  - type: `std::shared_ptr<Allocator>`\n- **`alloc_width`**: Width of the render buffer to allocate when no pre-allocated buffers are provided.\n  - type: `uint32_t`\n- **`alloc_height`**: Height of the render buffer to allocate when no pre-allocated buffers are provided.\n  - type: `uint32_t`\n\n##### Inputs\n\nAll inputs are optional.\n\n- **`volume_pose`**: Transform the volume.\n  - type: `nvidia::gxf::Pose3D`\n- **`crop_box`**: Volume crop box. Each `nvidia::gxf::Vector2f` contains the min and max values in range `[0, 1]` of the x, y and z axes of the volume.\n  - type: `std::array<nvidia::gxf::Vector2f, 3>`\n- **`depth_range`**: The distance to the near and far frustum planes.\n  - type: `nvidia::gxf::Vector2f`\n- **`left_camera_pose`**: Camera pose for the left camera when rendering in stereo mode.\n  - type: `nvidia::gxf::Pose3D`\n- **`right_camera_pose`**: Camera pose for the right camera when rendering in stereo mode.\n  - type: `nvidia::gxf::Pose3D`\n- **`left_camera_model`**: Camera model for the left camera when rendering in stereo mode.\n  - type: `nvidia::gxf::CameraModel`\n- **`right_camera_model`**: Camera model for the right camera when rendering in stereo mode.\n  - type: `nvidia::gxf::CameraModel`\n- **`camera_matrix`**: Camera pose when not rendering in stereo mode.\n  - type: `std::array<float, 16>`\n- **`color_buffer_in`**: Buffer to store the rendered color data to, format needs to be 8 bit per component RGBA and buffer needs to be in device memory.\n  - type: `nvidia::gxf::VideoBuffer`\n- **`depth_buffer_in`**: Buffer to store the rendered depth data to, format needs to be 32 bit float single component buffer needs to be in device memory.\n  - type: `nvidia::gxf::VideoBuffer`\n- **`density_volume`**: Density volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.\n  - type: `nvidia::gxf::Tensor`\n- **`density_spacing`**: Physical size of each density volume element.\n  - type: `std::array<float, 3>`\n- **`density_permute_axis`**: Density volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.\n  - type: `std::array<uint32_t, 3>`\n- **`density_flip_axes`**: Density volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.\n  - type: `std::array<bool, 3>`\n- **`mask_volume`**: Mask volume data. Needs to be a 3D single component array. Supported data types are signed|unsigned 8|16|32 bit integer and 32 bit floating point.\n  - type: `nvidia::gxf::Tensor`\n- **`mask_spacing`**: Physical size of each mask volume element.\n  - type: `std::array<float, 3>`\n- **`mask_permute_axis`**: Mask volume axis permutation of data space to world space, e.g. if x and y of a volume is swapped this is {1, 0, 2}.\n  - type: `std::array<uint32_t, 3>`\n- **`mask_flip_axes`**: Mask volume axis flipping from data space to world space, e.g. if x is flipped this is {true, false, false}.\n  - type: `std::array<bool, 3>`\n\n##### Outputs\n\n- **`color_buffer_out`**: Buffer with rendered color data, format is 8 bit per component RGBA and buffer is in device memory.\n  - type: `nvidia::gxf::VideoBuffer`\n- **`depth_buffer_out`**: Buffer with rendered depth data, format is be 32 bit float single component and buffer is in device memory.\n  - type: `nvidia::gxf::VideoBuffer`\n",
        "application_name": "volume_renderer",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "prohawk_video_processing",
            "authors": [
                {
                    "name": "Tim Wooldridge",
                    "affiliation": "Prohawk Technology Group"
                }
            ],
            "language": "C++",
            "version": "1.0.0",
            "changelog": {
                "1.0": "Added watermark to the prohawk restoration engine"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.1",
                "tested_versions": [
                    "0.5.1",
                    "0.6.0"
                ]
            },
            "platforms": [
                "arm64"
            ],
            "tags": [
                "Video processing",
                "Prohawk"
            ],
            "ranking": 4,
            "dependencies": {
                "SDK": "Prohawk runtime"
            }
        },
        "readme": "# HoloHub Operators\n\nThis directory contains operators for the Holoscan Platform.\n\n# Contributing to HoloHub Operators\n\nPlease review the [CONTRIBUTING.md file](https://github.com/nvidia-holoscan/holohub/blob/main/CONTRIBUTING.md) guidelines to contribute operators.\n",
        "application_name": "prohawk_video_processing",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "tool_tracking_postprocessor",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.5.0",
                "tested_versions": [
                    "0.5.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "visualization",
                "tool tracking"
            ],
            "ranking": 1,
            "dependencies": {}
        },
        "readme": "\n### Tool tracking postprocessor\n\nThe `tool_tracking_postprocessor` extension provides a codelet that converts inference output of `lstm_tensor_rt_inference` used in the endoscopy tool tracking pipeline to be consumed by the `holoviz` codelet.\n\n#### `nvidia::holoscan::tool_tracking_postprocessor`\n\nTool tracking postprocessor codelet\n\n##### Parameters\n\n- **`in`**: Input channel, type `gxf::Tensor`\n  - type: `gxf::Handle<gxf::Receiver>`\n- **`out`**: Output channel, type `gxf::Tensor`\n  - type: `gxf::Handle<gxf::Transmitter>`\n- **`min_prob`**: Minimum probability, (default: 0.5)\n  - type: `float`\n- **`overlay_img_colors`**: Color of the image overlays, a list of RGB values with components between 0 and 1, (default: 12 qualitative classes color scheme from colorbrewer2)\n  - type: `std::vector<std::vector<float>>`\n- **`host_allocator`**: Output Allocator\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`device_allocator`**: Output Allocator\n  - type: `gxf::Handle<gxf::Allocator>`\n- **`cuda_stream_pool`**: Instance of gxf::CudaStreamPool\n  - type: `gxf::Handle<gxf::CudaStreamPool>`\n",
        "application_name": "tool_tracking_postprocessor",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "webrtc_client",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "WebRTC",
                "Client",
                "Browser",
                "Video"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "libraries": [
                    {
                        "name": "aiortc",
                        "version": "1.5.0"
                    }
                ]
            }
        },
        "readme": "### WebRTC Client Operator\n\nThe `webrtc_client` operator receives video frames through a WebRTC connection. The application using this operator needs to call the `offer` method of the operator when a new WebRTC connection is available.\n\n### Methods\n\n- **`async def offer(self, sdp, type) -> (local_sdp, local_type)`**\n  Start a connection between the local computer and the peer.\n\n  **Parameters**\n  - **sdp** peer Session Description Protocol object\n  - **type** peer session type\n\n  **Return values**\n  - **sdp** local Session Description Protocol object\n  - **type** local session type\n\n### Outputs\n\n- **`output`**: Tensor with 8 bit per component RGB data\n  - type: `Tensor`\n",
        "application_name": "webrtc_client",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "basic_network",
            "authors": [
                {
                    "name": "Cliff Burdick",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "Network",
                "Networking",
                "UDP",
                "Ethernet",
                "IP",
                "TCP"
            ],
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "basic_network",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "\n### Basic networking operator\n\nThe `basic_network_operator` operator provides a way to send and receive data over Linux sockets. The\ndestination can be on the same machine or over a network. The basic network operator contains separate\noperators for transmit and receive. Users may choose one or the other, or use both in applications \nrequiring bidirectional traffic.\n\nFor TCP sockets the basic network operator only supports a single stream currently. Future versions\nmay expand this to launch multiple threads to listen on different streams.\n\nThe basic networking operators use class names: `BasicNetworkOpTx` and `BasicNetworkOpRx`\n\n#### `nvidia::holoscan::basic_network_operator`\n\nBasic networking operator\n\n##### Receiver Configuration Parameters\n\n- **`batch_size`**: Bytes in batch\n  - type: `integer`\n- **`max_payload_size`**: Maximum payload size for a single packet\n  - type: `integer`\n- **`udp_dst_port`**: UDP destination port for packets\n  - type: `integer`\n- **`l4_proto`**: Layer 4 protocol\n  - type: `string` (`udp`/`tcp`)\n- **`ip_addr`**: Destination IP address\n  - type: `string`    \n\n##### Transmitter Configuration Parameters\n\n- **`max_payload_size`**: Maximum payload size for a single packet\n  - type: `integer`\n- **`udp_dst_port`**: UDP destination port for packets\n  - type: `integer`\n- **`l4_proto`**: Layer 4 protocol\n  - type: `string` (`udp`/`tcp`)\n- **`ip_addr`**: Destination IP address\n  - type: `string`    \n- **`min_ipg_ns`**: Minimum inter-packet gap in nanoseconds\n  - type: `integer`  \n\n\n##### Transmitter and Receiver Operator Parameters\n\nThe transmitter and receiver operator both use the `NetworkOpBurstParams` structure as input\nand output to their ports, respectively. `NetworkOpBurstParams` contains the following fields:\n\n- **`data`**: Pointer to batch of packet data\n  - type: `uint8_t *`\n- **`len`**: Length of total buffer in bytes\n  - type: `integer`\n- **`num_pkts`**: Number of packets in batch\n  - type: `integer`\n\nTo receive messages from the Receive operator use the output port `burst_out`.\nTo send messages to the Transmit operator use the input port `burst_in`.",
        "application_name": "basic_network",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_decoder_response",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "decoder",
                "response"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videodecoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Decoder Response\n\nThe `video_decoder_response` handles the output of the decoded H264 bit stream.\n\n#### `holoscan::ops::VideoDecoderResponseOp`\n\nOperator class to handle the output of the decoded H264 bit stream.\n\nThis implementation is based on `nvidia::gxf::VideoDecoderResponse`.\n\n##### Parameters\n\n- **`output_transmitter`**: Transmitter to send the yuv data.\n  - type: `holoscan::IOSpec*`\n- **`pool`**: Memory pool for allocating output data.\n  - type: `std::shared_ptr<Allocator>`\n- **`outbuf_storage_type`**: Output Buffer Storage(memory) type used by this allocator. Can be 0: kHost, 1: kDevice.\n  - type: `uint32_t`\n- **`videodecoder_context`**: Decoder context Handle.\n  - type: `std::shared_ptr<holoscan::ops::VideoDecoderContext>`\n",
        "application_name": "video_decoder",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_decoder_context",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "decoder",
                "context"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videodecoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Decoder Context\n\nThe `video_decoder_context` is used to hold common variables and underlying context. The decoder context handle is passed to both `holoscan::ops::VideoDecoderRequestOp` and `holoscan::ops::VideoDecoderResponseOp`.\n\n#### `holoscan::ops::VideoDecoderContext`\n\nA class used to hold common variables and underlying context required by `holoscan::ops::VideoDecoderRequestOp` and `holoscan::ops::VideoDecoderResponseOp`.\n\nThis implementation is based on the GXF Component `nvidia::gxf::VideoDecoderContext`.\n\n##### Parameters\n\n- **`async_scheduling_term`**: Asynchronous scheduling condition required to get/set event state.\n  - type: `std::shared_ptr<holoscan::AsynchronousCondition>`\n\n",
        "application_name": "video_decoder",
        "source_folder": "operators"
    },
    {
        "metadata": {
            "name": "video_decoder_request",
            "authors": [
                {
                    "name": "Holoscan Team",
                    "affiliation": "NVIDIA"
                }
            ],
            "version": "1.0",
            "changelog": {
                "1.0": "Initial Release"
            },
            "holoscan_sdk": {
                "minimum_required_version": "0.6.0",
                "tested_versions": [
                    "0.6.0"
                ]
            },
            "platforms": [
                "amd64",
                "arm64"
            ],
            "tags": [
                "video",
                "decoder"
            ],
            "ranking": 1,
            "dependencies": {
                "gxf_extensions": [
                    {
                        "name": "videodecoder",
                        "version": "1.0"
                    },
                    {
                        "name": "videodecoderio",
                        "version": "1.0"
                    }
                ]
            }
        },
        "readme": "### Video Decoder Request\n\nThe `video_decoder_request` handles the input for the H264 bit stream decode.\n\n#### `holoscan::ops::VideoDecoderRequestOp`\n\nOperator class to handle the input for the H264 bit stream decode.\n\nThis implementation is based on `nvidia::gxf::VideoDecoderRequest`.\n\n##### Parameters\n\n- **`input_frame`**: Receiver to get the input image.\n  - type: `holoscan::IOSpec*`\n- **`inbuf_storage_type`**: Input Buffer storage type, 0:kHost, 1:kDevice.\n  - type: `uint32_t`\n- **`async_scheduling_term`**: Asynchronous scheduling condition.\n  - type: `std::shared_ptr<holoscan::AsynchronousCondition>`\n- **`videodecoder_context`**: Decoder context Handle.\n  - type: `std::shared_ptr<holoscan::ops::VideoDecoderContext>`\n- **`codec`**: Video codec to use,  0:H264, only H264 supported. Default:0.\n  - type: `uint32_t`\n- **`disableDPB`**: Enable low latency decode, works only for IPPP case.\n  - type: `uint32_t`\n- **`output_format`**: Output frame video format, nv12pl and yuv420planar are supported.\n  - type: `std::string`\n",
        "application_name": "video_decoder",
        "source_folder": "operators"
    }
]